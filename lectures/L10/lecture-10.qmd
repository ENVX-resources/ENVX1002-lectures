---
title: Topic 10 -- Simple Linear Regression
author: Si Yang Han
# format: soles-revealjs
embed-resources: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

if (!require("pacman")) install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(tidyverse, cowplot, HistData, patchwork, broom)

ggplot2::theme_set(cowplot::theme_half_open())
```

## Module overview

- [Week 9. Describing Relationships]{style="color: #D0D3D4;"}
    + [Correlation (calculation, interpretation)]{style="color: #D0D3D4;"}
    + [Regression (model structure, model fitting]{style="color: #D0D3D4;"}
    + [What/when/why/how]{style="color: #D0D3D4;"}

- **Week 10. Simple Linear Regression**
    + Can we use the model?(assumptions, hypothesis testing)
    + How good is the model?(interpretation, model fit)
    
- [Week 11. Multiple Linear Regression]{style="color: #D0D3D4;"}
    + [Multiple Linear Regression (MLR) modelling]{style="color: #D0D3D4;"}
    + [Assumptions, interpretation and the principle of parsimony]{style="color: #D0D3D4;"}

- [Week 12. Nonlinear Regression]{style="color: #D0D3D4;"}
    + [Common nonlinear functions]{style="color: #D0D3D4;"}
    + [Transformations]{style="color: #D0D3D4;"}

# Recap

## Last week...

- Correlation $r$: a measure of the strength and direction of the linear relationship between two variables
- Is there a moderate to strong *causal* relationship?
  
. . .

### Simple linear regression modelling

$$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$ 


> Basically, a deterministic straight line equation $y=c+mx$, with added random variation that is normally distributed

$$ Y = c + mx + \epsilon $$


## Fitting the line 
$$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$ 

$$ Y = c + mx + \epsilon $$

How do we fit a line to data if data are "noisy"?

```{r}
#| code-fold: true
x <- 1:10
y <- 2 * x + rnorm(10, 0, 2)
# generate y with predicted values
y_pred <- 2 * x
df <- data.frame(x, y)

p1 <- ggplot(df, aes(x, y_pred)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  labs(x = "x", y = "y", title = "A")

p2 <- ggplot(df, aes(x, y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "x", y = "y", title = "B (How do we fit this?)")

library(patchwork)
p1 + p2 + plot_layout(ncol = 2)
```

# Least squares

> The method of least squares is the **automobile of modern statistical analysis**: despite its limitations, ocassional accidents and incidental pollution, it and its numerous variations, extensions, and related conveyances **carry the bulk of statistical analyses**, and are known and valued by nearly all.

-- Stigler, 1981 (emphasis added)

## Usage

- **Student's t-test** (indirectly)
- **linear regression**
  
. . .

- **nonlinear regression (logistic, polynomial, exponential, etc.)**
- analysis of variance (ANOVA)
- generalised linear model
- principle component analysis
- machine learning models
- etc...


## Galton's data revisited

- Galton's data on the heights of parents and their children.
- Is there a relationship between the heights of parents and their children?

```{r}
#| code-fold: true
library(HistData)
data(Galton)
fit <- lm(child ~ parent, data = Galton)
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  labs(x = "Parent height (inches)", y = "Child height (inches)")
```

**How did we end up with the line in the plot above?**

# Fitting the model

## How do we analytically fit a line?

- *Minimise* the sum of the squared residuals:

$$\color{firebrick}{argmin_{\beta_0, \beta_1}} \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$ 


![[Source](https://github.com/Enchufa2/ls-springs)](images/leastsquares.gif){fig-align="center"}

## Residuals, $\hat \epsilon$

$$ \color{firebrick}{\hat{\epsilon_i}} = \color{royalblue}{y_i} - \color{forestgreen}{\hat{y_i}} $$

. . .

```{r}
#| code-fold: true
# simulate example data
set.seed(340)
x <- runif(8, 0, 30)
y <- 5 * x + rnorm(8, 0, 40)
df <- data.frame(x, y)

# fit linear model, add residual vertical lines as arrows
mod <- lm(y ~ x, data = df)
p1 <- ggplot(df, aes(x, y)) +
  geom_point() +
  geom_segment(aes(xend = x, yend = fitted(mod)),
    arrow = arrow(length = unit(0.2, "cm")),
    color = "royalblue"
  ) +
  labs(x = "x", y = "y")

p1 +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  annotate("text",
    x = 6.3, y = -6, size = 7,
    label = expression(hat(epsilon[i])), colour = "royalblue"
  ) +
  annotate("text",
    x = 5.6, y = 25, size = 7,
    label = expression(hat(y[i])), colour = "forestgreen"
  ) +
  annotate("text",
    x = 5.6, y = -36, size = 7,
    label = expression(y[i]), colour = "firebrick"
  )
```

## Slope, $\beta_1$

<!-- $$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$ -->

$$ \beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} =  \frac{Cov(x,y)}{Var(x)} = \frac{SS_{xy}}{SS_{xx}} $$



```{r}
#| code-fold: true
# Calculate slope from df
beta1 <- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) /
  sum((df$x - mean(df$x))^2)
# beta0 <- mean(df$y) - beta1 * mean(df$x)

p1 +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick", linetype = 2) +
  # label the line
  annotate("text",
    x = 15, y = 65, size = 7,
    label = expression(beta[1]), colour = "firebrick"
  )

```


<!-- $$ \beta_0 = \bar{y} - \beta_1 \bar{x} $$ -->

## Intercept

$$ \beta_0 = \bar{y} - \beta_1 \bar{x} $$


```{r}
#| code-fold: true
# calculate mean y from df
ybar <- mean(df$y)
xbar <- mean(df$x)
beta0 <- ybar - beta1 * xbar

p1 + geom_vline(xintercept = xbar, linetype = "dashed", color = "slateblue") +
  geom_hline(yintercept = ybar, linetype = "dashed", color = "slateblue") +
  # label the lines
  annotate("text",
    x = 25, y = ybar * 0.8, size = 7,
    label = expression(bar(y)), colour = "slateblue"
  ) +
  annotate("text",
    x = xbar * 1.05, y = 150, size = 7,
    label = expression(bar(x)), colour = "slateblue"
  ) +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick", linetype = 2) +
  # extend the geom_smooth line to intercept x=0
  geom_segment(aes(x = xbar, y = ybar, xend = 0, yend = beta0),
    color = "firebrick", linetype = 2
  ) +
  # label the slope line
  annotate("text",
    x = 15, y = 65, size = 7,
    label = expression(beta[1]), colour = "firebrick"
  ) +
  # add a dot at the intercept
  geom_point(aes(x = 0, y = beta0), color = "seagreen", size = 3) +
  # label the intercept
  annotate("text",
    x = 0, y = beta0 * 1.4, size = 7,
    label = expression(beta[0]), colour = "seagreen"
  )

```


# It's a lot easier in R...

## Fitting a linear model in R

Is there a relationship between the heights of parents and their children?

```{r}
fit <- lm(child ~ parent, data = Galton)
fit
```


$$ \widehat{child} = 23.9 + 0.646 \cdot parent$$

. . .

How good is the model? Do we trust our model? How can we interpret the results?

## Steps for Regression

1.  [Understand the variables]{style="color: #D0D3D4;"}
2.  [Explore data]{style="color: #D0D3D4;"}
3.  [Fit model]{style="color: #D0D3D4;"}
4.  Check assumptions
5.  Assess model fit
6.  Interpret output

# Assumptions

> The data **must** meet certain criteria for linear regression, which we often call *assumptions*.

## Assumptions - LINE

- **L**inearity. The relationship between $y$ and $x$ is linear.
- **I**ndependence. The errors $\epsilon$ are independent.
- **N**ormal. The errors $\epsilon$ are normally distributed.
- **E**qual Variance of errors $\epsilon$. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.

. . .

:::{.callout-tip}
All but the independence assumption can be assessed using diagnostic plots. 
:::

. . .

### Why do we care?

- If the assumptions are met, then we can be confident that the model is a good representation of the data.
- If they are *not* met, the results are still presented, but our interpretation of the model is likely to be flawed.
  - Hypothesis test results are unreliable
  - Standard error is unreliable
  - Poor estimates of coefficients = poor predictions

## How do we check the assumptions?

Recall that the linear model is a **deterministic straight line equation** $y = c + mx$ plus some **random noise** $\epsilon$:

$$ Y_i = \beta_0 + \beta_1 x + \epsilon $$

- **If the only source of variation in $y$ is $\epsilon$, then we can check our assumptions by just looking at the residuals $\hat{\epsilon}$.**

. . .

:::{.callout-warning}
R will not warn you if the assumptions are not met. It is up to you to check them!
:::

### How do we get the residuals?

- Fit the model!
- Residuals need to be calculated from the model, not from the raw data.
- In R, these values are stored automatically.

## Another way to look at residuals

![](images/residual.jpg){fig-align="center"}

Once you have fitted the line, it does not change. The residuals are the vertical distances between the points (not shown) and the line.

# Checking assumptions

linearity | normality | equal variance | outliers

## 1-step

- **Residuals vs. Fitted**: check for linearity, equal variance.
- **Q-Q Residuals**: check for normality.
- **Scale-Location**: check for equal variance (standardised).
- **Residuals vs. Leverage**: check for outliers (influential points).

```{r}
#| eval: false
par(mfrow = c(2, 2)) # need to do this to get 4 plots on one page
plot(fit)
```

---

```{r}
#| echo: false
par(mfrow = c(2, 2)) # need to do this to get 4 plots on one page
plot(fit)
```

## Assumption: Linearity

- Residuals vs. fitted plot looks at the relationship between the residuals and the fitted values.
- If the relationship is linear:
  - Residuals should be randomly scattered around the horizontal axis.
  - The red line should be reasonably straight.
- Could also look at a scatterplot of x and y!

```{r}
plot(fit, which = 1)
```

## Examples

![](images/residual_vs_fitted.png){fig-align="center"}
Linear Models with R (Faraway 2005, p59)

## Assumption: Normality

- Q-Q plot looks at the distribution of the residuals against a normal distribution function (the dotted line).
- Sometimes, a histogram is still useful to see the shape of the distribution.

```{r}
par(mfrow = c(1, 2))
plot(fit, which = 2)
hist(rstandard(fit))
```

## Assumption: Normality

- If **normally distributed**, the points should follow the red line.
- Deviation from the red line is common in the tails (i.e. the ends), but not in the middle.

. . .

### Tips

- **Light-tailed**: small variance in residuals, resulting in a narrow distribution.
- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution.
- **Left-skewed** (n shape): more data falls to the left of the mean.
- **Right-skewed** (u shape): more data falls to the right of the mean.

:::{.callout-tip}
Left or right-skewed? Look at where the tail points. 
:::

## Examples

```{r}
#| code-fold: true
set.seed(915)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 2)
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
par(mfrow = c(1, 2))
plot(fit_eg, which = 2)
hist(rstandard(fit_eg))
```

---

```{r}
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 3) * -1
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
par(mfrow = c(1, 2))
plot(fit_eg, which = 2)
hist(rstandard(fit_eg))
```

---

```{r}
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rnbinom(100, 10, .5)
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
par(mfrow = c(1, 2))
plot(fit_eg, which = 2)
hist(rstandard(fit_eg))
```

# Assumption: Equal variances

## Equal variances

- Look at the **scale-location plot**.
- If variances are equal, the points should be randomly scattered around the horizontal axis.
- The red line should be more or less horizontal.

```{r}
plot(fit, which = 3)
```

## Equal variances

- If variances are not equal we *may* see:
  - A funnel shape, where the points are more spread out at the ends than in the middle. Sometimes also called "fanning".
  - Patterns in the scale-location plot, such as a curve or a wave, indicating that the variance is changing.
- Look at the red line for a general trend, **but don't depend on it too much**.

```{r}
#| code-fold: true
set.seed(915)
x <- rnorm(100)
y <- 2 + 5 * x^2 + rchisq(100, df = 2)
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
plot(fit_eg, which = 3)
```

## Outliers

- **Leverage** is a measure of how far away the predictor variable is from the mean of the predictor variable.
- The Residuals vs Leverage plot shows the relationship between the residuals and the leverage of each point.
- **Cook's distance** is a measure of how much the model would change if a point was removed.

---

In general, points with **high leverage** and **high Cook's distance** are considered outliers.

```{r}
plot(fit, which = 5)
```

--- 

### Example of an influential outlier

```{r}
#| code-fold: true
set.seed(1028)
x <- rnorm(30)
y <- 2 + 2 * x

y[25] <- y[25] + 40 # Add an outlier

fit_eg <- lm(y ~ x)

par(mfrow=c(1,2))
plot(fit_eg, which = 5)

plot(x, y)
abline(fit_eg, col = "red")

fit_eg2 <- lm(y[-25] ~ x[-25])
abline(fit_eg2, col = "blue", lty = 3)

```

Points that exceed the dashed line (which appears once they approach the Cook's distance), are likely to influence the model. These outliers should be removed. 

e.g. this is of a perfect line with one extreme outlier - the line of best fit deviates because of a single point!

---

# What can we do if the assumptions aren't met?

## It depends...

...which assumption is not met and the type of data i.e. circumstances.

- If data is **non-linear**, try a transformation of the response variable $y$, from light to extreme:
  - root: $\sqrt{y}$ or $\sqrt{y+1}$ if $y$ contains zeros
  - log: $\log(y)$ or $\log(y+1)$ if $y$ contains zeros
  - inverse: $\frac{1}{y}$ or $\frac{1}{y+1}$ if $y$ contains zeros

. . .

- If residuals are **not normally distributed**, try a transformation of the response variable $y$ first, otherwise transform the predictor variable $x$. Both can be done at the same time.

. . .

- If **equal variances** assumption is not met, same as above.
- If **outliers** are present, try removing them, or transforming the response variable $y$.

## What if transformation doesn't work?

If the assumptions are still not met after trying the above, you can try:

- Using a different type of regression e.g. logistic regression, non-linear regression
- Using a different model e.g. machine learning.
- Using a non-parametric test.

## Back to Galton - model assumptions are met

```{r}
#| echo: false
par(mfrow = c(2, 2)) # need to do this to get 4 plots on one page
plot(fit)
```

Now what?

# Model Assessment and Interpretation

> Hypothesis?

> How good is the model?

> What can we understand about the relationship between `child` and `parent`?

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 1-2

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- `Call`: the model formula

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 4-6

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- `Residuals`: distribution of the residuals

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 8-11

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- `Coefficients`: a summary table of the coefficients, their standard errors, t-values, and p-values.
- `(Intercept)/Estimate`: the **y-intercept**, or the mean response when all predictors are 0. Not meaningful in most cases e.g. no parent will have a height of 0 inches!
- `parent/Estimate`: the **slope** coefficient - i.e. the change in the **mean** of the response for a **one-unit increase in the predictor**.
  - The *p-value* (`Pr`) tells us whether the slope is significantly different from 0 (the mean).
  - If p < 0.05, we can conclude that there is a **significant linear relationship** between the predictor and the response.
  
## Hypothesis testing

How does our null ($H_0: \beta_1=0$) model compare to the linear ($H_0: \beta_1 \neq 0$) model?

> In simple linear regression, the p-value for the slope coefficient is the same as the p-value for the F-statistic.

```{r}
#| code-fold: true

null_model <- Galton %>%
  lm(child ~ 1, data = .) %>%
  augment(Galton)
lin_model <- Galton %>%
  lm(child ~ parent, data = .) %>%
  augment(Galton)
models <- bind_rows(null_model, lin_model) %>%
  mutate(model = rep(c("Null model", "SLR model"), each = nrow(Galton)))

ggplot(data = models, aes(x = parent, y = child)) +
  geom_smooth(
    data = filter(models, model == "Null model"),
    method = "lm", se = FALSE, formula = y ~ 1, size = 0.5
  ) +
  geom_smooth(
    data = filter(models, model == "SLR model"),
    method = "lm", se = FALSE, formula = y ~ x, size = 0.5
  ) +
  geom_segment(
    aes(xend = parent, yend = .fitted),
    arrow = arrow(length = unit(0.1, "cm")),
    size = 0.3, color = "darkgray"
  ) +
  geom_point(alpha = .2) +
  facet_wrap(~model) +
  xlab("Parent height (in)") +
  ylab("Child height (in)")
```

## What are we testing?

- The null model is a model with no predictors, i.e. $y = \beta_0 + \epsilon$
- The linear model is a model with one predictor, i.e. $y = \beta_0 + \beta_1 x + \epsilon$
- We use the t-test to compare the two models:

$$ t = \frac{estimate - 0}{Standard\ error} = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)} $$ where $SE(\hat{\beta}_1)$ is the standard error of the slope estimate:

$$ SE(\hat{\beta}_1) = \frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}} $$

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 10-11

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- We can also use the `Estimate` values to write the equation of the regression line:
$$ \widehat{child} = 23.94153 + 0.64629 \cdot parent$$

- For every one-inch increase in the parent height, the child height is predicted to increase by 0.64629 inches.
- e.g. if a parent is 70 inches, how tall will the child be?

. . .

If the parent is 70 inches (177.8 cm), the child will be: $23.94153 + 0.64629 \cdot 70 = 68.5$ inches (174 cm).

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 15-15

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- `Residual standard error`: the standard deviation of the residuals. 
  -  Interpretation: the average amount that the response will *deviate* from the true regression line.
-  `degrees of freedom`: the number of observations minus the number of parameters being estimated. Used in hypothesis testing and calculating the standard error of the regression coefficients.

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 16-16

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- `Multiple R-squared`: the proportion of variance (0-1) explained by the model (for simple linear regression).
- `Adjusted R-squared`: the proportion of variance (0-1) explained by the model, adjusted for the number of predictors (for multiple linear regression).
- Ranges from 0 to 1; R^2^ = 1 is a perfect fit.
- "The proportion of variance in the response that is explained by `parent`: 21.05%."

## Interpreting the output

```{r}
#| eval: false
#| code-line-numbers: 17-17

Call:
lm(formula = child ~ parent, data = Galton)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8050 -1.3661  0.0487  1.6339  5.9264 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 23.94153    2.81088   8.517   <2e-16 ***
parent       0.64629    0.04114  15.711   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.239 on 926 degrees of freedom
Multiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 
F-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16
```

- `F-statistic`: the ratio of the variance explained by predictors, and the residual variance (variance not explained by predictors).
  - Also known as the partial F-test between the full model and the intercept-only (null) model.
- `p-value`: the probability that the F-statistic is greater than the observed value under the null hypothesis.
  - A significant p-value indicates that the linear model is a better fit than the intercept-only model.

## Reporting
> Galton collected data on the heights of parents and their children.

> There appeared to be a moderate, positive, linear relationship between parent and child height.

> We fitted a linear model to predict child height from parent height (formula: child ~ parent), and model assumptions were met.

> The model was statistically significant (R^2^ = 0.21, F(1, 926) = 246.84, p < .001), hence the null hypothesis was rejected in favour of the linear model.

> Parent height/the model explained 21.05% of the variance in child height.

> The effect of parent height is statistically significant and positive ($\beta$ = 0.65, t(926) = 15.71, p < .001).

> For every one-inch increase in parent height, child height is predicted to increase by 0.65 inches.

# Let's practice
Can we predict the weight of an alligator from its length?
[Download data ⬇](https://canvas.sydney.edu.au/courses/46921/pages/week-10-lectures-linear-functions?module_item_id=1752336)

![](images/alligator.jpg){width=70%}

Photo by <a href="https://unsplash.com/@eyedealstuff?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Shelly Collins</a>

## Explore

Read the data:

```{r}
library(readxl) # load the readxl package

alligator <- read_excel(path = "data/ENVX1002_Lecture_wk10_data.xlsx", 
  sheet = "Alligator") # read in the data
```

What does the data look like?

```{r}
str(alligator)
```

## Plot

::: {.panel-tabset}
## Using base R

```{r}
plot(x = alligator$Length, y = alligator$Weight, 
  xlab = "Length (cm)", ylab = "Weight (kg)")
```

## Using `ggplot2`

```{r}
library(ggplot2) # load the ggplot2 package
ggplot(data = alligator, aes(x = Length, y = Weight)) +
  geom_point() +
  labs(x = "Length (cm)", y = "Weight (kg)")
```

:::

## Plot residual diagnostics

To check assumptions, we need to fit the model first, then plot the model.

```{r}
fit <- lm(formula = Weight ~ Length, data = alligator)
par(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots
plot(fit)
```

## Check assumptions

### Is the relationship linear?

```{r}
plot(fit, which = 1)
```

If the linearity assumption is not met, there is no reason to validate the model since it is no longer suitable for the data.

## Dealing with non-linearity: transform the data

```{r}
#| code-fold: true

library(patchwork)

p1 <- ggplot(data = alligator, aes(x = Length, y = Weight)) +
  geom_point(size = 3) +
  labs(x = "Length (cm)", y = "Weight (kg)", title = "Original") +
  geom_smooth(se = FALSE, linetype = 2)

p2 <- ggplot(data = alligator, aes(x = Length, y = sqrt(Weight))) +
  geom_point(size = 3) +
  labs(x = "Length (cm)", y = "sqrt[Weight (kg)]", title = "Square root") +
  geom_smooth(se = FALSE, linetype = 2)

p3 <- ggplot(data = alligator, aes(x = Length, y = log(Weight))) +
  geom_point(size = 3) +
  labs(x = "Length (cm)", y = "log[Weight (kg)]", title = "Natural log") +
  geom_smooth(se = FALSE, linetype = 2)

p4 <- ggplot(data = alligator, aes(x = Length, y = log10(Weight))) +
  geom_point(size = 3) +
  labs(x = "Length (cm)", y = "log10[Weight (kg)]", title = "Log base 10") +
  geom_smooth(se = FALSE, linetype = 2)

p1 + p2 + p3 + p4
```

## Natural log transformation -- Check assumptions again

```{r}
fit <- lm(formula = log(Weight) ~ Length, data = alligator)
par(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots
plot(fit)
```

## Interpretation

```{r}
summary(fit)
```

- `Length` is a statistically significant predictor of `log(Weight)` (p < .001).
- The model explains a statistically significant and large proportion (96%) of variance (R^2^ = 0.96, F(1, 23) = 553, p < .001)
- For every 1 cm increase in `Length`, `log(Weight)` increases by 0.0354 and `Weight` increases by 3.54%
  - Note: only natural log transformations can be interpreted as 'percent change in y for 1% change in x'.

## Summary
### Workflow

1.  **Understand the variables**

> Which is the response variable? Is there a reason to believe a causal relationship?

2.  **Explore data**

> How many observations? Summary statistics? Scatterplot and correlation?

3.  **Fit model**

4.  **Check assumptions**

> Remember - it's about the residuals! If assumptions fail, try transforming and return to Step 3. If assumptions *still* fail, consider another model and return to Step 3.
  
5.  **Assess model fit**

> Hypothesis test, significance, F-statistic, p-value. R^2^, how much model variation was explained by the model.
  
6.  **Interpret output**

> 'For every one-unit increase in x, y increases by $\beta_1$ units...' and any additional research/insight.

# Thanks!

This presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].

<!-- Links -->
[cc-by]: http://creativecommons.org/licenses/by/4.0/