---
title: "L11 MLR -- Abalone Quiz"
author: Si Yang Han
format: soles-revealjs
embed-resources: true
---

```{r setup, include=FALSE}
if (!require("pacman")) install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(tidyverse, cowplot, HistData, datasauRus, patchwork, broom, remotes, corrplot, psych, plotly)
pacman::p_load_gh("datalorax/equatiomatic")

ggplot2::theme_set(cowplot::theme_half_open())
library(dplyr)
```

# Abalone Quiz

![](images/abalone.jpg)

> Pop quiz! (No marks, just check your understanding. It's...tricky.)

> This dataset records abalone from the coast of Tasmania, Australia (Nash, 1995) and was accessed from the [UCI Machine Learning Repository]<https://archive.ics.uci.edu/dataset/1/abalone>.

## Introduction

Abalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. Researchers measured 9 attributes of the abalone: `sex`, `length`, `diameter`, `height`, `whole`, `shucked`, `viscera`, `shell`, and `rings`.

Note: `whole`, `shucked`, `viscera` and `shell` are weight measurements.

. . .

**What is the response variable?**

a) length
b) rings
c) shell (weight)
c) whole (weight)

. . .

> Reading comprehension :)

## Research question {auto-animate="true"} 

Abalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. Researchers measured 9 attributes of the abalone: `sex`, `length`, `diameter`, `height`, `whole`, `shucked`, `viscera`, `shell`, and `rings`.

Note: `whole`, `shucked`, `viscera` and `shell` are weight measurements.

**What is the best research question, based on the context above?**

a) Is there a correlation between abalone age and weight?
b) Can abalone weight be predicted from other measured variables?
c) Is there a relationship between abalone size and age?
d) Can age be measured by size?

. . .

> A is not a complete answer, there are many more predictors. B is incorrect, we care about age/rings. D is incorrect, we are trying to *model or predict abalone age from size* -- terminology matters.

## Explore data {auto-animate="true"}

We sample the data to make it easier to visualise relationships. We also remove the `sex` variable because it is not numeric.

```{r}
abalone <- read.csv("data/abalone.csv")

set.seed(1113)          # reproducible randomness
abalone <- abalone %>% 
  select(-sex) %>%      # remove `sex` because it is categorical
  sample_n(100)         # sample 100 observations for cleaner curve
  
str(abalone)
```

---

```{r}
psych::pairs.panels(abalone)     # visualise relationships
```

:::{.columns}
:::{.column width="70%"}

**What is the most correlated predictor with (number of) abalone rings?**

a) age  
b) length  
c) whole (weight)  
d) shell (weight)

:::

:::{.column width="30%"}

:::{.fragment .fade-in}
> Age is a trick - it is not a predictor! The answer is **`shell` (weight)**.

:::

:::
:::

---

:::{.columns}
:::{.column width="60%"}

```{r}
#| fig.width: 8
#| fig.height: 6

ggplot(abalone, aes(x = rings, y = shell)) +
  geom_point(size = 3) +
  labs(x = "Rings", y = "Shell (weight)") +
  theme(text = element_text(size = 16))

cor(abalone) |> round(2) |> print() # visualise relationships

```

:::
:::{.column width="40%"}

**Which assumption/s do we need to be wary of?**

a) linearity
b) collinearity
c) equal variances
d) all of the above

:::{.fragment .fade-in}
> In the `ring` plot, the relationship is not clearly linear and there is fanning (unlikely equal variances met). There is also very high correlation between some predictors (e.g. `length` and `diameter`), so the answer is D.

:::

:::
:::

## Fit a model {auto-animate="true"}

:::{.columns}
:::{.column width="50%"}
We use natural log transformation on the response variable with `log()` to account for non-linear relationships.

```{r}
fit <- lm(log(rings) ~ ., data = abalone)
summary(fit)
```

:::
:::{.column width="50%"}

**Which predictor is NOT significant to the model?**

a) height
b) whole (weight)
c) shucked (weight)
d) viscera (weight)

:::{.fragment .fade-in}
> Whole (weight) has a period (.) beside the p-value -- this means the value is less than 0.10, but it needs to be <0.05 to be considered significant.

:::

:::
:::


## Fit a model

```r
Residual standard error: 0.1996 on 92 degrees of freedom
Multiple R-squared:  0.6187,    Adjusted R-squared:  0.5897 
F-statistic: 21.32 on 7 and 92 DF,  p-value: < 2.2e-16
```

**We determine model fit with:**

a) Multiple R-squared and p-value
b) Adjusted R-squared and p-value
c) Adjusted R-squared and residual standard error
d) Multiple R-squared and residual standard error

. . .

> We have multiple variables, so we use the Adjusted R-squared. The p-value tests the hypothesis on whether the model should be used at all, in favour of the mean. The residual error is a measure of model fit.

## The problem with using too many predictors

Here, the model is fit with all predictors, then the least significant predictor is removed. This process is repeated until only one predictor remains.

```{r}
#| code-fold: true

library(broom)

full7 <- lm(log(rings) ~ ., data = abalone)
part6 <- update(full7, . ~ . - whole)
part5 <- update(part6, . ~ . - viscera)
part4 <- update(part5, . ~ . - length)
part3 <- update(part4, . ~ . - height)
part2 <- update(part3, . ~ . - diameter)
part1 <- update(part2, . ~ . - shucked)

formulas <- c(part1$call$formula, 
              part2$call$formula, 
              part3$call$formula, 
              part4$call$formula, 
              part5$call$formula, 
              part6$call$formula, 
              full7$call$formula)

rs <- bind_rows(glance(part1),
                glance(part2),
                glance(part3),
                glance(part4),
                glance(part5),
                glance(part6),
                glance(full7)) %>%
  mutate(Model = formulas, n = 1:7) %>%
  select(Model, n, r.squared, adj.r.squared) %>%
  mutate_if(is.numeric, round, 3)

knitr::kable(rs)
```

::: {.columns}
:::{.column width="40%"}

**Considering only $R^2$, which model would we choose?**

a) Model with 1 predictor
b) Model with 3 predictors
c) Model with 4 predictors
d) Model with 7 predictors
:::
:::{.column width="60%"}
:::{.fragment .fade-in}

> The 1-predictor model sacrifices 14.5% of variation in the response (too much). The 7-predictor model is overfitted (worse than 4-predictor model). Between 3 and 4-predictor models - is a 0.7% improvement worth having to measure `height`? Realistically, the models with 2 or 3 predictors are justifiable.

:::

:::
:::

## Interpretation

```r
#| eval: false
Call:
lm(formula = log(rings) ~ diameter + shucked + shell, data = abalone)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.30290 -0.15469 -0.03485  0.11454  0.64573 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.4122     0.1594   8.859 4.19e-14 ***
diameter      2.0346     0.6034   3.372  0.00108 ** 
shucked      -1.3339     0.2152  -6.200 1.42e-08 ***
shell         2.0486     0.3672   5.579 2.23e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

```

::: {.columns}
:::{.column width="80%"}

**Which is the correct equation?**'

a) `log(rings)` = 1.41 + 2.04 x `diameter` - 1.33 x `shucked` + 2.04 x `shell`
b) `log(rings)` = 1.41 + 2.03 x `diameter` - 1.33 x `shucked` + 2.04 x `shell`
c) `log(rings)` = 1.41 + 2.04 x `diameter` + 1.33 x `shucked` + 2.05 x `shell`
d) `log(rings)` = 1.41 + 2.03 x `diameter` - 1.33 x `shucked` + 2.05 x `shell`
:::
:::{.column width="20%"}
:::{.fragment .fade-in}

> Attention to detail :)

:::

:::
:::

## Interpretation

The equation of our model is:

**`log(rings)` = 1.41 + 2.03 x `diameter` + -1.33 x `shucked` + 2.05 x `shell`**

Below are three statements. *Given all other predictors are held constant:*

- `rings` changes by $e^{-1.33}$ for every percent increase in `shucked` (weight)
- `log(rings)` changes by 1.33 for every unit increase in `shucked` (weight)
- `log(rings)` changes by approximately 1.33% for every percent increase in `shucked` (weight)

::: {.columns}
:::{.column width="40%"}

**How many statements are correct?**

a) none
b) 1 statement
c) 2 statements
d) all of them

:::
:::{.column width="60%"}
:::{.fragment .fade-in}

> The first two are correct, the third is not. The natural log percent change appoximation only applies to small $\beta$ values below |0.25|.

:::

:::
:::

## The most important question {auto-animate="true"}

**How do you feel about regression so far?**

a) Easy
b) OK
c) Hard
d) SOS

# Good work!

This presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].

<!-- Links -->
[cc-by]: http://creativecommons.org/licenses/by/4.0/
