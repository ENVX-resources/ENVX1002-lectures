---
title: "abalone-quiz"
author: Si Yang Han
# format: soles-revealjs
embed-resources: true
---

```{r setup, include=FALSE}
if (!require("pacman")) install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(tidyverse, cowplot, HistData, datasauRus, patchwork, broom, remotes, corrplot, psych, plotly)
pacman::p_load_gh("datalorax/equatiomatic")

ggplot2::theme_set(cowplot::theme_half_open())
library(dplyr)
```

# Abalone Quiz

![](images/abalone.jpg)

> Pop quiz! (No marks, just check your understanding.)

> This dataset records abalone from the coast of Tasmania, Australia (Nash, 1995) and was accessed from the [UCI Machine Learning Repository]<https://archive.ics.uci.edu/dataset/1/abalone>.

## Introduction

Abalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. It is also not possible if the abalone is sold live (not dried).

The dataset contains 9 variables: `sex`, `length`, `diameter`, `height`, `whole`, `shucked`, `viscera`, `shell`, and `rings`.

Note: `whole`, `shucked`, `viscera` and `shell` are weight measurements.

. . .

**What is the response variable?**

a) length
b) rings
c) shell (weight)
c) whole (weight)

. . .

:::{.callout-note}
A reading comprehension question :)
:::

## Research question {auto-animate="true"} 

Abalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. It is also not possible if the abalone is sold live (not dried).

The dataset contains 9 variables: `sex`, `length`, `diameter`, `height`, `whole`, `shucked`, `viscera`, `shell`, and `rings`.

Note: `whole`, `shucked`, `viscera` and `shell` are weight measurements.

**What is the best research question, based on the context above?**

a) Is there a correlation between abalone age and whole weight?
b) Can abalone weight be predicted from other measured variables?
c) Can abalone age be predicted from more easily measured variables?
d) Is there a relationship between abalone size and age?

. . .

:::{.callout-note}
B is incorrect - we care about age/rings. A is not a complete answer - there are many more predictors. D is close, but C is the best response to the context provided!
:::

## Explore data {auto-animate="true"}

We sample the data to make it easier to visualise relationships. We also remove the `sex` variable because it is not numeric.

```{r}
abalone <- read.csv("data/abalone.csv")

set.seed(1113)          # reproducible randomness
abalone <- abalone %>% 
  select(-sex) %>%      # remove `sex` because it is categorical
  sample_n(100)         # sample 100 observations for cleaner curve
  
str(abalone)
```

---

```{r}
psych::pairs.panels(abalone)     # visualise relationships
```

**What is the most correlated predictor with (number of) abalone rings?**

a) age
b) length
c) whole (weight)
d) height

. . .

:::{.callout-note}
Age is a trick - it is not a predictor (in the dataset)! Be sure you're looking in the right column, the answer is `height`.
:::

---

```{r}
psych::pairs.panels(abalone)     # visualise relationships
```

**Which assumption/s do we need to be wary of?**

a) linearity
b) collinearity
c) equal variances
d) all of the above

. . .

:::{.callout-note}
Focus first on the rings plots - the relationship is not linear, and there is fanning (as the number of rings increases, the other measurements become more variable). There is very high correlation between some of the predictors (e.g. `length` and `diameter`), so the answer is D.
:::

## Fit a model {auto-animate="true"}

We use natural log transformation on the response variable with `log()` to account for non-linear relationships.

```{r}
fit <- lm(log(rings) ~ ., data = abalone)
summary(fit)
```

**Which predictor is NOT significant to the model?**

a) height
b) whole (weight)
c) shucked (weight)
d) viscera (weight)

. . .

:::{.callout-note}
Whole (weight) has a period (.) beside the p-value - this means the value is less than 0.10, but it needs to be <0.05 to be considered significant.
:::

## Fit a model

```{r}
#| eval: false
Residual standard error: 0.1996 on 92 degrees of freedom
Multiple R-squared:  0.6187,    Adjusted R-squared:  0.5897 
F-statistic: 21.32 on 7 and 92 DF,  p-value: < 2.2e-16
```

**We determine model fit with:**

a) Multiple R-squared and p-value
b) Adjusted R-squared and p-value
c) Adjusted R-squared and residual standard error
d) Multiple R-squared and residual standard error

. . .

:::{.callout-note}
We have multiple variables, so we use the Adjusted R-squared. The p-value tests the hypothesis on whether the model should be used at all, in favour of the mean. The residual error is a measure of model fit.
:::

## The problem with using too many predictors

Here, the model is fit with all predictors, then the least significant predictor is removed. This process is repeated until only one predictor remains.

```{r}
#| code-fold: true

library(broom)

full7 <- lm(log(rings) ~ ., data = abalone)
part6 <- update(full7, . ~ . - diameter)
part5 <- update(part6, . ~ . - shell)
part4 <- update(part5, . ~ . - length)
part3 <- update(part4, . ~ . - viscera)
part2 <- update(part3, . ~ . - whole)
part1 <- update(part2, . ~ . - shucked)

formulas <- c(part1$call$formula, 
              part2$call$formula, 
              part3$call$formula, 
              part4$call$formula, 
              part5$call$formula, 
              part6$call$formula, 
              full7$call$formula)

rs <- bind_rows(glance(part1),
                glance(part2),
                glance(part3),
                glance(part4),
                glance(part5),
                glance(part6),
                glance(full7)) %>%
  mutate(Model = formulas, n = 1:7) %>%
  select(Model, n, r.squared, adj.r.squared) %>%
  mutate_if(is.numeric, round, 3)

knitr::kable(rs)
```

**Considering only $R^2$, which model would we choose?**

a) Model with 1 predictor
b) Model with 4 predictors
c) Model with 6 predictors
d) Model with 7 predictors

. . .

:::{.callout-note}
The 1-predictor model sacrifices 13.5% of variation in the response variable, which is too harsh. The 7-predictor model is overfitted - it's actually worse than the 6-predictor model. Between the 4 and 6-predictor models - is a 0.015 or 1.5% improvement worth 2 extra predictors? Imagine being the one to weigh abalone shells and measure abalone length! Realistically, the models with 3 or 4 predictors could be justified.
:::

## Interpretation

```{r}
fit <- lm(log(rings) ~ height + whole + shucked + viscera, data = abalone)
summary(fit)
```

**Which is the correct equation?**'

a) `log(rings)` = 1.37 + 7.13 x `height` + 1.05 x `whole` + -1.83 x `shucked` + -1.69 x `viscera`
b) `log(rings)` = -0.38 + 1.05 x `height` + 7.13 x `whole` + -1.69 x `shucked` + -1.83 x `viscera`
c) `log(rings)` = 13.62 + 5.75 x `height` + 4.13 x `whole` + -4.95 x `shucked` + -2.63 x `viscera`
d) `log(rings)` = 1.38 + 7.14 x `height` + 1.05 x `whole` + -1.83 x `shucked` + -1.69 x `viscera`

. . .

:::{.callout-note}
Attention to detail is all that is required here. Round decimal places correctly!
:::

## Interpretation

The equation of our model is:

**log(rings) = 1.38 + 7.14 x height + 1.05 x whole + -1.83 x shucked + -1.69 x viscera**

**Which TWO statements are correct?**

a) rings increases by 7.14 for every unit increase in height, given all other predictors are held constant.
b) log(rings) increases by 7.14 for every unit increase in height, given all other predictors are held constant.
c) rings increases by 7.14% for every unit increase in height, given all other predictors are held constant.
d) log(rings) increases by 7.14% for every percent increase in height, given all other predictors are held constant.

. . .

:::{.callout-note}
The answers are B and C. `rings` does not increase by 7.14 units, but `log(rings)` does. Because we used a natural log transformation, `rings` increases by the equivalent *percentage*.
:::

## The most important question {auto-animate="true"}

**How do you feel about regression so far?**

a) Easy
b) OK
c) Hard
d) SOS

# Good work!

This presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].

<!-- Links -->
[cc-by]: http://creativecommons.org/licenses/by/4.0/
