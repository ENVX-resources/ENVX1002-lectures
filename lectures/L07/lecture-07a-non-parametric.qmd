---
title: Topic 7 -- Non-parametric tests
author: Januar Harianto

---

```{r setup}
#| include: false
library(tidyverse)

library(cowplot)
library(gt)
theme_set(theme_cowplot())
```

# Evaluation task (Week 8)

- Testing materials in weeks 2, 5 and 6 (excludes this week).
- See this [Ed announcement](https://edstem.org/au/courses/14893/discussion/1861580) for all information.
- **No GenAI**: we will check.
- Practice before the test using the [Practice Assignment on Canvas](https://canvas.sydney.edu.au/courses/55713/assignments/498760)

# Parametric vs non-parametric methods

## Overview

### Parametric methods

Depends on the assumption that the data is normally distributed with mean $\mu$ and standard deviation $\sigma$ ,e.g. $t$-test, ANOVA, linear regression.

::: fragment
### Non-parametric methods

Do **not** make *any* assumptions about the distribution of the data. 

**Uses other properties** e.g. ranking of the data, e.g. Wilcoxon signed-rank test, Mann-Whitney U test, Kruskal-Wallis test.
:::


## Rank-based tests

### General idea

- Rank the data e.g., from smallest to largest.
- Replace the data with their ranks.
- Perform the test on the ranks.

## It's *kind of* like a transformation...

. . .

For the **Wilcoxon signed-rank test** suppose we have the following data:


| sample: | [12]{style="color:black"} | 10 | 8 | 6 | 4 | 10 | 8 | 6 | 10 |
|--------|----|----|----|---|---|---|---|----|----|

. . .

We arrange the data in ascending order (*similar values are given the same colour for illustration*):

| ordered: | [4]{style="color:blue"} | [6]{style="color:orange"} | [6]{style="color:orange"} | [8]{style="color:green"} | [8]{style="color:green"} | [10]{style="color:red"} | [10]{style="color:red"} | [10]{style="color:red"} | [12]{style="color:violet"} |
|--------|----|----|----|---|---|---|---|----|----|

. . .

Then, we rank the data:

| ordered ranks: | [1]{style="color:blue"} | [2]{style="color:orange"} | [3]{style="color:orange"} | [4]{style="color:green"} | [5]{style="color:green"} | [6]{style="color:red"} | [7]{style="color:red"} | [8]{style="color:red"} | [9]{style="color:violet"} |
|--------|----|----|----|---|---|---|---|----|----|

. . .

Finally, ranks that are *tied* are given the average rank:

| final rank: | [1]{style="color:blue"} | [2.5]{style="color:orange"} | [2.5]{style="color:orange"} | [4.5]{style="color:green"} | [4.5]{style="color:green"} | [7]{style="color:red"} | [7]{style="color:red"} | [7]{style="color:red"} | [9]{style="color:violet"} |
|--------|----|----|----|---|---|---|---|----|----|

**These ranks are then used to perform the test, instead of the original data.**


# Use case


## Two-sample $t$-test

Consider two sets of **identical** data that compares between a group **A** and **B**, where one contains an outlier.


:::: columns
::: column
**Data:**

```{r}
#| code-fold: true

library(tidyverse)

# for display only
data.frame(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
           B = c(7, 8, 9, 10, 11, 12, 13, 14, 15, 16)) %>%
  gt()
```
:::

::: column
**Data *with* outlier:**

```{r}
#| code-fold: true

data.frame(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
           B = c(7, 8, 9, 10, 11, 12, 13, 14, 15, 200)) %>%
  gt()
```
:::
::::

## Should there be a difference?

Without the outlier, the data would have been normally distributed.

```{r}
#| code-fold: true

df <- data.frame(
  group = rep(c("A", "B"), each = 10),
  response = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
            7, 8, 9, 10, 11, 12, 13, 14, 15, 16))

df_with_outlier <- data.frame(
  group = rep(c("A", "B"), each = 10),
  response = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
            7, 8, 9, 10, 11, 12, 13, 14, 15, 200))

ggplot(df, aes(x = response, y = group)) +
  geom_boxplot()
```



## Outlier

The same data, but with a single outlier in group **B**:

```{r}
#| code-fold: true

ggplot(df_with_outlier, aes(x = response, y = group)) +
  geom_boxplot()
```

## Analysis

If we perform $t$-tests on both data sets, we get the following results:

:::: columns
::: column
```{r}
fit <- t.test(response ~ group, data = df)
fit
```

Results indicate that there is a statistically significant difference between the two groups (t~18~ = -4.4, p < 0.05).
:::
::: column
```{r}
fit2 <- t.test(response ~ group, data = df_with_outlier)
fit2

```

Results indicate that the two groups are **not** significantly different (t~18~ = -2.1, p = `{r} round(fit2$p.value, 2)`).
:::
::::

. . .

The real difference between the two groups is *obscured* by the outlier. **Type II error** (false negative)?



# Non-parametric alternatives

## When to use

1. If assumptions are met (normality, homogeneity of variance), use parametric tests as they are more powerful and efficient than non-parametric tests.
2. If the normality assumption is violated, transform the data and check for normality again (*optional*).
3. **Non-parametric tests are a good way to deal with circumstances in which parametric tests perform "poorly".**

## What to use


```{r}
#| code-fold: true
library(tidyverse)
library(gt)

df <- tibble(
  parametric = c("One-sample t-test", "Two-sample t-test", "ANOVA", "Pearson's correlation"),
  non_parametric = c("Wilcoxon signed-rank test", "Mann-Whitney U test", "Kruskal-Wallis test", "Spearman's rank correlation"))

gt(df) %>%
  cols_label(parametric = "Parametric tests", non_parametric = "Non-parametric counterpart") %>%
  tab_options(
    table.font.size = px(24),
    column_labels.font.weight = "bold")

```

All of the non-parametric techniques above convert the data into **ranks** before performing the test.

. . .

::: callout-note
We will focus on the **Wilcoxon signed-rank test** and the **Mann-Whitney U test**.
:::


# Wilcoxon signed-rank test

Altenrative to the one-sample $t$-test and the paired $t$-test.

## Overview

The Wilcoxon signed-rank test is a non-parametric test used to compare two related samples, matched pairs, or repeated measures on a single sample.

. . .

Is an alternative to:

- One-sample $t$-test
- Paired $t$-test


## Assumptions

. . .

- Data comes from the same population
- Data are randomly and independently sampled

. . .

Basically, used in same situations as the one-sample or paired $t$-test, but when the data is not normally distributed but **still symmetric**.


## Calculating ranks

If comparing two groups, the ranks are calculated as follows:

::: incremental
1. Calculate the difference $D$ between the two groups.
2. Rank the absolute values of the differences in ascending order.
3. Assign the sign of the difference to the rank.
4. Sum the ranks for each group -- **zero differences are ignored**.
:::

. . .

::: callout-note
See Slide 5 to recall how ranks are calculated, but we will show another example in the next slide.
:::

# Example: Paired data

## Weight gain

We measured weight gain in chickens before and after a diet.

```{r}
#| code-fold: true

weight <- c(2.5, 3.5, 3.5, 3.4)
weight_after <- c(4, 5, 5, 4.6)

df <- tibble(
  chicken = 1:4,
  weight = weight,
  weight_after = weight_after)

gt(df) %>%
  tab_options(
    table.font.size = px(24),
    column_labels.font.weight = "bold")
```

### Is there a significant increase in weight gain after the diet?

## Rank values

```{r}
#| code-fold: true

df %>%
  mutate(D = weight_after - weight) %>%
  mutate(Sign = ifelse(D > 0, "+", "-")) %>%
  mutate(rank = rank(abs(D))) %>%
  mutate("Signed rank" = ifelse(D > 0, rank, -rank)) %>%
  gt() %>%
  tab_options(
    table.font.size = px(24),
    column_labels.font.weight = "bold")
```

::: callout-note
The order of the ranks is based on the **absolute** values of the differences; the signs are assigned afterward.
:::

## Hypothesis

> Is there a significant increase in weight gain after the diet?

$$H_0: \mu_{\text{before}} = \mu_{\text{after}}$$
$$H_1: \mu_{\text{before}} < \mu_{\text{after}}$$


. . .

In words: 

- $H_0$: There is no difference in weight gain before and after the diet.
- $H_1$: There is an increase in weight gain after the diet.

---

Alternatively, since the data is paired, we may also consider hypotheses based on the differences between the two groups:

$$H_0: \mu_D = 0$$
$$H_1: \mu_D > 0$$

where $D$ is the difference between the two groups.


## Assumptions

```{r}
#| code-fold: true

p1 <- ggplot(df, aes(x = weight_after - weight)) +
  geom_histogram(bins = 10)
p2 <- ggplot(df, aes(sample = weight_after - weight)) +
  stat_qq() +
  stat_qq_line()
p3 <- ggplot(df, aes(x = "", y = weight_after - weight)) +
  geom_boxplot()

library(patchwork)
p1 + p2 + p3
```

With so few data points, we may want to use a formal test to check for normality.

## Assumptions

```{r}
#| code-fold: false

shapiro.test(df$weight_after - df$weight)
```

Results indicate that the data significantly deviates from normality (W = 0.63, p < 0.05). We will use the Wilcoxon signed-rank test.

## Performing the test
### In R

```{r}
#| code-fold: false

wilcox.test(x = df$weight_after, y = df$weight,  # data must be x - y
  alternative = "greater",  # because we are testing for an increase
  paired = TRUE)            # because the data is paired
```


where `V` is the sum of the signed ranks.


::: fragment
The results indicate that there is a **significant increase in weight gain** after the diet (V = 10, p < 0.05). 
:::

# Example: One-sample data

## Beetle consumption in lizards

Researchers investigated differences in beetle
consumption between two size classes of eastern horned lizard (*Phrynosoma
douglassi brevirostre*) 


- **Larger class**: adult females.
- **Smaller class**: adult males, yearling females.

. . .

**Focusing on just the smaller size class** (for now) -- it was hypothesised that this size class would eat a minimum of 100 beetles per day.

## Hypothesis

Does the average smaller size class lizard eat about 100 beetles per day?

$$H_0: \mu = 100$$
$$H_1: \mu \neq 100$$

### Dataset [Download](data/beetle.csv)

```{r}
beetle <- read_csv("data/beetle.csv")
glimpse(beetle)
```

## First, check assumptions

```{r}
#| code-fold: true

p1 <- ggplot(beetle, aes(x = BEETLES)) +
  geom_histogram(bins = 10)
p2 <- ggplot(beetle, aes(sample = BEETLES)) +
  stat_qq() +
  stat_qq_line()
p3 <- ggplot(beetle, aes(x = "", y = BEETLES)) +
  geom_boxplot()

library(patchwork)
p1 + p2 + p3
```

Is it normally distributed?

## Run the test

The Wilcoxon signed-rank test for one sample can be performed as follows:

```{r}
#| code-fold: false

beetle %>%
  filter(SIZE == "small") %>%  # filter only the smaller size class
  pull(BEETLES) %>%            # convert to a vector using pull()
  wilcox.test(mu = 100)        # wilcox.text(x, mu)
              
```

. . .

Results indicate that the average number of beetles consumed by the smaller size class lizard is **not significantly different from 100** (V = 92, p = 0.1).

. . .

::: callout-important
We are unable to make a conclusion about effect size from non-parametric tests as the information is lost when the data is transformed into ranks.
:::


# Mann-Whitney U test

Alternative to the **two-sample $t$-test**. 

*Also called the Mann–Whitney–Wilcoxon (MWW/MWU), **Wilcoxon rank-sum test (what R calls it)**, or Wilcoxon–Mann–Whitney test*.

## About

- A **non-parametric test** used to compare two independent samples similar to the two-sample $t$-test.
- Like the Wilcoxon signed-rank test, it uses **ranks** to perform the test and does not assume normality. 
- It is also more *relaxed* in that it does not assume symmetry in the distribution of the data -- instead, it assumes that the two groups have the same shape/distribution.

# Example: Back to the lizards

## Beetle consumption in lizards

Researchers investigated differences in beetle consumption between two size classes of eastern horned lizard (*Phrynosoma douglassi brevirostre*) 

- **Larger class**: adult females.
- **Smaller class**: adult males, yearling females.

We will now compare the number of beetles consumed by the **larger** and **smaller** size classes of lizards.

## Hypotheses

> Are the number of beetles consumed by the larger and smaller size classes of lizards different?

Loosely speaking, because we are not assuming symmetry, the most appropriate summary statistic to use when comparing the two groups is the **median**.

$$H_0: median_{\text{larger}} = median_{\text{smaller}}$$
$$H_1: median_{\text{larger}} \neq median_{\text{smaller}}$$

More accurately, we are testing for a difference in the *distribution* of the two groups.

## Assumptions

```{r}
#| code-fold: true

p1 <- ggplot(beetle, aes(x = BEETLES)) +
  geom_histogram(bins = 14, position = "dodge") +
  facet_wrap(~SIZE, ncol = 1)
p2 <- ggplot(beetle, aes(x = SIZE, y = BEETLES)) +
  geom_boxplot()
p3 <- ggplot(beetle, aes(sample = BEETLES)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~SIZE, ncol = 1)

library(patchwork)
p1 + p2 + p3
```

Data does not meet the normality assumption.


## Test statistic

The same function `wilcox.test()` can be used to perform the Mann-Whitney U test.

```{r}
#| code-fold: false

wilcox.test(BEETLES ~ SIZE, data = beetle)
```

- `W` is the sum of the ranks of the *smaller* group.
- The "true location shift" is the median of the larger group minus the median of the smaller group.

::: fragment
The results indicate that the number of beetles consumed by the larger and smaller size classes of lizards is **not significantly different** (W = 329, p = 0.07).
:::


# What about transformations?

## Transform, or non-parametric?

::: incremental
- As usual, there is ongoing debate on whether to transform the data or use non-parametric tests, but the general consensus is to always prefer parametric tests and transformations when assumptions are met using those techniques.
  - e.g. [Parametric analysis of transformed data is more powerful than non-parametric analysis](https://doi.org/10.1177/001316449105100402)
- Some argue that non-parametric tests **must** be decided during experimental design and not when the data fails to meet the normality assumption: **as the decision to rank data has implications on the interpretation of the results.**
  - e.g. [Graphpad Advice: Don't automate the decision to use a nonparametric test](https://www.graphpad.com/guides/prism/latest/statistics/using_a_normality_test_to_choo.htm)
- The conventional wisdom is to **transform the data** and check for normality if the assumption is not met. If the data is *still* not normal, then use non-parametric tests (after considering the implications on interpretation).
  - Or, use bootstrapping (next week!).

:::

## Summary

- **Wilcoxon signed-rank test**: alternative to the one-sample $t$-test and paired $t$-test.
- **Mann-Whitney U test**: alternative to the two-sample $t$-test.
- **Advantages**: **Robust** to outliers, skewness, and non-normality.
- **Drawbacks**: Less powerful than parametric tests when assumptions are met, provide no insight into the size of the effect.


# Questions?

This presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].


<!-- Links -->
[cc-by]: http://creativecommons.org/licenses/by/4.0/