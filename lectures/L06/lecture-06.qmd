---
title: Topic 6 -- Two-sample *t*-tests
author: Floris van Ogtrop
format: soles-revealjs
embed-resources: true
---

# Testing differences in means
## Recap: we have one sample {auto-animate="true"}

::: fragment
**One-sample** *t*-test: compare the sample of data to a *fixed* value of interest (e.g. a hypothesised value, or a population mean).
:::

::: fragment
### Examples

-   *Is the mean height of students in ENVX different from the population mean of 170 cm?*
-   *Is the mean heart rate of students in ENVX different from the population mean of 70 bpm*

```{r}
#| code-fold: true
#| fig-height: 3.5

library(patchwork)
library(ggplot2)

set.seed(108)
heights <- rnorm(100, mean = 170, sd = 10)
heart <- rnorm(100, mean = 70, sd = 10)

# heights
p1 <- ggplot(data.frame(heights), aes(x = heights)) +
  geom_histogram(aes(y = after_stat(density)),
    binwidth = 5, fill = "lightblue", color = "black") +
  geom_vline(aes(xintercept = mean(heights)), 
    color = "red", linetype = "dashed") +
  ggtitle("Height of students") +
  theme_classic()


# heart rate
p2 <- ggplot(data.frame(heart), aes(x = heart)) +
  geom_histogram(aes(y = after_stat(density)), 
    binwidth = 5, fill = "lightblue", color = "black") +
  geom_vline(aes(xintercept = mean(heart)), 
    color = "red", linetype = "dashed") +
  ggtitle("Heart rate") +
  theme_classic()


p1 + p2


```

:::


## What if we want to compare a sample of data to *another* sample?

::: fragment
### Examples

-   *Is the mean height of students in ENVX1002 different from ENVX2001?*
-   *Is the mean heart rate of students in ENVX1002 different from ENVX2001?*

```{r}
#| code-fold: true
#| fig-height: 3.5

set.seed(129)
## Generate data for 2 groups
heights_all <- data.frame(
  group = rep(c("ENVX1002", "ENVX2001"), each = 100),
  heights = c(rnorm(100, mean = 165, sd = 7), 
    rnorm(100, mean = 185, sd = 9)))
heart_all <- data.frame(
  group = rep(c("ENVX1002", "ENVX2001"), each = 100),
  heart_rates = c(rnorm(100, mean = 70, sd = 10), 
    rnorm(100, mean = 72, sd = 5)))

## Plot
p3 <- ggplot(heights_all, aes(x = heights, fill = group)) +
  geom_histogram(aes(y = ..density..), 
    binwidth = 5, color = "black", alpha = .2) +
  geom_vline(aes(xintercept = 170), 
    color = "red", linetype = "dashed") +
  geom_vline(aes(xintercept = 185),
    color = "blue", linetype = "dashed") +
  ggtitle("Height of students (cm)") +
  theme_classic()
p4 <- ggplot(heart_all, aes(x = heart_rates, fill = group)) +
  geom_histogram(aes(y = ..density..), 
    binwidth = 5, color = "black", alpha = .2) +
  geom_vline(aes(xintercept = 70), 
    color = "red", linetype = "dashed") +
  geom_vline(aes(xintercept = 72), 
    color = "blue", linetype = "dashed") +
  ggtitle("Heart rates (bpm)") +
  theme_classic()

p3 + p4
```
:::


# Two-sample *t*-test
## Comparing two samples: visualisation

```{r}
#| echo: false
#| fig-height: 3

p3 + p4
```

::: fragment
```{r}
#| code-fold: true
#| fig-height: 3
p5 <- ggplot(heights_all, aes(x = group, y = heights, fill = group)) +
  geom_boxplot(alpha = .2) +
  ggtitle("Height of students (cm)") +
  theme_classic()
p6 <- ggplot(heart_all, aes(x = group, y = heart_rates, fill = group)) +
  geom_boxplot(alpha = .2) +
  ggtitle("Heart rates (bpm)") +
  theme_classic()

p5 + p6
```
:::

## Some considerations: the boxplot

::: fragment
```{r}
#| echo: false
#| fig-height: 3.5
p5 + p6
```
:::

::: fragment
- Trade-off between being able to see the distribution of the data and being able to compare between groups.
- The *recommended* approach when comparing two or more groups of data in most cases.
:::

## 

![][1]

  [1]: images/redbull.png


# Does Red Bull increase the heart rate of students?
## Data

*A simulated example* (data is not real):

```{r}
redbull <- data.frame(
  group = c(rep("redbull", 12), rep("control", 12)), 
  heart_rate = c(72, 88, 72, 88, 76, 75, 84, 80, 60, 96, 80,  84, 84, 76, 68, 80, 64, 62, 74, 84, 68, 96, 80, 64))
```

::: fragment
**Experimental design**: two groups of students selected at random, *without* replacement, from the ENVX1002 cohort.

-   `redbull` group: students who consumed 250 ml of Red Bull.
-   `control` group: students who consumed 250 ml of water (control group).

Heart rate in beats per minute (bpm) was measured *20 minutes after consumption*.
:::

::: fragment
### Structure of data
```{r}
str(redbull)
```

:::


# HATPC

Hypothesis \| Assumptions \| Test \| P-value \| Conclusion

## Hypothesis

For a two-sample *t*-test, the null hypothesis is that the means of the two groups are equal, and the alternative hypothesis is that the means are different.

$$H_0: \mu_{\text{redbull}} = \mu_{\text{control}}$$
$$H_1: \mu_{\text{redbull}} \neq \mu_{\text{control}}$$

::: fragment
*Compare this to the one-sample *t*-test, where the null hypothesis is that the sample mean is equal to a fixed value:*
$$H_0: \mu = \mu_0$$
$$H_1: \mu \neq \mu_0$$
:::

## Assumptions
The assumptions of the two-sample *t*-test include:

1.  **Normality**: the data are normally distributed.
2.  **Homogeneity of variance**: the variances of the two groups are equal.

::: fragment
### Why are these assumptions important?

::: incremental

- Since the *t*-test compares the means of two groups, normality ensures that the means are the *best estimate* of the population means.
- Equal variances indicates that the two groups have similar "noise" influencing their means, except for the "treatment" effect.
  - In the Red Bull example, this means that the range of heart rate values in students for both groups is similar, except for the effect of consuming Red Bull.
:::
:::


# Assumption: normality
Histogram | QQ-plot | Shapiro-Wilk test

## Normality: histogram

- We visually inspect the distribution of the data using histograms, generally for **each group**.
- Look out for: symmetry, skewness, and multimodality.
- Hard to visualise when n (sample size is small)

::: panel-tabset

## R base graphics

```{r}
#| code-fold: true
#| fig-height: 3.5

par(mfrow = c(1, 2))
hist(redbull$heart_rate[redbull$group == "redbull"], 
  main = "Red Bull group", xlab = "Heart rate (bpm)", 
  col = "lightblue", border = "black")
hist(redbull$heart_rate[redbull$group == "control"],
  main = "Control group", xlab = "Heart rate (bpm)", 
  col = "lightblue", border = "black")
```

## ggplot2
```{r}
#| code-fold: true
#| fig-height: 3.5

library(ggplot2)
ggplot(redbull, aes(x = heart_rate, fill = group)) +
    geom_histogram(aes(y = ..density..),
        binwidth = 12, color = "black", alpha = .5
    ) +
    facet_wrap(~group) +
    theme_classic()
```

## ggpubr
```{r}
#| code-fold: true
#| fig-height: 3.5

library(ggpubr)
gghistogram(redbull, x = "heart_rate", fill = "group", binwidth = 6) +
    facet_wrap(~group) +
    theme_classic()
```

:::

::: fragment
Conclusion: *The data appear to be normally distributed, but it is better to confirm this with a QQ-plot.* 
:::

## Normality: QQ-plot

- The qq-plot is a graphical method to *specifically* assess the normality of the data. Again, we look at the data for **each group**.
- Look out for: deviations from the straight line.

::: panel-tabset

## R base graphics
```{r}
#| code-fold: true
#| fig-height: 3.5

par(mfrow = c(1, 2))
qqnorm(redbull$heart_rate[redbull$group == "redbull"],
    main = "Red Bull group", xlab = "Theoretical quantiles",
    ylab = "Sample quantiles", col = "blue"
)
qqline(redbull$heart_rate[redbull$group == "redbull"], col = "red")

qqnorm(redbull$heart_rate[redbull$group == "control"],
    main = "Control group", xlab = "Theoretical quantiles",
    ylab = "Sample quantiles", col = "blue"
)
qqline(redbull$heart_rate[redbull$group == "control"], col = "red")
```

## ggplot2
```{r}
#| code-fold: true
#| fig-height: 3.5

library(ggplot2)
ggplot(redbull, aes(sample = heart_rate)) +
    stat_qq() +
    stat_qq_line() +
    facet_wrap(~group) +
    theme_classic()

```

## ggpubr
```{r}
#| code-fold: true
#| fig-height: 3.5

library(ggpubr)
ggqqplot(redbull$heart_rate) +
    facet_wrap(~ redbull$group) +
    theme_classic()
```

:::

::: fragment
Conclusion: *The data appear to be normally distributed.*
:::

## Normality: formal test

- Use the Shapiro-Wilk test which tests the null hypothesis that the data are normally distributed.
- This test is sensitive to deviations from normality in the tails of the distribution, and is suitable for small sample sizes (about 5 to 50 observations).

```{r}
#| code-fold: true
#| fig-height: 3.5

shapiro.test(redbull$heart_rate[redbull$group == "redbull"])
shapiro.test(redbull$heart_rate[redbull$group == "control"])
```

::: fragment
Conclusion: *p*-values are greater than 0.05, so we do not reject the null hypothesis of normality. The data are normally distributed.
:::

## What if the normality assumption is violated?

- The *t*-test is robust to deviations from normality, especially for large sample sizes due to the Central Limit Theorem.
- If the sample size is small, consider using a non-parametric test (e.g. the Wilcoxon rank-sum test): **next week**
- Alternatively, transform the data: **later**


# Assumption: homogeneity of variance
Boxplots | Formal tests 

## Equal variances: boxplot

- We visually inspect the spread of the data using boxplots, generally for **each group**.
- Look out for: differences in spread, outliers, and symmetry.

::: panel-tabset

## R base graphics
```{r}
#| code-fold: true
#| fig-height: 3.5

par(mfrow = c(1, 2))
boxplot(heart_rate ~ group,
    data = redbull,
    main = "Heart rate", xlab = "Group", ylab = "Heart rate (bpm)"
)
```

## ggplot2
```{r}
#| code-fold: true
#| fig-height: 3.5

library(ggplot2)
ggplot(redbull, aes(x = group, y = heart_rate, fill = group)) +
    geom_boxplot(alpha = .2) +
    labs(x = "Group", y = "Heart rate (bpm)") +
    theme_classic()
```

## ggpubr
```{r}
#| code-fold: true
#| fig-height: 3.5

library(ggpubr)
ggboxplot(redbull,
    x = "group", y = "heart_rate",
    fill = "group", alpha = .2
) +
  labs(x = "Group", y = "Heart rate (bpm)") +
  theme_classic()
```

:::

::: fragment
Conclusion: *The spread of the data appears to be similar between the two groups.*
:::

## Equal variances: formal tests

- Bartlett's and Levene's tests may be used to test the null hypothesis that the variances of the groups are equal.
- These tests are sensitive to deviations from normality (Levene's test is less so compared to Bartlett's), and are suitable for small sample sizes.

::: panel-tabset

## Levene's test
```{r}
library(car)
leveneTest(heart_rate ~ group, data = redbull)
```

## Bartlett's test
```{r}
bartlett.test(heart_rate ~ group, data = redbull)
```

:::

::: fragment
Conclusion: *p*-values are greater than 0.05, so we do not reject the null hypothesis of equal variances. The variances of the two groups are equal.
:::

## What if the equal variance assumption is violated?

Some debate exists on what to do, but choices include:

- Use the **Welch's *t*-test**, which is robust to unequal variances: *coming up next*
- **Transform** the data to stabilise the variances: **later**
- Perform a non-parametric test: **next week**

## The Welch's *t*-test

- The Welch's *t*-test is a modification of the two-sample *t*-test that **does not assume equal variances**.
- Also applicable when the **sample sizes are unequal**.

### Why not use the Welch's *t*-test all the time?

- Ongoing debate on whether to use the Welch's *t*-test or the Student's *t*-test when the variances are equal. 
- The Welch's *t*-test is generally considered more robust, and **is the default in R's `t.test()` function**.
- You can still use the Student's *t*-test by setting `var.equal = TRUE` in the `t.test()` function.

## Are the assumptions of normality and homogeneity of variance met?
When reporting in journals, it is common to simply state that the assumptions were met and what tests were used to confirm them, without showing the exact results of the tests!

::: fragment
### Example 1

> *The assumptions of normality and homogeneity of variance were met for the data (Shapiro-Wilk test, $p > 0.05$; Levene's test, $p > 0.05$). Thus, we performed a two-sample *t*-test...*
:::

::: fragment
### Example 2

> *Visual inspection of the histograms, QQ-plots, and boxplots showed that the data met the assumptions of both normality and homogeneity of variance. Thus, we performed a two-sample *t*-test..*
:::

::: fragment
**For your lab reports, you should show the results of the tests (because we want to check your work!)**
:::


# P-value and Conclusion
## Performing the *t*-test

```{r}
#| code-fold: false

t.test(heart_rate ~ group, data = redbull, var.equal = TRUE)
```

Results indicate that the means of the two groups are **not** significantly different (p = 0.27).

::: fragment
### Compare with the Welch's *t*-test
```{r}
#| code-fold: false

t.test(heart_rate ~ group, data = redbull)
```
:::

## Conclusion
Differences in heart rate we not statistically significant between the Red Bull and control groups (t~22~ = -1.1, p = 0.27) indicating that Red Bull did not significantly increase the heart rate of students sampled from ENVX1002.


# The paired *t*-test
## Are the two sample independent?
When testing if two samples are different from each other, we need to consider two possible scenarios:

::: fragment
-   **Independent samples**: The samples are drawn from two different populations, **or** the samples are not related to each other -- **independent groups**.
-   **Related samples**: The samples are drawn from the same population, **and/or** the samples are related to each other -- **repeated measures** or **matched pairs**.
:::

::: fragment
If the samples are related, a **paired *t*-test** is more appropriate than a two-sample *t*-test as it accounts for the relationship between the samples that could confound the results.
:::


## Paired *t*-test
### Experimental design (what if?)
#### Before

> Two groups of students selected at random, *without* replacement, from the ENVX1002 cohort.

::: fragment

#### Paired design
**The same student** was used in a before/after experiment, where the heart rate was measured before and after consuming 250ml of Red Bull. Twelve (12) students were selected at random from the ENVX1002 cohort.

::: incremental
- Data is no longer independent, as the same student is measured twice.
- The student now confounds the results, as the heart rate of the same student is likely to be **correlated** even without consuming Red Bull.
- Total number of students is now 12, not 24.
- **Let's assume the data collected are exactly the same.**
:::
:::

## Hypothesis
For a paired *t*-test, the null hypothesis is that the mean difference between the two groups is zero, and the alternative hypothesis is that the mean difference is different from zero.

$$H_0: \mu_{\text{diff}} = 0$$
$$H_1: \mu_{\text{diff}} \neq 0$$

::: fragment
*Compare this to the two-sample *t*-test, where the null hypothesis is that the means of the two groups are equal:*
$$H_0: \mu_{\text{redbull}} = \mu_{\text{control}}$$
$$H_1: \mu_{\text{redbull}} \neq \mu_{\text{control}}$$

:::

## Assumptions of the paired *t*-test

- The assumption of the paired *t*-test is that the differences between the two groups are normally distributed.
- There is no assumption of equal variances, as the paired *t*-test is a one-sample *t*-test on the differences.
  - Another way to think about it is that since the data are paired, the variance of the differences is the same for both groups.

## Performing the paired *t*-test

There are two ways.

:::: columns
::: column
::: fragment
### Method 1: Calculate the differences, then perform a one-sample *t*-test using `t.test()`

```{r}
#| code-fold: false

diff <- redbull$heart_rate[redbull$group == "redbull"] - 
  redbull$heart_rate[redbull$group == "control"]
t.test(diff)
```

:::
:::

::: column
::: fragment
### Method 2: Use the `t.test()` function with the `paired = TRUE` argument

```{r}
#| code-fold: false

# t.test(heart_rate ~ group, data = redbull, 
#   paired = TRUE)
```

:::
:::
::::

::: fragment
The results for both methods are identical; the mean difference is not significantly different from zero (p = 0.13).
:::


# When assumptions of the *t*-test are violated
## Recap: Assumptions of the two-sample *t*-test

### With independent samples:

- **Normality**: the data are normally distributed
- **Homogeneity of variance** (equal variances): the variances of the two groups are equal

### With paired samples:

- **Normality**: the differences between the paired samples are normally distributed
- Equal variances is implied

## If we analyse the data anyway...

The *t*-test:

- may provide incorrect results as **mean and variance calculations depend on normally distributed data**.
- may be **less powerful** (i.e., less likely to detect a true difference).
- may be **biased** (i.e., systematically over- or under-estimating the true difference).


# Don't throw the data away...
## What can we do?

The *t*-test is quite robust to violations of normality, especially when the sample size is large. However, the assumption of equal variances is more critical – we cannot simply depend on large sample sizes to "fix" the problem.

Options include:

- **Transform** the data to normalise the data and/or scale the variance
- Use a **Welch's *t*-test** or a **Welch's ANOVA** (limited cases)
- Use a **non-parametric test**, such as the **Mann-Whitney U test** or **Wilcoxon signed-rank test** (paired samples) -- however, these tests have *less power* than the *t*-test i.e. less likely to detect a true difference.


# Ants - a foraging biomass study

![*Formica rufa*, the horse ant - native to Eurasia.](images/formica_rufa.jpg) {fig-align="left"}

## Is the food collected by ants different between two sites?

### Data structure

```{r}
#| code-fold: true
library(tidyverse)
ants <- read.csv("data/ants.csv") %>%
  mutate(Tree = factor(Tree))

glimpse(ants)
```

We want to compare the mean biomass of food, collected by ants between the two sites in **dry weight (mg) of prey, divided by the total number of ants leaving the tree in 30 minutes**.

## Visualising the data

```{r}
#| code-fold: true
library(ggplot2)
p_ants <-
  ggplot(ants, aes(x = Tree, y = Food)) +
  geom_boxplot() +
  ylab("Biomass of food (mg per ant)") +
  theme_minimal()

p_ants
```

Does this data meet the assumptions of the two-sample *t*-test?

## Checking assumptions

We have some idea that the data may not be normally distributed, but are not quite sure. So let's check using the Q-Q plot.

```{r}
#| code-fold: true

ggplot(ants, aes(sample = Food)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Tree) +
  theme_minimal()
```

::: incremental
- Curvature of the data points away from the line indicates non-normality.
- Boxplots (previous slide) suggest equal variances.
- **Let's transform the data.**
:::

## Picking a transformation

We need to consider the **type of data** and the **shape of its distribution** when choosing a transformation. These can be assessed using:

::: incremental
- **Histograms** and **Q-Q plots** to assess normality - DONE
- **Box plots** to assess homogeneity of variance - DONE
- **Skewness** and **kurtosis** to assess the shape of the distribution - NEXT
:::

## Skewness

The degree of asymmetry in the data distribution when compared to a normal distribution.

::: fragment
- Represented by the **skewness coefficient** ($\gamma_1$) and can be positive, negative, or zero.
- Skewness values between **-0.5 and 0.5** are considered acceptable (fairly symmetrical).
- **Negative** skewness indicates a *left*-skewed distribution, while **positive** skewness indicates a *right*-skewed distribution.
- Above 1 or below -1, the distribution is considered ***highly* skewed**.
:::

## Example: skewness

```{r}
#| code-fold: true

library(tidyverse)
library(patchwork)

x <- seq(0, 1, length.out = 100)

# Calculate the density of the Beta distribution at these points
data1 <- data.frame(x = x, y = dbeta(x, 5, 2), dist = "Negative (left) skewed")
data2 <- data.frame(x = x, y = dbeta(x, 5, 5), dist = "Symmetrical")
data3 <- data.frame(x = x, y = dbeta(x, 2, 5), dist = "Positive (right) skewed")
data <- rbind(data1, data2, data3) %>%
  mutate(dist = factor(dist, levels = c("Negative (left) skewed", "Symmetrical", "Positive (right) skewed")))

# Create the plot
ggplot(data, aes(x = x, y = y)) +
  geom_line(color = "skyblue") +
  geom_area(fill = "skyblue", alpha = 0.4) +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) +
  facet_wrap(~dist) +
  ylab("density") +
  xlab("")
```

## Kurtosis

Used to describe the extreme values (outliers) in the distribution versus the tails.

::: fragment
- **High kurtosis (>3)** indicates a distribution with **heavy tails** and a **peaked centre**. When this happens, we should investigate the data for outliers.
- **Low kurtosis (<3)** indicates a distribution with **light tails** and a **flat centre**. There are fewer to no outliers in the data.
:::

## Example: kurtosis

```{r}
#| code-fold: true


library(tidyverse)
library(patchwork)
library(moments)

# Generate data
set.seed(123)
x1 <- seq(-10, 10, length.out = 1000)
x2 <- seq(-5, 5, length.out = 1000)
data1 <- data.frame(x = x1, y = dt(x1, df = 1), dist = "High Kurtosis")
data2 <- data.frame(x = x2, y = dt(x2, df = 10), dist = "Low Kurtosis")
data <- rbind(data1, data2) %>%
  mutate(dist = factor(dist, levels = c("High Kurtosis", "Low Kurtosis")))

# Create the plot
ggplot(data, aes(x = x, y = y)) +
  geom_line(color = "skyblue") +
  geom_area(fill = "skyblue", alpha = 0.4) +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) +
  facet_wrap(~dist, scales = "free_x") +
  ylab("density") +
  xlab("")
```

## Skewness and kurtosis in the ants data

With experience we can "eyeball" the data, but we can also calculate the skewness and kurtosis.

```{r}
ants %>%
  group_by(Tree) %>%
  summarise(skewness = skewness(Food), kurtosis = kurtosis(Food))
```

```{r}
#| echo: false
p_ants
```

::: fragment
From the results we can see that both sites have a **positive skewness**. Site `Rowan` has high kurtosis.
:::


# Data transformation

## Workflow

::: incremental
1. Check the data for normality and homogeneity of variance (i.e. **test assumptions**). 
2. If the assumptions are violated, consider **transforming ALL the data**.
3. **Repeat** checks on assumptions. If assumptions are **met**, proceed with the *t*-test on the transformed scale. *Otherwise, use a different transformation or consider using a non-parametric test.*
4. Interpret the statistical results and **back-transform the results** to the original scale (optional but recommended) to aid interpretation.
:::

## Picking a transformation

::: fragment
### For positive skewness

- **Square root** transformation: $\sqrt{x}$ for skewness between 0.5 and 1 and kurtosis < 3.
- **Logarithmic** transformation: $\log(x)$ for skewness > 1 and kurtosis < 3.
- **Reciprocal** transformation: $\frac{1}{x}$ for skewness > 1 and kurtosis > 3 (quite extreme).
:::

::: fragment
### For negative skewness
- This is rare as most biological data are positively skewed. However, you can try the **square** $x^2$ or **cube** $x^3$ transformation.
- If negatively skewed data contains zeros, consider using the log transform and adding a constant to the data before transformation e.g. $\log(x + 1)$.
:::

::: fragment
::: callout-note
There is also the **Box-Cox transformation** which informs us of the best transformation to apply to the data without the need to check skewness and kurtosis. This method is not covered in this unit, but you can read more about it [here](https://r-coder.com/box-cox-transformation-r/) (the simple R version) or [here](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation) (more detailed mathematical explanation).
:::
:::

## How do we check if the transformation worked?

We need to apply the transformation to the entire dataset and check the Q-Q plot again.

```{r}
ants$Food_log <- log(ants$Food)
```

```{r}
#| code-fold: true

# compare the Q-Q plot before and after transformation
pfood <- ggplot(ants, aes(sample = Food)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Tree) +
  ggtitle("Before transformation") +
  theme_classic()

pfoodlog <- ggplot(ants, aes(sample = Food_log)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Tree) +
  ggtitle("After transformation") +
  theme_classic()

pfood / pfoodlog
```

## Checking skewness and kurtosis after transformation

```{r}
ants %>%
  group_by(Tree) %>%
  summarise(skewness = skewness(Food_log), kurtosis = kurtosis(Food_log))
```

## Performing the *t*-test

```{r}
fit <- t.test(Food_log ~ Tree, data = ants, var.equal = TRUE)
fit
```

::: fragment
### How do we interpret the results?

Evidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045).
:::

## Back-transforming the results

- For power transformations, we can back-transform the results to the original scale using the inverse function.
- Log transformations are a bit tricky as the inverse function is the exponential function.
  - For the natural log transformation which is `log()` in R, the inverse function is the exponential function: $e^x$.
  - For the base 10 log transformation which is `log10()` in R, the inverse function is $10^x$.

## Interpretation

### Back-transforming mean values

```{r}
browan <- exp(fit$estimate[[1]]) # mean biomass from the Rowan site
bsycamore <- exp(fit$estimate[[2]]) # mean biomass from the Sycamore site

# check the ratio
bsycamore / browan
```

> Evidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045).

The mean biomass of food collected by ants from the Sycamore site (`r round(bsycamore, 1)` mg) is 1.4 times greater than the mean biomass of food collected by ants from the Rowan site (`r round(browan, 1)` mg).

### Back-transforming confidence intervals

```{r}
ant_ci <- exp(fit$conf.int)
ant_ci
```

## Comparing to a test without transformation

```{r}
fit2 <- t.test(Food ~ Tree, data = ants, var.equal = TRUE)
fit2
```

:::: columns
::: column
- Original mean values: 
  - Rowan = `r round(fit2$estimate[[1]], 1)` mg
  - Sycamore = `r round(fit2$estimate[[2]], 1)` mg
- Log-transformed mean values: 
  - Rowan = `r round(fit$estimate[[1]], 1)` lg(mg)
  - Sycamore = `r round(fit$estimate[[2]], 1)` lg(mg)
- Back-transformed mean values: 
  - **Rowan = `r round(browan, 1)` mg**
  - **Sycamore = `r round(bsycamore, 1)` mg**
  

The original mean values are based on the arithmetic mean, while the log-transformed mean values are based on the geometric mean. The geometric mean is more appropriate for skewed data.

:::

::: column
- Original 95% confidence interval: 
  - `r round(fit2$conf.int[1], 1)` to `r round(fit2$conf.int[2], 1)` mg
- Log-transformed 95% confidence interval:
  - `r round(ant_ci[1], 1)` to `r round(ant_ci[2], 1)` lg(mg)
- Back-transformed 95% confidence interval:
  - **`r round(exp(ant_ci[1]), 1)` to `r round(exp(ant_ci[2]), 1)` mg**

The influence of kurtosis on the 95% confidence interval is evident when comparing the original and back-transformed confidence intervals, as the log transform reduces the effect of outliers on the data.
:::
::::

# Thanks!

This presentation is based on the [SOLES Quarto reveal.js template] and is licensed under a [Creative Commons Attribution 4.0 International License].

[SOLES Quarto reveal.js template]: https://github.com/usyd-soles-edu/soles-revealjs
[Creative Commons Attribution 4.0 International License]: http://creativecommons.org/licenses/by/4.0/



## References

- Quinn G. P. & Keough M. J. (2002) Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK.
- Logan, M. (2010). Biostatistical design and analysis using R a practical guide. Hoboken, N.J., Wiley-Blackwell.
 
