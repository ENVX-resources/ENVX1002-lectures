{
  "hash": "005bb24647a0c949916aa680b7f416da",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 11 -- Multiple Linear Regression\nauthor: Si Yang Han\nformat: ochre-revealjs\nscrollable: true\nprefer-html: true\n---\n\n## Module overview\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n- [Week 9. Describing Relationships]{style=\"color: #D0D3D4;\"}\n    + [Correlation (calculation, interpretation)]{style=\"color: #D0D3D4;\"}\n    + [Regression (model structure, model fitting]{style=\"color: #D0D3D4;\"}\n    + [What/when/why/how]{style=\"color: #D0D3D4;\"}\n\n- [Week 10. Simple Linear Regression]{style=\"color: #D0D3D4;\"}\n    + [Can we use the model?(assumptions, hypothesis testing)]{style=\"color: #D0D3D4;\"}\n    + [How good is the model?(interpretation, model fit)]{style=\"color: #D0D3D4;\"}\n    \n- **Week 11. Multiple Linear Regression**\n    + Multiple Linear Regression (MLR) modelling\n    + Assumptions, interpretation and the principle of parsimony\n\n- [Week 12. Nonlinear Regression]{style=\"color: #D0D3D4;\"}\n    + [Common nonlinear functions]{style=\"color: #D0D3D4;\"}\n    + [Transformations]{style=\"color: #D0D3D4;\"}\n\n## Last week: simple linear regression\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$\n\nIdeal for predicting a continuous response variable from a single predictor variable: *\"How does $y$ change as $x$ changes?\"*\n\n* Identify/quantify relationships between variables\n* Predict future values\n\n. . .\n\n### What if we have more than one predictor?\n\nWhat is the model and how do we interpret the results?\n\n# Multiple linear regression\n\n> Nearly identical to simple linear regression, just more predictors!\n\n## History\n\n::: {.columns}\n::: {.column width=\"50%\"}\n![](images/galton.jpg)\n![](images/pearson.jpg)\n\n*Francis Galton and Karl Pearson*\n\n:::\n::: {.column width=\"50%\"}\n\n- First suggested by **Francis Galton** in 1886 while studying genetic variations in sweet peas over several generations\n- **Karl Pearson** developed the mathematical formula for multiple linear regression model later (early 1900s)\n\n:::\n:::\n\n> *“The somewhat complicated mathematics of multiple correlation, with its repeated appeals to the geometrical notions of hyperspace, remained a closed chamber to him.”*\n\n-- Pearson (1930), on Galton's work with MLR\n\n## Steps for Regression\n\n1.  Understand the variables\n2.  Explore data\n3.  Fit model\n4.  Check assumptions\n5.  Assess fit of model/s (parsimony)\n6.  Interpret output\n\n## Example: Air Quality in New York (1973)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(airquality)\ndplyr::glimpse(airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 153\nColumns: 6\n$ Ozone   <int> 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R <int> 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    <dbl> 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    <int> 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n```\n\n\n:::\n:::\n\n\n. . .\n\nOzone ($O_3$) is a harmful air pollutant at ground level - the main component of smog:\n\n- `Ozone`: ozone concentration (ppb)\n- `Solar.R`: solar radiation (lang, Langleys)\n- `Wind`: wind speed (mph)\n- `Temp`: ambient temperature (degrees F)\n- `Month`: month (1-12)\n- `Day`: day of the month (1-31)\n\n## Scatterplots\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(airquality)\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n## Correlations via base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(airquality, use = \"complete.obs\") |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Ozone Solar.R  Wind  Temp Month   Day\nOzone    1.00    0.35 -0.61  0.70  0.14 -0.01\nSolar.R  0.35    1.00 -0.13  0.29 -0.07 -0.06\nWind    -0.61   -0.13  1.00 -0.50 -0.19  0.05\nTemp     0.70    0.29 -0.50  1.00  0.40 -0.10\nMonth    0.14   -0.07 -0.19  0.40  1.00 -0.01\nDay     -0.01   -0.06  0.05 -0.10 -0.01  1.00\n```\n\n\n:::\n:::\n\n\n## Correlations via `corrplot`\n\n<!---\nExplain how to interpret these\n--->\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot::corrplot(cor(airquality, use = \"complete.obs\"), method = \"circle\")\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-5-1.png)\n:::\n:::\n\n\n## Correlations via `psych`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npsych::pairs.panels(airquality)\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-6-1.png)\n:::\n:::\n\n\n- What predictors could be useful to predict `Ozone`? \n\n. . .\n\n`Temp` ($r$ = 0.70), `Wind` ($r$ = -0.60) and `Solar.R` ($r$ = 0.35) are the most correlated with `Ozone`.\n\nWhat can we understand about the relationship between `Ozone` and `Temp` ($r$ = 0.70)?\n\n## Relationship {auto-animate=\"true\"} \n\nWhat can we understand about the relationship between `Ozone` and `Temp` ($r$ = 0.70)?\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(data = airquality, aes(x = Temp, y = Ozone)) +\n  geom_point() + \n  labs(x = expression(\"Temperature \" (degree~F)), y = \"Ozone (ppb)\") +\n  theme_classic()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 37 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-7-1.png)\n:::\n:::\n\n\n. . .\n\nThe higher the temperature, the higher the ozone concentration. The relationship is almost linear.\n\n## Fitting a simple model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(formula = Ozone ~ Temp, data = airquality)\n```\n:::\n\n\n- Simple linear regression between `Ozone` and `Temp`\n- This is our baseline or control model\n\n## Assumptions via base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-9-1.png)\n:::\n:::\n\n\n## Assumptions via `ggfortify` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\nggplot2::autoplot(fit) # requires ggfortify but is a ggplot2 function\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `fortify(<lm>)` was deprecated in ggplot2 4.0.0.\nℹ Please use `broom::augment(<lm>)` instead.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at <https://github.com/sinhrks/ggfortify/issues>.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at <https://github.com/sinhrks/ggfortify/issues>.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ggfortify package.\n  Please report the issue at <https://github.com/sinhrks/ggfortify/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-10-1.png)\n:::\n:::\n\n\n## Assumptions via `performance` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-11-1.png)\n:::\n:::\n\n\n## Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,\tAdjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n- `Temp` is a statistically significant predictor of `Ozone` (p < .001).\n- The (simple linear) model explains 49% of variance (R^2^ = 0.49).\n\n. . .\n\n**Can we improve the model in other ways?** Maybe - by transforming or adding more variables.\n\n## Principle of parsimony\n\n- Also known as Occam's razor;\n\n> *Entia non sunt multiplicanda praeter necessitatem.*\n> *“Entities should not be multiplied without necessity.”*\n\n- Oxford definition;\n\n> The most acceptable explanation of an occurrence, phenomenon, or event is the simplest, involving the fewest entities, assumptions, or changes.\n\n- Simple is best; i.e. if a simple (one variable) model and a complex (many variables) model predict similarly well, the simple model is preferred.\n\n. . .\n\n### A parsimonius model:\n\n- Has only *useful* predictors\n- No *redundant* predictors\n\n## The problem with using too many predictors\n\n- Generally, the more predictors we add, the better the model fits data\n- However, adding too many may cause **overfitting**, i.e. the model becomes too complex\n- An overfitted model won't be able to **generalise** to new data\n\n![](images/fit.jpg)\n\n## The multiple linear regression model {auto-animate=\"true\"}\n\nAn extension of simple linear regression to include **more than one** predictor variable: *\"How does $y$ change as $x_1$, $x_2$, ..., $x_k$ change?\"*\n\n$$ Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i $$\n\n:::{.fragment} \nTherefore, estimating the model involves *estimating the values of* $\\beta_0$, $\\beta_1$, $\\beta_2$, ..., $\\beta_k$.\n\n- $\\beta_0$ is the intercept\n- $\\beta_1$ to $\\beta_k$ are the partial regression coefficients\n- $\\epsilon$ is the error (residual) term\n:::\n\n:::{.fragment}\n### Fit MLR model to Air Quality data\n\nThe variables `Month` and `Day` are not useful predictors, so we will exclude them from the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_multi <- lm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality)\n# fit_multi <- lm(formula = Ozone ~ .-Month -Day, data = airquality) # all variables excluding Month and Day\n```\n:::\n\n:::\n\n## Visualisation: not easy {auto-animate=\"true\"}\n\nAre the plots useful?\n\n### 3D plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\nplot_ly(data = airquality, \n  x = ~Temp, y = ~Ozone, z = ~Solar.R,\n  type = \"scatter3d\", mode = \"markers\", opacity = 0.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Ignoring 42 observations\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-14-1.png)\n:::\n:::\n\n\n## Visualisation: not easy\n\nAre the plots useful?\n\n### 4D plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\nplot_ly(data = airquality, \n  x = ~Temp, y = ~Ozone, z = ~Solar.R, color = ~Wind,\n  type = \"scatter3d\", mode = \"markers\", opacity = 0.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Ignoring 42 observations\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n## Partial regression coefficients {auto-animate=\"true\"}\n\nGiven the multiple linear model:\n$$ Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i $$\n\n\nThe partial regression coefficient for a predictor $x_i$ is the amount by which the response variable $Y$ changes when $x_k$ is increased by one unit, **while all other predictors are held constant**.\n\n$$ \\beta_k = \\frac{\\Delta Y}{\\Delta x_k} $$\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequatiomatic::extract_eq(fit_multi)\n```\n\n::: {.cell-output-display}\n$$\n\\operatorname{Ozone} = \\alpha + \\beta_{1}(\\operatorname{Solar.R}) + \\beta_{2}(\\operatorname{Wind}) + \\beta_{3}(\\operatorname{Temp}) + \\epsilon\n$$\n\n:::\n:::\n\n\n> *With `Wind` and `Solar.R` held constant, how does `Temp` affect `Ozone`?*\n\n## Partial regression coefficients: visualisation {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::plot_model(fit_multi,\n  type = \"pred\", \n  terms = c(\"Temp\", \"Solar.R\", \"Wind\"), \n  ci.lvl = NA)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nIgnoring unknown labels:\n• linetype : \"Solar.R\"\n• shape : \"Solar.R\"\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-17-1.png)\n:::\n:::\n\n\n> *With `Wind` and `Solar.R` held constant, how does `Temp` affect `Ozone`?*\n\n:::{.callout-note}\nNot necessary to do this - lecture content only.\n:::\n\n## Interpreting the partial regression coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_multi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality)\n\nCoefficients:\n(Intercept)      Solar.R         Wind         Temp  \n  -64.34208      0.05982     -3.33359      1.65209  \n```\n\n\n:::\n:::\n\n\n:::{.fragment}\nHolding **all** other variables constant:\n\n- For every 1 unit increase in `Solar.R`, `Ozone` increases by a mean value of 0.06 ppb.\n- For every 1 degree increase in `Temp`, `Ozone` increases by a mean value of 1.65 ppb.\n- For every 1 unit increase in `Wind`, `Ozone` decreases by a mean value of 3.33 ppb.\n:::\n\n:::{.fragment}\n:::{.callout-caution}\nIf the model is not \"valid\" (via assumptions or hypothesis), then the partial regression coefficients are not meaningful.\n:::\n:::\n\n## Assumptions\n\nIn SLR, the model is made up of the [**deterministic**]{style=\"color:seagreen\"} component (the line) and the [***random***]{style=\"color:firebrick\"}  component (the error term).\n\n$$ Y_i = \\color{seagreen}{\\beta_0 + \\beta_1 x_i} + \\color{firebrick}\\epsilon_i $$\n\n. . .\n\n**This is the same for MLR:**\n$$ Y_i = \\color{seagreen}{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k} + \\color{firebrick}{\\epsilon_i} $$\n\nSince *only* the error term is random, the assumptions are *still* about the error term (residuals), $\\hat\\epsilon$, which is simple to assess!\n\n## Assumptions - CLINE\n\nAs with Simple Linear Regression, we need to check the assumptions of the model (LINE):\n\n- **L**inearity: the relationships between the response and the predictors are all linear.\n- **I**ndependence: the observations are independent of each other.\n- **N**ormality: the residuals are normally distributed.\n- **E**qual variance: the variance of the residuals is constant.\n\nWith one extra assumption!\n\n- **Collinearity**: there is no perfect linearity between predictors\n\nTwo predictors that have a *perfect* linear relationship (i.e. $r$ = 1 or -1) breaks the assumption of collinearity. High (but not perfect) collinearity (e.g. strong/very strong $r$) does not break the assumption but can lead to unstable estimates and large standard errors.\n\n> The largest correlation between the predictors is between Temp and Wind ($r$ = -0.5). This is not a problem.\n\n## Assumptions of MLR \n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit_multi)\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-19-1.png)\n:::\n:::\n\n\n## Transformation using `log()`\n\nSome evidence of non-linearity in the diagnostic plots. Transform and re-check assumptions.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_multi_log <- lm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit_multi_log)\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-20-1.png)\n:::\n:::\n\n\n## Results -- MLR vs SLR {auto-animate=\"true\"} \n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_multi_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,\tAdjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n::::\n\n- All three predictors are statistically significant (p < .001).\n- The MLR model explains 66% of variance (adjusted R^2^ = 0.66), whereas the SLR explains 48% of variance (multiple R^2^ = 0.48).\n- Thus the MLR is the better model.\n\n## Hypothesis Testing\n\nFor multiple linear regression, there are two hypothesis tests:\n\n- Individual predictors, where the significance of each predictor is tested via t-tests\n\n$$H_0: \\beta_k = 0$$\n$$H_1: \\beta_k \\neq 0$$\n\n- The overall model, which is tested with an F-test (to get F-stat). $H_0$ is an intercept-only model (i.e. the mean), so if at least one predictor is useful, the model is better than the intercept-only model.\n\n$$H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0$$\n$$H_1: \\text{At least one } \\beta_k \\neq 0$$\n\n## Interpreting coefficients\n\n```r\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n```\n\nAll three predictors are statistically significant (p < .001). **Holding all other variables constant:**\n\n- For every 1 unit increase in `Solar.R`, `log(Ozone)` increases by a mean value of 0.0025, \n- For every 1 unit increase in `Wind`, `log(Ozone)` decreases by a mean value of 0.062, \n- For every 1 degree increase in `Temp`, `log(Ozone)` increases by a mean value of 0.049.\n\nOR\n\n- For every 1 unit increase in `Solar.R`, `Ozone` increases by *approximately* a mean value of 0.25%,\n- For every 1 unit increase in `Wind`, `Ozone` decreases by *approximately* a mean value of 6.2%,\n- For every 1 degree increase in `Temp`, `Ozone` increases by *approximately* a mean value of 4.9%.\n\n## Model fit\n\n```r\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\nOn average, the model predicts `log(Ozone)` within 0.51 ppb (**residual standard error**) of the true value. *Not bad?*\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(0.51) #backtransform\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.665291\n```\n\n\n:::\n:::\n\n\n- On average, the model predicts `Ozone` within 1.67 ppb of the true value.\n- Degrees of freedom (107) = number of observations (111) - number of parameters in the model (3 predictors and 1 intercept)\n\n. . .\n\nIf there are >1 predictors, use the **adjusted R-Squared** as it penalises the model for having more predictors that are not useful.\n\n- The MLR model explains 66% of variance (adjusted R^2^ = 0.66)\n\n## The R^2^ value\n\nThe R-squared value is the proportion of variance explained by the model.\n\n$$ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n\nThe adjusted R-squared value is the proportion of variance explained by the model, adjusted for the number of predictors.\n\n$$ R^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} $$\n\nwhere $n$ is the number of observations and $p$ is the number of predictors.\n\n## F-stat\n\n```r\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n- The F-statistic tests the null hypothesis that all the regression coefficients are equal to zero, i.e. $H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0$.\n- As a ratio, it tells us how much better the model is than the null model (i.e. a model with no predictors, the mean).\n- If the p-value is less than our specified critical value (e.g. 0.05), we reject the null hypothesis and conclude that the current model is better than the null model.\n\n## Reporting {auto-animate=\"true\"}\n\nA quick (but not complete) summary:\n\n- New York air quality data was collected in 1973 by the New York State Department of Conservation and the National Weather Service (meteorological data). There were 111 observations of 6 variables.\n\n- There were non-linear relationships between `Ozone` (the response) and `Temp`, `Wind` and `Solar.R` (the predictors), hence a natural log transformation was applied to `Ozone`.\n\n- Multiple linear regression was conducted on these variables, and model assumptions (CLINE) were met.\n\n- Solar radiation, wind speed and temperature are **significant predictors** of Ozone concentration (**p < 0.001**) with the model accounting for **66% of the variation** in log(Ozone). The model explained more variance than a one-predictor model and was found to be significantly better than the null model.\n\n# Abalone Quiz\n\n![](images/abalone.jpg)\n\n> Pop quiz! (No marks, just check your understanding.)\n\n## Context\n\nAbalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. Researchers measured 9 attributes of the abalone: `sex`, `length`, `diameter`, `height`, `whole`, `shucked`, `viscera`, `shell`, and `rings`.\n\nNote: `whole`, `shucked`, `viscera` and `shell` are weight measurements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabalone <- read.csv(\"data/abalone.csv\")\n\nset.seed(1113)          # reproducible randomness\nabalone <- abalone %>% \n  select(-sex) %>%      # remove `sex` because it is categorical\n  sample_n(100)         # sample 100 observations for cleaner curve\n  \nstr(abalone)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t100 obs. of  8 variables:\n $ length  : num  0.52 0.71 0.33 0.67 0.65 0.35 0.695 0.52 0.6 0.61 ...\n $ diameter: num  0.405 0.57 0.255 0.55 0.51 0.25 0.53 0.41 0.475 0.48 ...\n $ height  : num  0.14 0.195 0.095 0.17 0.19 0.1 0.15 0.14 0.15 0.17 ...\n $ whole   : num  0.692 1.348 0.188 1.247 1.542 ...\n $ shucked : num  0.276 0.8985 0.0735 0.472 0.7155 ...\n $ viscera : num  0.137 0.444 0.045 0.245 0.373 ...\n $ shell   : num  0.215 0.454 0.06 0.4 0.375 ...\n $ rings   : int  11 11 7 21 9 7 14 11 10 10 ...\n```\n\n\n:::\n:::\n\n\n## Scatterplots and correlations\n\nWe remove `sex` from the dataset (not numerical), and subset 100 samples for a cleaner view.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npsych::pairs.panels(abalone)     # visualise relationships\n```\n\n::: {.cell-output-display}\n![](lecture-11_files/figure-typst/unnamed-chunk-25-1.png)\n:::\n:::\n\n\n## Full model\n\nWe use natural log transformation on the response variable with `log()` to account for non-linear relationships.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(log(rings) ~ ., data = abalone)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(rings) ~ ., data = abalone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37297 -0.12727 -0.01584  0.08787  0.61636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.34626    0.18219   7.389 6.57e-11 ***\nlength      -1.25389    1.50969  -0.831  0.40837    \ndiameter     3.24138    1.91481   1.693  0.09388 .  \nheight       2.26408    1.34813   1.679  0.09646 .  \nwhole        0.03089    0.29250   0.106  0.91612    \nshucked     -1.30902    0.38861  -3.368  0.00111 ** \nviscera     -0.24785    0.55098  -0.450  0.65389    \nshell        1.73328    0.60179   2.880  0.00494 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1996 on 92 degrees of freedom\nMultiple R-squared:  0.6187,\tAdjusted R-squared:  0.5897 \nF-statistic: 21.32 on 7 and 92 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## All models\n\nHere, the model is fit with all predictors, then the least significant predictor is removed. This process is repeated until only one predictor remains.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(broom)\n\nfull7 <- lm(log(rings) ~ ., data = abalone)\npart6 <- update(full7, . ~ . - whole)\npart5 <- update(part6, . ~ . - viscera)\npart4 <- update(part5, . ~ . - length)\npart3 <- update(part4, . ~ . - height)\npart2 <- update(part3, . ~ . - diameter)\npart1 <- update(part2, . ~ . - shucked)\n\nformulas <- c(part1$call$formula, \n              part2$call$formula, \n              part3$call$formula, \n              part4$call$formula, \n              part5$call$formula, \n              part6$call$formula, \n              full7$call$formula)\n\nrs <- bind_rows(glance(part1),\n                glance(part2),\n                glance(part3),\n                glance(part4),\n                glance(part5),\n                glance(part6),\n                glance(full7)) %>%\n  mutate(Model = formulas, n = 1:7) %>%\n  select(Model, n, r.squared, adj.r.squared) %>%\n  mutate_if(is.numeric, round, 3)\n\nknitr::kable(rs)\n```\n\n::: {.cell-output-display}\n\n\n|Model                                                                     |  n| r.squared| adj.r.squared|\n|:-------------------------------------------------------------------------|--:|---------:|-------------:|\n|log(rings) ~ shell                                                        |  1|     0.445|         0.439|\n|log(rings) ~ shucked + shell                                              |  2|     0.557|         0.548|\n|log(rings) ~ diameter + shucked + shell                                   |  3|     0.604|         0.591|\n|log(rings) ~ diameter + height + shucked + shell                          |  4|     0.614|         0.598|\n|log(rings) ~ length + diameter + height + shucked + shell                 |  5|     0.618|         0.597|\n|log(rings) ~ length + diameter + height + shucked + viscera + ,     shell |  6|     0.619|         0.594|\n|log(rings) ~ .                                                            |  7|     0.619|         0.590|\n\n\n:::\n:::\n\n\n## Reduced model\n\n```r\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.4122     0.1594   8.859 4.19e-14 ***\ndiameter      2.0346     0.6034   3.372  0.00108 ** \nshucked      -1.3339     0.2152  -6.200 1.42e-08 ***\nshell         2.0486     0.3672   5.579 2.23e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n```\n# How did you do?\n\n> Read exam questions carefully and use the process of elimination.\n\n## Model and variable selection\n\n- Aim is to achieve the best balance between **model fit** and **model complexity**.\n- Follow the rules of parsimony: the simplest model that explains the data is the best, given similar model fit.\n  - Consider the effect of removing non-significant predictors from the model.\n  - If model fit (i.e. R^2^) reduces drastically, keep the predictor, else keep culling.\n- Covered in more detail in **second year (ENVX2001)** (stepwise regression)\n\n## Summary\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n**Multiple Linear Regression**\n\n- More than one predictor\n- Fit y to multiple x -- multiple dimensions (hyperplane)\n- Principle: minimise sum of squared residuals\n- Assumptions: CLINE (collinearity)\n- Adjusted R-squared\n\n:::\n\n::: {.column width=\"50%\"}\n\n**Simple Linear Regression**\n\n- One predictor, fit a straight line\n- Fit straight line between y and x\n- Principle: minimise sum of squared residuals\n- Assumptions: LINE\n- Multiple R-squared\n\n:::\n::::\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "supporting": [
      "lecture-11_files/figure-typst"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}