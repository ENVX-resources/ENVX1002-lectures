{
  "hash": "d7a0d75edb3eb175fe9ca26cfb95132f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 8 -- Permutation tests and bootstrap\nauthor: Floris van Ogtrop \nformat: soles-revealjs\nembed-resources: true\n---\n\n\n\n## Announcement\n\n- Practice Skills assessment this week, run in your respective labs. **You must attend your lab to complete the assessment.**\n- A reminder that you are allowed to use cheat sheets for the assessment, but you must write or print them yourself\n  - maximum 4 sides of A4 paper\n  - no electronic devices allowed\n- Printing services are available on campus. See [here](https://www.sydney.edu.au/students/student-it/print-scan-copy.html).\n\n## Learning Outcomes\n\n### At the end of this week, you will be able to:\n\n1. Understand and explain resampling techniques as alternatives to parametric and non-parametric tests\n2. Apply permutation tests to analyse data when parametric assumptions are not met\n3. Implement bootstrap methods to estimate confidence intervals and statistical parameters\n4. Compare and contrast different statistical approaches (parametric, non-parametric, and resampling)\n\n\n# Recap\n\n## Non-normal data\n\nWhere data does not meet the assumptions of parametric tests, we have two options:\n\n1. **Transform** the data, and continue with parametric tests; or\n2. Use **non-parametric** \"equivalents\" of parametric tests at the cost of power and loss of information.\n\n::: fragment\n### A third option exists.\n:::\n\n::: incremental\n- Use *computer intensive*, randomisation-based methods to test hypotheses, called **resampling techniques**.\n- These methods include **randomisation (or permutation) tests** and **bootstrap**.\n- Retains estimates of **effect size** and **confidence intervals**.\n:::\n\n\n# Resampling techniques\n\n. . .\n\n> Two roads diverged in a yellow wood,  \n> And sorry I could not travel both  \n> And be one traveler, long I stood  \n> And looked down one as far as I could  \n> To where it bent in the undergrowth;\n\n– Robert Frost, The Road Not Taken, 1916\n\n## Model-based inferential techniques\n\n::: incremental\n- Traditionally, inferential statistics is based on **mathematical approximations** and *assumptions* about how data is obtained.\n- Based on knowledge that \"randomness\" somehow obeys certain patterns in nature which can be *reliably* described by **probability distributions**.\n- Uses **probability theory** to draw approximate conclusions about these patterns when we observe data.\n:::\n\n## Resampling techniques\n\n::: incremental\n- Based on the idea that we can use the **data itself** to estimate the distribution of the test statistic or parameter of interest.\n- These methods are **model-free** and **distribution-free**.\n- Requires comparatively higher computational power, **but nowadays it is not a problem** -- any modern personal computer can handle it.\n:::\n\n## Randomisation or Bootstrap?\n\n. . .\n\n### They are not the same:\n\n:::: columns\n::: column\n::: fragment\n### Randomisation\nGenerate a distribution of the **test statistic** under the null hypothesis by randomly sub-sampling the data, *without replacement*. Can be used to estimate a **p-value**.\n:::\n:::\n\n::: column\n::: fragment\n### Bootstrap\nGenerate a distribution of the **parameter** of interest (e.g. mean) by resampling the data with replacement[^resampling]. Can be used to estimate **confidence intervals**.\n\n[^resampling]: Also known as hallucination as it creates *alternative* versions of the data.\n\n:::\n:::\n::::\n\n:::: columns\n::: column\n::: fragment\n> Basically, shuffle the data and see what happens.\n:::\n:::\n\n::: column\n::: fragment\n> Basically, create alternative versions of the data and see what happens.\n:::\n:::\n::::\n\n## Why would these techniques work?\n\nAt the core of the resampling approach is the idea that the **observed data** is a **random sample** from a **larger population**.\n\n::: fragment\n### If the sampled data is truly representative of the population...\n\nThen, if we *infinitely resample from the sample itself*, we should be able to *somewhat* approximate the distribution of the test statistic under the null hypothesis, or parameter of interest (will show example later).\n\n:::\n\n# Randomisation tests\n\nTo generate a distribution of the test statistic under the null hypothesis.\n\n## Example: comparing two groups\n\n::: incremental\n\n- Suppose we have two samples (groups) and we want to test if the **mean scores[^scores]** are different.\n- Under the **null hypothesis** that there is *no difference* between the groups, the two sets of scores will have the same distribution.\n- Thus, we can **pool** the scores and reassign them to the two groups, since any score is equally likely to belong in either group, i.e. the scores are **exchangeable**.\n:::\n\n## Steps\n\n::: incremental\n1. **Pool** the scores from both groups into a single dataset.\n2. **Randomly reassign** the scores to two groups.\n3. Calculate the **test statistic** of interest, in this case the *t*-test statistic.\n4. Repeat steps 2 and 3 many times to generate a distribution of the test statistic under the null hypothesis.\n5. Compare the **observed** test statistic to the **randomised** distribution to calculate a **p-value**.\n:::\n\n[^scores]: Basically any measure of interest.\n\n## Data\n\nThe `sleep` dataset in R contains the average **extra** hours of sleep, compared to control,  for 10 patients who were given two different drugs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nglimpse(sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 20\nColumns: 3\n$ extra <dbl> 0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0.0, 2.0, 1.9, 0.8, …\n$ group <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ ID    <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n```\n\n\n:::\n:::\n\n\n\n## Are assumptions of normality met?\n\nWe picked a dataset where the assumptions are met, so that we can compare the results with the parametric test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(sleep, aes(x = group, y = extra)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](lecture-08_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n## Calculating the *t*-test statistic\n\nRecall that the test statistic for the two-sample *t*-test is:\n\n$$ t = \\frac{Difference\\ in\\ the\\ means}{Standard\\ error\\ of\\ the\\ difference} $$\n\nWe could calculate it manually, but let's just use the `t.test()` function in R since the function calculates the test statistic for us. For example, the observed test statistic for the `sleep` data is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(extra ~ group, data = sleep)$statistic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        t \n-1.860813 \n```\n\n\n:::\n:::\n\n\n\n## Step 1: Pool the scores {auto-animate=true}\n\nThe first step is to pool the data. The pooled data is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npooled_data <- sleep$extra\npooled_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0  1.9  0.8  1.1  0.1 -0.1\n[16]  4.4  5.5  1.6  4.6  3.4\n```\n\n\n:::\n:::\n\n\n\nWhere the first 10 scores are from the first group, and the next 10 scores are from the second group.\n\n## Step 2: Randomly reassign the scores {auto-animate=true}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npooled_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0  1.9  0.8  1.1  0.1 -0.1\n[16]  4.4  5.5  1.6  4.6  3.4\n```\n\n\n:::\n:::\n\n\n\nNext, we randomly shuffle the pooled data and re-assign the first 10 scores to group 1, and the next 10 scores to group 2.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1022)\nshuffled_data <- sample(pooled_data)\nshuffled_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -1.2  2.0  3.7  0.1  0.8  1.6  0.8  1.9  0.0  3.4 -1.6 -0.2  1.1 -0.1  0.7\n[16]  4.4  5.5  3.4 -0.1  4.6\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\ngroup1 <- shuffled_data[1:10]\ngroup2 <- shuffled_data[11:20]\n```\n:::\n\n\n\n\n## Step 3: Calculate the test statistic\n\nWe're not using the results from the `t.test()` function, but just extracting the test statistic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(group1, group2)$statistic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         t \n-0.4995608 \n```\n\n\n:::\n:::\n\n\n\n\n## Step 4: Repeat many times\n\nPutting it all together, we can write a function to obtain the test statistic:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nrandom_t <- function(data) {\n  shuffled_data <- sample(data)\n  group1 <- shuffled_data[1:10]\n  group2 <- shuffled_data[11:20]\n\n  return(t.test(group1, group2)$statistic)\n}\n```\n:::\n\n\n\n\nRepeat the function 10,000 times:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1034)\nrandom_t_values <- replicate(10000, random_t(pooled_data))\n```\n:::\n\n\n\n## Step 5: Compare the observed test statistic\n\nFinally, we can compare the observed test statistic to the randomised distribution. This can be done by calculating the proportion of randomised test statistics that are more extreme than the observed test statistic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nfit <- t.test(extra ~ group, data = sleep) # test on observed data\np_value <- mean(abs(random_t_values) >= abs(fit$statistic))\nround(p_value, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08\n```\n\n\n:::\n:::\n\n\n\n\n### How does this compare to the parametric t-test?\n\nIf we round the p-values of both tests to two decimal places, we get:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nround(fit$p.value, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08\n```\n\n\n:::\n:::\n\n\n\nAs we can see, the p-values are very similar. This is because the assumptions of the parametric test are met, so the results will be close even though the methods are different!\n\n## What's the difference between the two techniques?\n\n. . .\n\n:::: columns\n::: column\n\n### *t*-test\n- The **parametric** test compares the **observed** test statistic to a **theoretical** distribution that has fixed parameters.\n:::\n\n::: column\n### Randomisation test\n- The **randomisation** test compares the **observed** test statistic to a **randomised** distribution that is generated from the data itself.\n:::\n::::\n\n. . .\n\n:::: columns\n::: column\n- Assumes that the data is **normally distributed**.\n:::\n\n::: column\n- **No assumptions** about the data distribution, but if the assumption were met, the results would be similar.\n:::\n::::\n\n. . .\n\n:::: columns\n::: column\n- P-value is calculated from the **theoretical** distribution.\n:::\n\n::: column\n- P-value is calculated from the *simulated* distribution.\n:::\n::::\n\n## Visualising the randomised distribution\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data.frame(t = random_t_values), aes(x = t)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = fit$statistic, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Randomised distribution of t-test statistic\",\n       x = \"t-test statistic\",\n       y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](lecture-08_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\nWe can see that the observed test statistic is well within the distribution of the randomised test statistics (which is normally distributed).\n\n# Randomisation test using a package\n\n## Randomisation tests using `coin`\n\nThe `coin` package in R provides a simple interface to perform randomisation tests.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coin)\n```\n:::\n\n\n\nLet's use the `sleep` dataset to demonstrate how to use the `coin` package to perform a randomisation test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperm_test <- independence_test(extra ~ group, data = sleep, distribution = approximate(B = 9999))\n\n# Print results\nprint(perm_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tApproximative General Independence Test\n\ndata:  extra by group (1, 2)\nZ = -1.7508, p-value = 0.08321\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n\n\n# Bootstrap\n\nTo generate a distribution of the parameter of interest.\n\n## Example: estimating the mean\n\n::: incremental\n- Suppose we have two samples (groups) and we want to estimate the **difference in means**, and the **95% confidence interval** of the difference.\n- We can use the usual **mathematical equation** to calculate 95% CI, but if the data *does not meet the assumption of normality*, then  the CI will be a bad estimate.\n- Instead, we can use the **bootstrap** to estimate the 95% CI, which is based on the **simulated distribution of the mean difference**.\n:::\n\n## Steps\n\n::: incremental\n1. **Resample** the data with replacement.\n2. Calculate the **parameter of interest** (e.g. mean) for each resample.\n3. Repeat steps 1 and 2 many times ($N$) to generate a distribution of the parameter of interest.\n4. Calculate the **95% confidence interval** from the simulated distribution:\n   - The mean of the distribution is the **point estimate**.\n   - The $0.025 \\times N$th smallest mean is the **lower bound** of the 95% CI.\n   - The $0.975\\times N$th smallest mean is the **upper bound** of the 95% CI.\n:::\n\n## Data\n\nThe `BOD` dataset in R contains the **biochemical oxygen demand** (mg/L) measurements of 6 samples over time.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglimpse(BOD)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6\nColumns: 2\n$ Time   <dbl> 1, 2, 3, 4, 5, 7\n$ demand <dbl> 8.3, 10.3, 19.0, 16.0, 15.6, 19.8\n```\n\n\n:::\n:::\n\n\n\n## Step 1: Resample the data\n\nFrom the pooled original data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nBOD$demand\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  8.3 10.3 19.0 16.0 15.6 19.8\n```\n\n\n:::\n:::\n\n\n\nWe `sample()` with replacement:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1113)\nresampled_data <- sample(BOD$demand, replace = TRUE)\nresampled_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.6  8.3  8.3  8.3 16.0  8.3\n```\n\n\n:::\n:::\n\n\n\nNoting that some scores will be repeated, and some will be missing.\n\n## Step 2: Calculate the parameter of interest\n\nThe parameter of interest is the mean value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(resampled_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.8\n```\n\n\n:::\n:::\n\n\n\n## Step 3: Repeat many times\n\nSince it's a simple process, we can write a function to calculate the mean:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbootstrap_mean <- function(data) {\n  resampled_data <- sample(data, replace = TRUE)\n  return(mean(resampled_data))\n}\n```\n:::\n\n\n\nThen repeat the function 10,000 times:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1116)\nbootstrap_means <- replicate(10000, bootstrap_mean(BOD$demand))\n```\n:::\n\n\n\n## Step 4: Calculate the 95% CI\n\nThe 95% CI is calculated from the simulated distribution:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmeanval <- mean(bootstrap_means)\nCI <- quantile(bootstrap_means, c(0.025, 0.975))\n```\n:::\n\n\n\nPutting it together, the mean is 14.85131 with a 95% CI of [11.25, 18.13].\n\n\n\n### How does this compare to the parametric test?\n\nIf we use the `t.test()` function to calculate the 95% CI:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(BOD$demand)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  BOD$demand\nt = 7.8465, df = 5, p-value = 0.0005397\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  9.973793 19.692874\nsample estimates:\nmean of x \n 14.83333 \n```\n\n\n:::\n:::\n\n\n\n## How different are the results?\n\n| Method | Mean | 95% CI | CI size |\n|--------|------|--------|---------|\n| Bootstrap | 14.85 | [11.25, 18.13] | 6.88 |\n| Parametric | 14.83 | [9.97, 19.69] | 9.72 |\n\n::: incremental\n- The **point estimates** of the mean are almost identical.\n- The **95% CI** of the mean are similar, but the **bootstrap** CI is non-symmetric - it represents the **true** distribution of the mean.\n- The **size** of the CI is smaller for the **bootstrap** method, indicating that the **parametric** method is **overestimating** the precision of the estimate.\n:::\n\n# Bootstrap test using `boot`\n\n## Again we can use a package to do this\n\nThe `boot` package in R provides a simple interface to perform bootstrap tests.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(boot)\n```\n:::\n\n\n\nLet's use the `BOD` dataset to demonstrate how to use the `boot` package to perform a bootstrap test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define function to compute mean\nboot_mean <- function(data, indices) {\n  return(mean(data[indices]))\n}\n\nboot_test <- boot(BOD$demand, boot_mean, R = 10000)\n\n# Print results \nprint(boot_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = BOD$demand, statistic = boot_mean, R = 10000)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1* 14.83333 -0.010455     1.72223\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compute bootstrap confidence interval\nboot.ci(boot_test, type = c(\"perc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_test, type = c(\"perc\"))\n\nIntervals : \nLevel     Percentile     \n95%   (11.40, 18.13 )  \nCalculations and Intervals on Original Scale\n```\n\n\n:::\n:::\n\n\n\n# What is recommended?\n\n::: callout-note\n## TLDR\nThe general trend in modern statistics is to use resampling techniques over traditional methods, even when the assumptions are met -- and this is currently led by the `tidymodels` framework in R.\n:::\n\n## Using `infer`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n:::\n\n\n\nLet's use the `sleep` dataset to demonstrate how to use the `infer` package to perform a randomisation test (also makes it easier to compare against manual method).\n\nThe `infer` package requires the user to use an expressive grammar to specify the analysis. \n\n### Steps\n\n1. `specify()` the response variable of interest, then\n2. `hypothesise()` the null hypothesis, then\n3. `generate()` the null distribution, and finally\n4. `calculate()` the p-value.\n\n## Two-sample t-test using `infer`\n\nFirst we need to calculate the observed test statistic so that we can compare it to the simulated distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nobserved <- sleep %>%\n  specify(extra ~ group) %>%\n  calculate(stat = \"diff in means\", order = c(1, 2))\nobserved\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: extra (numeric)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 -1.58\n```\n\n\n:::\n:::\n\n\n\nThen we generate the null distribution and calculate the p-value:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1034)\npval_infer <- \n  sleep %>%\n  specify(extra ~ group) %>%\n  hypothesise(null = \"independence\") %>%\n  generate(reps = 10000, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(1, 2)) %>%\n   get_p_value(obs_stat = observed,\n              direction = \"two-sided\")\n```\n:::\n\n\n\n## What are the differences?\n\n| Method | P-value |\n|--------|---------|\n| Manual | 0.082 |\n| `infer` | 0.084 |\n| `t.test()` | 0.079 |\n\nAs we can see, the results are very similar because the assumptions of the parametric test were already met.\n\n::: callout-note\nTo calculate confidence intervals, use the `get_ci()` function as documented [here](https://infer.netlify.app/articles/observed_stat_examples#confidence-intervals).\n:::\n\n# What about non-normal data?\n\n## Example: beetles\n\nThe `beetle` dataset was used in last week's lecture to demonstrate the non-parametric Wilcoxon rank-sum test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nbeetle <- readr::read_csv(\"data/beetle.csv\")\nglimpse(beetle)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 45\nColumns: 2\n$ SIZE    <chr> \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\",…\n$ BEETLES <dbl> 256, 209, 0, 0, 0, 44, 49, 117, 6, 0, 0, 75, 34, 13, 0, 90, 0,…\n```\n\n\n:::\n:::\n\n\n\n\n## Assumption\n\nRecall that the data does not meet the assumptions of normality:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np1 <- ggplot(beetle, aes(x = BEETLES)) +\n  geom_histogram(bins = 14, position = \"dodge\") +\n  facet_wrap(~SIZE, ncol = 1)\np2 <- ggplot(beetle, aes(x = SIZE, y = BEETLES)) +\n  geom_boxplot()\np3 <- ggplot(beetle, aes(sample = BEETLES)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~SIZE, ncol = 1)\n\nlibrary(patchwork)\np1 + p2 + p3\n```\n\n::: {.cell-output-display}\n![](lecture-08_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## T-test via resampling using `infer`\n\nFirst, calculate the test statistic:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nobserved <- beetle %>%\n  specify(BEETLES ~ SIZE) %>%\n  calculate(stat = \"diff in means\", order = c(\"small\", \"large\"))\n```\n:::\n\n\n\nThen generate the null distribution and calculate the p-value:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(1034)\npval_infer <- \n  beetle %>%\n  specify(BEETLES ~ SIZE) %>%\n  hypothesise(null = \"independence\") %>%\n  generate(reps = 10000, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"small\", \"large\")) %>%\n  get_p_value(obs_stat = observed,\n              direction = \"two-sided\")\n\npval_infer\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.025\n```\n\n\n:::\n:::\n\n\n\n## Comparisons\n\nLet's compare the p-values from \n\n1. a t-test (if we ignore violations of assumptions),\n2. the wilcoxon rank-sum test, and\n3. the `infer` randomisation test.\n\n## Comparisons\n\n### T-test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_ttest <- t.test(BEETLES ~ SIZE, data = beetle)$p.value %>%\n  round(3)\n```\n:::\n\n\n\n### Wilcoxon rank-sum test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_wilcox <- wilcox.test(BEETLES ~ SIZE, data = beetle)$p.value %>%\n  round(3)\n```\n:::\n\n\n\n### Randomisation test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_infer <- pull(pval_infer, p_value) %>%\n  round(3)\n```\n:::\n\n\n\n## Results\n\n| Method | P-value |\n|--------|---------|\n| T-test | 0.037 |\n| Wilcoxon | 0.075 |\n| `infer` | 0.025 |\n\n- As we can see, the p-values are quite different because the assumptions of the parametric test were violated.\n- The randomisation test is more robust and provides a more accurate estimate of the p-value than the Wilcoxon rank-sum test.\n\n### How to report results of randomisation test\n\nThe results of the randomisation test can be reported as follows:\n\n> Beetle consumption was significantly different between small and large beetles (t = 2.19, R = 10000, p = 0.025).\n\n\n# Summary\n\n- Resampling techniques are **model-free** and **distribution-free** and requires only that the data is a random sample that is representative of the population.\n- If the **assumptions of parametric tests are met**, the results of resampling techniques will be **similar** to traditional methods.\n- No information is lost in resampling techniques, and they are more robust than traditional methods.\n\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}