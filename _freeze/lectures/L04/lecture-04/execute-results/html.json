{
  "hash": "e5d2b5d1eaa3e89d26debc5606b732ad",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04 -- The central limit theorem\"\nauthor: Januar Harianto\nformat:\n  ochre-revealjs: default\n  ochre-typst: default\nexecute:\n  cache: true\n---\n\n\n\n## Learning Outcomes\n\n### At the end of this week, you will be able to:\n\n1. Define probability distributions and their key properties\n2. Apply the normal distribution to calculate probabilities\n3. Standardise variables using the normal distribution\n4. Differentiate between population, sample and sampling distributions\n5. Explain the difference between standard deviation and standard error\n6. Apply the central limit theorem and understand its significance\n7. Use R to implement these statistical concepts\n\n## Quick checklist\n\n- [ ] Create and edit Quarto documents\n- [ ] Understand the concept of central tendency and spread/dispersion\n- [ ] Interpret skewness (and kurtosis) in a distribution of data\n- [ ] Know that different data types require different approaches to summarising data\n- [ ] Generate some plots in `ggplot2`\n\n# Probability distributions\n\n## Example: rainfall {.scrollable}\n\nConsider measuring daily rainfall amounts in a coastal wetland. *Some* days have no rain. **Most** days have small to moderate amounts. *Few* days have very heavy rainfall.\n\n::: {.fragment}\nThis creates a pattern of variation in rainfall amounts, which at some point we can predict. This pattern is what we call a **probability distribution** (i.e. we can calculate the probability of a certain amount of rainfall on a given day).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nrainfall_data <- data.frame(\n  rainfall = rgamma(1000, shape = 2, scale = 5)\n)\n\np_rainfall <- ggplot(rainfall_data, aes(x = rainfall)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", colour = \"white\") +\n  labs(\n    x = \"Daily Rainfall (mm)\",\n    y = \"Frequency\"\n  )\np_rainfall\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/rainfall-sim-1.png){width=960}\n:::\n:::\n\n:::\n\n\n## Probability distribution?\n\nA probability distribution is a function or equation that tells us:\n\n- How likely different values are to occur\n- The pattern of variation in our measurements\n- The range and spread of possible values\n\nIt is like a mathematical recipe for how values are distributed.\n\n::: {.fragment}\n### Different types of distributions\n\nDepending on the data, distributions can be:\n\n- Discrete (e.g. number of trees in 100m^2^ of forest) --  probability **distribution** function $P = f(x)$\n  - What is the probability that 100m^2^ of forest contains 50 trees?\n- Continuous (e.g. rainfall amounts over a period) -- probability **density** function $f(x)$\n  - What is the probability of rainfall between 10-20mm a day?\n- Both discrete and continuous (e.g. age of trees) $F(x) = P(X \\leq x)$\n  - What is the probability that a tree is older than 50 years?\n:::\n\n## Properties of distributions\n\nWe now combine all of what we have learnt about data in the last 3 weeks:\n\n- **Central tendency** -- mean, median, mode\n- **Spread** -- standard deviation, variance\n- **Shape** -- symmetry, skewness\n- **Tails** -- how quickly they \"decay\" towards zero\n\nBut how do we use these properties to make predictions? **Do some distributions have special properties?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Annotate our rainfall distribution with key properties\np_rainfall +\n  geom_vline(\n    xintercept = mean(rainfall_data$rainfall),\n    colour = \"red\", linetype = \"dashed\"\n  ) +\n  annotate(\"text\",\n    x = mean(rainfall_data$rainfall), y = 100,\n    label = \"Mean\", angle = 90, vjust = -0.5\n  ) +\n  labs(\n    x = \"Daily Rainfall (mm)\",\n    y = \"Frequency\",\n    title = \"Can we use the mean to predict rainfall?\"\n  )\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/dist-properties-1.png){width=960}\n:::\n:::\n\n\n# The general normal distribution\n\n## Tree heights\n\nTree heights in a forest often follow a normal distribution:\n\n- Most trees cluster around the average height\n- Fewer trees are very short or very tall\n- The pattern is symmetrical -- a bell-shaped curve with two parameters (mean $\\mu$ and standard deviation $\\sigma$) such that $$X \\sim N(\\mu, \\sigma^2)$$\n\n## Comparing distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up the data\nset.seed(123)\ntree_heights <- data.frame(\n  height = rnorm(1000, mean = 20, sd = 3)\n)\n\n# Create plots for comparison\np1 <- ggplot(rainfall_data, aes(x = rainfall)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 2,\n    fill = \"steelblue\",\n    colour = \"white\"\n  ) +\n  labs(\n    x = \"Daily Rainfall (mm)\",\n    y = \"Density\",\n    title = \"Skewed Distribution\\n(Rainfall)\"\n  )\n\np2 <- ggplot(tree_heights, aes(x = height)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 0.5,\n    fill = \"forestgreen\",\n    colour = \"white\"\n  ) +\n  labs(\n    x = \"Tree Height (m)\",\n    y = \"Density\",\n    title = \"Normal Distribution\\n(Tree Heights)\"\n  )\n\n# Arrange plots side by side\nplot_grid(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/compare-dist-1.png){width=960}\n:::\n:::\n\n\n## Why do tree heights follow a normal distribution?\n\n1. The middle (mean) height is most common -- most trees are around 20 metres tall\n2. Heights spread out evenly on both sides -- as many short trees as tall trees, like a mirror image around the middle\n3. Extreme heights are rare -- very few extremely short or tall trees -- numbers decrease gradually as we move away from the middle\n\nThe normal distribution is a natural phenomenon in many real-world situations -- a **fundamental** concept in statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np2\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/tree-heights-1.png){width=960}\n:::\n:::\n\n\n\n## A continuous distribution\n\n- There is an infinite number of possible heights within a certain range (e.g. 0-40m)\n- We can calculate the probability of a tree being a certain height or within a range\n- Some heights have a probability of zero (e.g. negative heights, or heights above 40m)\n\n\nWe can use these properties to make reasonable predictions about tree heights:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate mean and standard deviations\nmean_height <- mean(tree_heights$height)\nsd_height <- sd(tree_heights$height)\n\n# Create plot with standard deviation ranges\np2 +\n  # Add vertical lines for mean and SDs\n  geom_vline(\n    xintercept = mean_height,\n    colour = \"darkred\",\n    size = 1,\n    linetype = \"dashed\"\n  ) +\n  # Add SD ranges\n  annotate(\"rect\",\n    xmin = mean_height - sd_height,\n    xmax = mean_height + sd_height,\n    ymin = 0, ymax = Inf,\n    fill = \"yellow\", alpha = 0.2\n  ) +\n  annotate(\"rect\",\n    xmin = mean_height - 2 * sd_height,\n    xmax = mean_height + 2 * sd_height,\n    ymin = 0, ymax = Inf,\n    fill = \"orange\", alpha = 0.1\n  ) +\n  # Add labels\n  annotate(\"text\",\n    x = mean_height, y = 0.15,\n    label = \"Mean\", angle = 90, vjust = -0.5\n  ) +\n  annotate(\"text\",\n    x = mean_height - sd_height/2, y = 0.12,\n    label = \"68% within\\n1 SD\", angle = 90, vjust = -0.5\n  ) +\n  annotate(\"text\",\n    x = mean_height - sd_height, y = 0.09,\n    label = \"95% within\\n2 SD\", angle = 90, vjust = -0.5\n  ) +\n  labs(\n    x = \"Tree Height (m)\",\n    y = \"Density\",\n    title = \"Normal Distribution\\n(Tree Heights)\"\n  )\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/spread-visual-1.png){width=960}\n:::\n:::\n\n\n## The 68-95-99.7 rule\n\nA data that follows a normal distribution:\n\n- About 68% of values fall within 1 standard deviation\n- About 95% fall within 2 standard deviations\n- About 99.7% fall within 3 standard deviations\n- The remaining 0.3% are extreme values and extend to infinity\n\nThis pattern is the same for ALL normal distributions -- a **universal** rule. \n\nImportantly, it is a **predictable** pattern (although not exact).\n\n## From data to normal distribution\n\n- When data follows a reasonably normal pattern, we can \"fit\" a normal distribution to make *equally* reasonable predictions.\n- It is possible to fit a normal distribution to a skewed distribution (but **not recommended** -- other techniques exist)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate statistics for both distributions\ntree_mean <- mean(tree_heights$height)\ntree_sd <- sd(tree_heights$height)\nrain_mean <- mean(rainfall_data$rainfall)\nrain_sd <- sd(rainfall_data$rainfall)\n\n# Create comparison plots showing fitted normal curves\np1 <- ggplot(tree_heights, aes(x = height)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 0.5,\n    fill = \"forestgreen\",\n    colour = \"white\",\n    alpha = 0.7\n  ) +\n  # Add fitted normal curve\n  stat_function(\n    fun = dnorm,\n    args = list(mean = tree_mean, sd = tree_sd),\n    colour = \"darkred\",\n    size = 1\n  ) +\n  # Add mean line and 1 SD range\n  geom_vline(xintercept = tree_mean, colour = \"blue\", linetype = \"dashed\") +\n  annotate(\"rect\",\n    xmin = tree_mean - tree_sd,\n    xmax = tree_mean + tree_sd,\n    ymin = 0, ymax = Inf,\n    fill = \"yellow\", alpha = 0.2\n  ) +\n  labs(\n    x = \"Tree Height (m)\",\n    y = \"Density\",\n    title = \"Normal Data:\\nCan Fit Normal Distribution\"\n  )\n\np2 <- ggplot(rainfall_data, aes(x = rainfall)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 2,\n    fill = \"steelblue\",\n    colour = \"white\",\n    alpha = 0.7\n  ) +\n  # Try to fit normal curve (shows poor fit)\n  stat_function(\n    fun = dnorm,\n    args = list(mean = rain_mean, sd = rain_sd),\n    colour = \"darkred\",\n    size = 1\n  ) +\n  geom_vline(xintercept = rain_mean, colour = \"blue\", linetype = \"dashed\") +\n  annotate(\"rect\",\n    xmin = rain_mean - rain_sd,\n    xmax = rain_mean + rain_sd,\n    ymin = 0, ymax = Inf,\n    fill = \"yellow\", alpha = 0.2\n  ) +\n  labs(\n    x = \"Rainfall (mm)\",\n    y = \"Density\",\n    title = \"Skewed Data:\\nPoor Normal Fit\"\n  )\n\nplot_grid(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/predictability-1.png){width=960}\n:::\n:::\n\n\n## When data is normal...\n\n1. We can use normal probability functions to:\n   - Calculate exact probabilities\n   - Find specific percentiles\n   - Make reliable predictions\n2. Properties are predictable (the 68-95-99.7 rule)\n3. Can standardise measurements (**more on this later**)\n   - Compare different variables\n   - Use z-scores\n   - **Apply statistical tests**\n\n## Working with normal distributions in R {.scrollable}\n\nR provides three main functions that help us work with normal distributions. Think of them as different ways to ask questions about our data:\n\n### 1. `pnorm()`: \"What's the probability?\"\n- Like asking \"What proportion of trees are shorter than X meters?\"\n  * Probability of a tree being under 15m tall\n  * Chance of rainfall being less than 50mm\n  * Proportion of measurements below average\n\n## Working with normal distributions in R {.scrollable}\n\nR provides three main functions that help us work with normal distributions. Think of them as different ways to ask questions about our data:\n\n### 2. `qnorm()`: \"What's the value?\"\n- Like asking \"How tall are the shortest 10% of trees?\"\n  * Finding the height that only 5% of trees exceed\n  * Determining rainfall threshold for \"extreme\" events\n  * Identifying typical range boundaries\n\n## Working with normal distributions in R {.scrollable}\n\nR provides three main functions that help us work with normal distributions. Think of them as different ways to ask questions about our data:\n\n### 3. `dnorm()`: \"What's the relative likelihood?\"\n- Like asking \"How common is this exact height?\"\n  * Comparing how likely different heights are\n  * Finding peak probability\n  * Plotting the normal curve shape\n\n\n## Using `pnorm()` for probabilities\n\nWhat proportion of trees in our forest are shorter than 23m, knowing that mean height is 20m and the sd is 3m?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nprob_under_23 <- pnorm(q = 23, mean = 20, sd = 3) # Calculate proportion of trees under 23m\nprob_under_23\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8413447\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualise the probability\npercent_under_23 <- prob_under_23 * 100\n\nggplot(data.frame(x = c(10, 30)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 20, sd = 3)) +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = 20, sd = 3),\n    fill = \"lightblue\", alpha = 0.5, xlim = c(10, 23)\n  ) +\n  geom_vline(xintercept = 23, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\",\n    x = 23, y = 0.02,\n    label = sprintf(\"%.1f%% of trees\", percent_under_23),\n    hjust = -0.1\n  ) +\n  labs(\n    x = \"Tree Height (m)\", y = \"Density\",\n    title = \"Tree Heights Below 23m\"\n  )\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/pnorm-visual-1.png){width=960}\n:::\n:::\n\n\n\n## Using `qnorm()` for thresholds\n\nHow tall are the largest 10% of trees in our forest?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Find the height threshold for tallest 10%\nheight_90th <- qnorm(p = 0.90, mean = 20, sd = 3)\nheight_90th\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23.84465\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualise the threshold\nggplot(data.frame(x = c(10, 30)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 20, sd = 3)) +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = 20, sd = 3),\n            fill = \"lightgreen\", alpha = 0.5, xlim = c(height_90th, 30)) +\n  geom_vline(xintercept = height_90th, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\", x = height_90th, y = 0.02,\n           label = sprintf(\"Trees above %.1f m\\nare in tallest 10%%\", height_90th),\n           hjust = 1.1) +\n  labs(x = \"Tree Height (m)\", y = \"Density\",\n       title = \"Identifying Tallest Trees\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/qnorm-visual-1.png){width=960}\n:::\n:::\n\n\n\n## Using `dnorm()` for relative likelihood\n\nWhich tree height is most common in our forest: 17m, 20m, or 23m? (Or, how likely are these heights?)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Find relative likelihood at different heights\nheights <- c(17, 20, 23) # Example heights to compare\ndensities <- dnorm(heights, mean = 20, sd = 3)\ndensities\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08065691 0.13298076 0.08065691\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot relative likelihoods\nggplot(data.frame(x = c(10, 30)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 20, sd = 3)) +\n  geom_point(data = data.frame(x = heights, y = densities),\n             aes(y = y), colour = \"red\", size = 3) +\n  geom_segment(data = data.frame(x = heights),\n              aes(x = x, xend = x, y = 0, yend = dnorm(x, 20, 3)),\n              linetype = \"dashed\", colour = \"blue\") +\n  labs(x = \"Tree Height (m)\", y = \"Relative Likelihood\",\n       title = \"Height Probabilities in the Forest\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/dnorm-visual-1.png){width=960}\n:::\n:::\n\n\nTrees closest to 20m are most common, with likelihood decreasing as we move away.\n\n## Practical uses of normal distribution functions\n\nThese functions help us answer questions like:\n\n- What proportion of trees are between 18m and 22m tall?\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Probability between 18m and 22m - calculate the difference\nprob_between <- pnorm(22, 20, 3) - pnorm(18, 20, 3)\nprob_between\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4950149\n```\n\n\n:::\n:::\n\n\n- How tall are the top 5% of trees?\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Height of tallest 5% of trees\ntall_threshold <- qnorm(0.95, 20, 3)\ntall_threshold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 24.93456\n```\n\n\n:::\n:::\n\n\n- What are the height thresholds for \"extreme\" trees (top 1% and bottom 1%)?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Height thresholds for \"extreme\" trees\nextreme_thresholds <- qnorm(c(0.01, 0.99), 20, 3)\nextreme_thresholds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.02096 26.97904\n```\n\n\n:::\n:::\n\n\n\n\n\n# The standard normal curve\n\n## Standardising variables\nWe can convert any normal distribution to a standard form where:\n\n$$ X \\sim N(\\mu, \\sigma^2) \\rightarrow Z \\sim N(0, 1) $$\n\n\n### Why standardise?\n\nStandardising helps us:\n\n1. Compare measurements on different scales\n2. Identify unusual values (e.g., z > 2 is unusual)\n3. Calculate probabilities using standard normal tables\n\nFormula: $$z = \\frac{x - \\mu}{\\sigma}$$ (subtract mean, divide by SD)\n\n## Comparing different species\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data for two tree species\nset.seed(123)\nspecies1 <- rnorm(1000, mean = 20, sd = 3)  # Tall species\nspecies2 <- rnorm(1000, mean = 15, sd = 2)  # Shorter species\n\n# Plot original heights\ndata.frame(\n  height = c(species1, species2),\n  species = rep(c(\"Species 1\", \"Species 2\"), each = 1000)\n) |>\n  ggplot(aes(x = height, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Tree Height (m)\", y = \"Density\",\n       title = \"Original Height Distributions\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/standard-species-1.png){width=960}\n:::\n:::\n\n\n## After standardisation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to z-scores\nz1 <- scale(species1)\nz2 <- scale(species2)\n\n# Plot standardised distributions\ndata.frame(\n  zscore = c(z1, z2),\n  species = rep(c(\"Species 1\", \"Species 2\"), each = 1000)\n) |>\n  ggplot(aes(x = zscore, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Z-score\", y = \"Density\",\n       title = \"Standardised Distributions\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/z-scores-1.png){width=960}\n:::\n:::\n\n\n## Example\nA z-score of 2 means:\n\n- Species 1: trees above 26 m\n- Species 2: trees above 19 m\n- Both are equally tall for their species\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Calculate z-scores for specific heights\nz_26 <- (26 - 20) / 3\nz_26\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nz_19 <- (19 - 15) / 2\nz_19\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n## Working with standardised data: The standard normal curve\n\nOnce we standardise our data to z-scores, we can calculate probabilities without knowing the original mean or standard deviation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a visual of the standard normal curve\nggplot(data.frame(x = c(-3.5, 3.5)), aes(x = x)) +\n  stat_function(fun = dnorm, size = 1, colour = \"black\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"blue\") +\n  annotate(\"text\", x = 0.3, y = 0.1, label = \"Mean = 0\") +\n  labs(x = \"Z-score\", y = \"Density\",\n       title = \"Standard Normal Distribution\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/standard_curve_visual-1.png){width=960}\n:::\n:::\n\n\n## Answering questions with standardised data\n\n1. `pnorm()`: What proportion of values fall below a given z-score?\n2. `qnorm()`: What z-score corresponds to a specific percentile?\n3. `dnorm()`: How likely is a specific z-score to occur?\n\nLet's see how these help us understand our tree height data:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Example: For a tree with z-score = 1.5 (taller than average)\n# What percentage of trees are shorter than this tree?\npnorm(1.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9331928\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# Example: How tall must a tree be to be in the tallest 5%?\n# (Find the z-score at the 95th percentile)\nqnorm(0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.644854\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n# Example: Compare likelihood of average height vs. very tall trees\ndnorm(0)/dnorm(2)  # How much more common is z=0 compared to z=2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.389056\n```\n\n\n:::\n:::\n\n\n## Visualising standard normal functions\n\nThe blue shaded area shows `pnorm(1)` = 0.84, meaning 84% of trees have a z-score less than 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a visual showing pnorm in action\nggplot(data.frame(x = c(-3.5, 3.5)), aes(x = x)) +\n  stat_function(fun = dnorm, size = 1, colour = \"black\") +\n  geom_area(stat = \"function\", fun = dnorm,\n            fill = \"lightblue\", alpha = 0.5, xlim = c(-3.5, 1)) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\", x = -1, y = 0.3,\n           label = \"pnorm(1) = 0.84\\n84% of values\\nbelow z=1\") +\n  labs(x = \"Z-score\", y = \"Density\",\n       title = \"Using pnorm() with the Standard Normal Curve\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/standard_functions_visual-1.png){width=960}\n:::\n:::\n\n\n## Beyond the normal distribution \n\nDifferent data types need different distributions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sequence for x-axis\nx <- seq(-4, 4, length = 100)\n\n# Create data frame for plotting\ndistributions <- data.frame(\n  x = rep(x, 3),\n  density = c(\n    dt(x, df = 5),        # t-distribution\n    dchisq(x + 4, df = 3),# chi-square (shifted)\n    dnorm(x)              # normal for comparison\n  ),\n  Distribution = rep(c(\"t\", \"Chi-square\", \"Normal\"), each = 100)\n)\n\n# Plot distributions\nggplot(distributions, aes(x = x, y = density, colour = Distribution)) +\n  geom_line(size = 1) +\n  labs(x = \"Value\", y = \"Density\",\n       title = \"Different Types of Data, Different Distributions\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/other-distributions-1.png){width=960}\n:::\n:::\n\n\n## Beyond the normal distribution \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot distributions\nggplot(distributions, aes(x = x, y = density, colour = Distribution)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"Value\", y = \"Density\",\n    title = \"Different Types of Data, Different Distributions\"\n  ) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/other-distributions2-1.png){width=960}\n:::\n:::\n\n\n### We use these for:\n\n- Comparing means of small samples (t-distribution)\n- Analysing counts and proportions (Chi-square)\n- Testing other hypotheses (many more distributions)\n\nWe'll learn when to use each one as we need them.\n\n\n# Sampling and sampling distributions\n\n## What is a sample?\n  \nA sample is a subset of a population used to represent the entire group. It allows us to make inferences about the population without examining every member. \n\n- **Population**: all possible measurements\n- **Sample**: a subset of the population\n- **Sample size**: number of measurements in the sample\n- **Sampling distribution**: the distribution of sample means from multiple samples\n\n\n## Sampling distribution\n\nA sampling distribution tells us:\n\n- How sample statistics (like means) vary\n- What patterns emerge when we take many samples\n- Why we get different results with different samples\n\nThink of it as **distribution of sample statistics**, not individual measurements. **Your variable and all its values are a *single* number in this distribution.**\n\n## Taking samples from our rainfall data\n\nLet's take samples from our skewed rainfall data and see what happens:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Our population: all rainfall measurements (simulated)\nset.seed(123)\nrainfall_population <- rainfall_data$rainfall\n\n# Take a small sample of 5 days\nsmall_sample <- sample(rainfall_population, size = 5)\n\n# Calculate the sample mean\nmean(small_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.49786\n```\n\n\n:::\n:::\n\n\nIf we take another sample, we get a different mean:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Take another sample\nanother_sample <- sample(rainfall_population, size = 5)\nmean(another_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.656836\n```\n\n\n:::\n:::\n\n\nEach sample gives us a slightly different estimate of the true mean rainfall.\n\n## Distribution of sample means for rainfall\n\nWhat happens if we take many samples and look at all their means?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Take 1000 samples of size 5 and calculate their means\nset.seed(456)\nrain_means_n5 <- replicate(1000, mean(sample(rainfall_population, size = 5)))\n\n# Plot the distribution of sample means\nggplot(data.frame(sample_mean = rain_means_n5), aes(x = sample_mean)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", colour = \"white\") +\n  geom_vline(xintercept = mean(rainfall_population),\n             colour = \"red\", linetype = \"dashed\") +\n  annotate(\"text\", x = mean(rainfall_population) + 1, y = 80,\n           label = \"Population mean\", colour = \"red\") +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Frequency\",\n       title = \"Distribution of Sample Means (n = 5)\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/many_rainfall_samples-1.png){width=960}\n:::\n:::\n\n\nNotice how the sample means are distributed around the true population mean, but the distribution is still somewhat skewed.\n\n## Effect of sample size on rainfall means\n\nWhat happens when we increase our sample size?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Take 1000 samples of different sizes\nset.seed(404)\nrain_means_n1 <- replicate(1000, mean(sample(rainfall_population, size = 1)))\n# We already have n5 from previous slide\nrain_means_n15 <- replicate(1000, mean(sample(rainfall_population, size = 15)))\nrain_means_n20 <- replicate(1000, mean(sample(rainfall_population, size = 20)))\nrain_means_n30 <- replicate(1000, mean(sample(rainfall_population, size = 30)))\nrain_means_n100 <- replicate(1000, mean(sample(rainfall_population, size = 100)))\n\n# Combine data for comparison with more sample sizes\nrain_means_df <- data.frame(\n  sample_mean = c(rain_means_n1, rain_means_n5, rain_means_n15,\n                 rain_means_n20, rain_means_n30, rain_means_n100),\n  sample_size = factor(rep(c(1, 5, 15, 20, 30, 100), each = 1000),\n                      levels = c(1, 5, 15, 20, 30, 100))\n)\n\n# Create faceted plot\nggplot(rain_means_df, aes(x = sample_mean)) +\n  geom_density(fill = \"skyblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(rainfall_population),\n             colour = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ sample_size, ncol = 3, scales = \"free_y\",\n             labeller = labeller(sample_size = function(x) paste(\"n =\", x))) +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Density\",\n       title = \"Effect of Sample Size on Distribution of Sample Means\") +\n  theme(strip.background = element_rect(fill = \"lightgrey\"),\n        strip.text = element_text(size = 12, face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/rainfall_sample_sizes-1.png){width=960}\n:::\n:::\n\n\nAs sample size increases, something remarkable happens:\n\n1. The distribution becomes more symmetrical and bell-shaped\n2. The spread of sample means decreases\n3. The distribution approaches a normal distribution - **did it become better with 100 samples?**\n\n## Zoomed-in\n\nWhat happens when we increase our sample size?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rain_means_df, aes(x = sample_mean)) +\n  geom_density(fill = \"skyblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(rainfall_population),\n             colour = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ sample_size, ncol = 3, scales = \"free\",\n             labeller = labeller(sample_size = function(x) paste(\"n =\", x))) +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Density\",\n       title = \"Effect of Sample Size on Distribution of Sample Means\") +\n  theme(strip.background = element_rect(fill = \"lightgrey\"),\n        strip.text = element_text(size = 12, face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/rainfall_sample_sizes_zoom-1.png){width=960}\n:::\n:::\n\n\n\nAs sample size increases, something remarkable happens:\n\n1. The distribution becomes more symmetrical and bell-shaped\n2. The spread of sample means decreases\n3. The distribution approaches a normal distribution\n\n# The Central Limit Theorem\n\n> I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the law of frequency of error.  The law would have been personified by the Greeks if they had known of it.  It reigns with serenity and complete self-effacement amidst the wildest confusion.  The larger the mob, the greater the apparent anarchy, the more perfect is its sway.  It is the supreme law of unreason.\" \n\n-- Sir Francis Galton [(source)](https://www.ucl.ac.uk/taxome/jim/Mim/galton.html) (1822-1911)\n\n## What is the CLT?\n\nThe Central Limit Theorem (CLT) states that:\n\nWhen we take sufficiently large random samples from any population:\n\n1. The distribution of **sample means** will be approximately normal\n2. The mean of the sample means will equal the population mean\n3. The standard deviation of sample means (standard error) equals $\\frac{\\sigma}{\\sqrt{n}}$\n\nThis is true regardless of the shape of the original population distribution.\n\n## Comparing original data with sampling distribution\n\nLet's compare our original skewed rainfall data with the distribution of sample means:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot original population and sampling distributions\np1 <- ggplot(data.frame(x = rainfall_population), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1,\n                fill = \"steelblue\", colour = \"white\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(rainfall_population),\n                           sd = sd(rainfall_population)),\n                colour = \"red\", linetype = \"dashed\") +\n  labs(x = \"Rainfall (mm)\", y = \"Density\",\n       title = \"Original Population\\n(Skewed)\")\n\np2 <- ggplot(data.frame(x = rain_means_n30), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.5,\n                fill = \"skyblue\", colour = \"white\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(rain_means_n30),\n                           sd = sd(rain_means_n30)),\n                colour = \"red\") +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Density\",\n       title = \"Sampling Distribution\\n(n = 30)\")\n\nplot_grid(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/clt_comparison-1.png){width=960}\n:::\n:::\n\n\nEven with skewed data, the sampling distribution becomes normal as sample size increases!\n\n## The CLT with binary biological data\n\nIn biology, many processes have binary outcomes: a gene is expressed or not, a cell divides or not, an organism survives or dies. Let's demonstrate the CLT with gene expression data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a binary population (gene expression: 0 = off, 1 = on)\n# Assume the gene is expressed in 30% of cells\nset.seed(123)\nn_cells <- 100000\np_expressed <- 0.3\ngene_expression <- rbinom(n_cells, size = 1, prob = p_expressed)\n\n# Plot the original population distribution\nggplot(data.frame(expression = factor(gene_expression)), aes(x = expression)) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Gene Expression (0 = off, 1 = on)\", y = \"Frequency\",\n       title = \"Original Population: Binary Gene Expression\",\n       subtitle = \"Gene expressed in 30% of cells\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/binary_population-1.png){width=960}\n:::\n:::\n\n\nThis is a highly non-normal distribution - it's discrete with only two possible values!\n\n## Sampling distribution with increasing sample sizes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define sample sizes (powers of 2)\nsample_sizes <- c(1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048)\n\n# Function to generate sample means for a given sample size\ngenerate_sample_means <- function(sample_size, n_samples = 1000) {\n  replicate(n_samples, mean(sample(gene_expression, size = sample_size, replace = TRUE)))\n}\n\n# Generate sample means for each sample size\nset.seed(456)\nall_sample_means <- lapply(sample_sizes, generate_sample_means)\nnames(all_sample_means) <- paste(\"n =\", sample_sizes)\n\n# Combine into a data frame\nsample_means_df <- data.frame(\n  sample_mean = unlist(all_sample_means),\n  sample_size = factor(rep(names(all_sample_means), each = 1000),\n                      levels = names(all_sample_means))\n)\n\n# Create faceted plot with density plots instead of histograms\nggplot(sample_means_df, aes(x = sample_mean)) +\n  geom_density(fill = \"purple\", color = \"white\", alpha = 0.7) +\n  geom_vline(xintercept = mean(gene_expression),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ sample_size, scales = \"free_y\", ncol = 4) +\n  labs(x = \"Sample Mean (Proportion of Cells with Gene Expressed)\",\n       y = \"Density\",\n       subtitle = \"Red line shows true population mean (0.3)\") +\n  theme(strip.background = element_rect(fill = \"lavender\"),\n        strip.text = element_text(size = 10, face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/binary_clt_samples-1.png){width=960}\n:::\n:::\n\n\nEven with the most non-normal data possible (binary data), the sampling distribution of means becomes normal with sufficient sample size.\n\n## Another example: Seed germination (discrete data)\n\nLet's explore another example with a discrete, highly skewed distribution:\n\nIn ecological restoration, practitioners need to estimate germination rates of native seeds. Imagine each seed has only a 10% chance of germinating (p = 0.1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a binomial population - number of germinated seeds in batches of 20\nset.seed(789)\nn_trials <- 20  # 20 seeds per batch\np_success <- 0.1  # 10% germination rate\nn_batches <- 10000  # 10,000 batches to create our \"population\"\n\n# Generate the population data\ngermination_population <- rbinom(n_batches, size = n_trials, prob = p_success)\n\n# Plot the population distribution\nggplot(data.frame(germinated = germination_population), aes(x = germinated)) +\n  geom_bar(fill = \"darkgreen\", color = \"white\") +\n  scale_x_continuous(breaks = 0:10) +\n  labs(x = \"Number of Seeds Germinated (out of 20)\",\n       y = \"Frequency\",\n       title = \"Seed Germination Population Distribution\",\n       subtitle = \"Binomial(n=20, p=0.1): Discrete and highly skewed\")\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/seed_germination_population-1.png){width=960}\n:::\n:::\n\n\nThis distribution is:\n- Discrete (only whole numbers of seeds can germinate)\n- Highly skewed (most batches have very few germinating seeds)\n- Very different from our continuous rainfall example\n\n## Sampling distribution for seed germination data\n\nWhat happens when we take samples from this discrete, skewed distribution?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Take samples of different sizes and calculate means\nset.seed(123)\nseed_means_n5 <- replicate(1000, mean(sample(germination_population, size = 5)))\nseed_means_n30 <- replicate(1000, mean(sample(germination_population, size = 30)))\n\n# Calculate theoretical standard error for n=30\nse_n30 <- sd(germination_population)/sqrt(30)\n\n# Plot the original population vs. sampling distributions\np1 <- ggplot(data.frame(x = germination_population), aes(x = x)) +\n  geom_bar(fill = \"darkgreen\", color = \"white\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 2)) +\n  labs(x = \"Seeds Germinated\", y = \"Frequency\",\n       title = \"Original Population\\n(Discrete & Skewed)\")\n\np2 <- ggplot(data.frame(x = seed_means_n5), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.2,\n                fill = \"lightgreen\", colour = \"white\") +\n  geom_vline(xintercept = mean(germination_population),\n             colour = \"red\", linetype = \"dashed\") +\n  labs(x = \"Sample Mean (n = 5)\", y = \"Density\",\n       title = \"Sampling Distribution\\nSmall Samples\")\n\n# For n=30, use density scale and properly scaled normal curve\np3 <- ggplot(data.frame(x = seed_means_n30), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.1,\n                fill = \"lightgreen\", colour = \"white\") +\n  geom_vline(xintercept = mean(germination_population),\n             colour = \"red\", linetype = \"dashed\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(germination_population),\n                           sd = se_n30),\n                colour = \"red\", size = 1) +\n  labs(x = \"Sample Mean (n = 30)\", y = \"Density\",\n       title = \"Sampling Distribution\\nLarger Samples\") +\n  # Set x-axis limits to focus on the relevant range\n  xlim(mean(germination_population) - 4*se_n30,\n       mean(germination_population) + 4*se_n30)\n\n# Arrange plots\nplot_grid(p1, p2, p3, ncol = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-04_files/figure-revealjs/seed_sampling_distribution-1.png){width=960}\n:::\n:::\n\n\nEven with discrete, highly skewed data, the CLT still applies:\n\n- The sampling distribution becomes more normal as sample size increases\n- The sampling distribution centers on the population mean (≈ 2, since 20 × 0.1 = 2)\n- The standard error decreases as sample size increases\n\n## Standard error: Measuring the precision of sample means\n\nThe standard error (SE) tells us how much sample means typically vary:\n\n$$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n\nWhere:\n- $\\sigma$ is the population standard deviation\n- $n$ is the sample size\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Calculate theoretical standard errors\npop_sd <- sd(rainfall_population)\nse_n5 <- pop_sd / sqrt(5)\nse_n15 <- pop_sd / sqrt(15)\nse_n30 <- pop_sd / sqrt(30)\n\n# Compare standard errors\nse_n5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.89159\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nse_n15\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.66946\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nse_n30\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.180487\n```\n\n\n:::\n:::\n\n\nAs sample size increases, standard error decreases, making our estimates more precise.\n\n## Standard error vs. standard deviation\n\nWhile we have been using the standard deviation to measure variability in our data, the standard error measures variability in our **sample means**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create comparison table between Standard Deviation and Standard Error\nsd_se_comparison <- data.frame(\n  Aspect = c(\"Definition\", \"Formula\", \"Measures\", \"Effect of sample size\", \"Typical use\"),\n  Standard_Deviation = c(\n    \"Measures spread of individual data points around their mean\",\n    \"σ = √[Σ(x-μ)²/N] or s = √[Σ(x-x̄)²/(n-1)]\",\n    \"Variability within a dataset\",\n    \"Unaffected by sample size\",\n    \"Describing the spread of data in a single sample\"\n  ),\n  Standard_Error = c(\n    \"Measures precision of a sample statistic (usually the mean)\",\n    \"SE = σ/√n or SE = s/√n\",\n    \"Variability between sample means from the same population\",\n    \"Decreases as sample size increases (∝ 1/√n)\",\n    \"Inferential statistics, confidence intervals, hypothesis testing\"\n  )\n)\n\nkable(sd_se_comparison, \n      col.names = c(\"Aspect\", \"Standard Deviation (SD)\", \"Standard Error (SE)\"),\n      caption = \"Standard Deviation vs. Standard Error\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Standard Deviation vs. Standard Error\n\n|Aspect                |Standard Deviation (SD)                                     |Standard Error (SE)                                              |\n|:---------------------|:-----------------------------------------------------------|:----------------------------------------------------------------|\n|Definition            |Measures spread of individual data points around their mean |Measures precision of a sample statistic (usually the mean)      |\n|Formula               |σ = √[Σ(x-μ)²/N] or s = √[Σ(x-x̄)²/(n-1)]                    |SE = σ/√n or SE = s/√n                                           |\n|Measures              |Variability within a dataset                                |Variability between sample means from the same population        |\n|Effect of sample size |Unaffected by sample size                                   |Decreases as sample size increases (∝ 1/√n)                      |\n|Typical use           |Describing the spread of data in a single sample            |Inferential statistics, confidence intervals, hypothesis testing |\n\n\n:::\n:::\n\n\n\n## Practical implications of the CLT\n\nThe Central Limit Theorem enables modern inferential statistics:\n\n1. **Making inferences from samples to populations**\n   - Estimate population parameters with quantifiable precision\n   - Calculate confidence intervals for parameters (coming soon!)\n   - Perform hypothesis tests with known error rates\n2. **Works regardless of population distribution**\n   - Non-normal data still produces normally distributed sample means\n   - Enables parametric tests even when original data is skewed\n   - Only requires sufficiently large samples (n ≥ 30 is often adequate)\n3. **Foundation for statistical theory**\n\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}