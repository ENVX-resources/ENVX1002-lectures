{
  "hash": "f1c9d5e2a8b062479e1c050f843fbf0d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 5 -- Introduction to hypothesis testing\nauthor: Floris van Ogtrop\nformat: soles-revealjs\nembed-resources: true\n---\n\n\n\n# Quick reminder on Project 1\n\n**Before submitting to Canvas** please check that you have `embed-resources: true` in your YAML header.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**`embed-resources: true`**\n\n![](images/with-embed.png){.lightbox style=\"border: 1px solid forestgreen;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n**`embed-resources: false` or not included**\n\n![](images/no-embed.png){.lightbox style=\"border: 1px solid grey;\"}\n\n:::\n::::\n\n# Outline\n\n- Inferential statistics\n- Confidence intervals\n- Hypothesis testing\n- 1-sample tests\n\n\n# Inferential statistics\n\n- You should be familiar with **EDA** by now\n- Now we are going to take the next step into the world of **inferential statistics**\n- This is an important and sometimes difficult step in statistics\n- We are going to draw conclusions about a population from a “smaller” sample of that population\n\n\n# Hypothesis testing\n\n- A hypothesis consists either of a suggested explanation for a phenomenon or  of  a  reasoned  proposal  suggesting  possible  relationships  between multiple phenomena\n\n**Why?**\n\n- To make evidence based decisions, we need to evaluate the evidence\n- Hypothesis testing is a scientific method for weighing up the evidence given the data against the given hypothesis (model)\n\n**Evidence?**\n\n- The data is not consistent with the hypothesis of the gap between the observed value (data) and the expected value is too big (> 2 or 3 Standard Errors)\n\n\n\n# Hypothesis testing\n\n**Framework**\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- Null Hypothesis (H0) vs Alternative Hypothesis (H1)\n- Is the data consistent with H0? Wait, but I’m interested in if there is a difference?\n  - Falsifiability: only takes one contrary observation to falsify a statement (Karl Popper) \n    - Can we falsify this statement? \"There are invisible and undetectable fairies in my garden.\" - it is non-falsifiable so we cannot test it with empirical data.\n    - Can we falsify this statement? \"All swans are black\" - if we observe a single white swan, then the statement is falsified.\n\n:::\n\n::: {.column width=\"50%\"}\n\n![generated by DALL-E 3](images/DALL·E 2024-02-10 16.01.27.png)\n\n:::\n\n::::\n\n\n\n# Hypothesis testing\n\n![Underwood, A.J., 2009, April. Components of design in ecological field experiments. In Annales Zoologici Fennici (Vol. 46, No. 2, pp. 93-111). Finnish Zoological and Botanical Publishing Board.](images/underwood.png)\n\n\n# Hypothesis testing\n\n**We will follow the steps in hypothesis testing**\n\n- Choose level of significance (usually 0.05)\n- Write null and alternate hypotheses\n- Check assumptions\n- Calculate test statistic\n- Obtain P-value or critical value\n- Make statistical conclusion\n- Write a scientific (biological) conclusion\n\n\n# Hypothesis testing\n\n**Acronyms**\n\nSometimes we use HATPC to remember these steps - HAT Prevent Cancer - come up with your own\n\n- **H:** Hypothesis\n- **A:** Assumptions\n- **T:** Test statistic\n- **P:** P-Value\n- **C:** Conclusion\n\n\n# Hypothesis (H)\n\n- The **null hypothesis** $(H_0)$ assumes that the difference between the observation (data) and the expected value is due to chance\n- The **alternative hypothesis** $(H_1)$ assumes that the difference between the observed value (data) and expected value is NOT due to chance alone.\n\n\n# Assumptions (A)\n\n- We need to check the assumptions of the test we are using.\n- For example, if we are using a one sample t-test we need to check the assumption of normality.\n- This is done by 1) looking at the distribution of our data (EDA) and 2) using a hypothesis test such as the Shapiro-Wilk test.\n- If the assumptions are not met, we may need to transform the data or use a non-parametric test.\n\n\n# Test Statistic (T)\n\nMeasures the difference between what is observed in the data and what is expected from the null hypothesis.\n\n$\\text{Test statistic}=\\frac{\\text{Observed value - Expected value}}{\\text{Standard error}}$\n\n\n# P-Value (P)\n\n**What is P-value?**\n\n- The P-value is a way of weighing up whether the sample (data) is consistent with $H_0$ \n\nMore formally:\n\n- The P-value is the chance of observing the test statistic (or something more extreme) if the $H_0$ is true.\n\n\n# P-Value (P)\n\n**What does this have to do with sampling distributions??**\n\n![](images/two_tail.png)\n\n\n# P-Value (P)\n\n**\"If the P-value is low the Null hypothesis must go.\"** \n\n- So we calculate the P-value to assess the level of evidence against the Null hypothesis.\n  - Generally if a P-value > 0.05 we say there is **no strong** evidence to reject the null hypothesis (we fail to reject it)\n  - We never say we accept the null hypothesis!!\n  - 0.05 is called the significance level\n  - We may choose other significance levels such as 0.2, 0.01, 0.001 (for this course we will use 0.05)\n\n\n# P-Value (P)\n\n**Common mistakes**\n\n- The p-value is not the chance that the NULL hypothesis is true\n- A large p-value does not mean that $H_0$ is true\n\n![](images/p_value.png)\n\n- The use of 0.05 is not mandatory, it is a convention.\n\n\n# P-value (P)\n\n**Controversy**\n\n[Nature article 1](https://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503) \n\n[Nature article 2](https://www.nature.com/articles/d41586-019-00857-9) \n\n- Should we move away from an over reliance on p-values towards more robust statistical methods and a greater emphasis on the size and reproducibility of effects?\n- Look up terms like **\"P Hacking\"**...\n\n\n# A little more on the P-value\n\nWhy is the p-value 0.05?\n\n- The p-value is the probability of observing a test statistic as extreme as the one calculated, assuming the null hypothesis is true.\n- It is based on the idea we 1 in 20 times we will reject the null hypothesis when it is true.\n\n\n# Conclusion (C)\n\n- We make a statistical conclusion based on the p-value and the level of significance.\n- We also make a scientific (biological) conclusion based on the statistical conclusion.\n\n\n# Tails\n\n**Tails??**\n\n![](images/tails.png)\n\n- In this course we will mostly use two-tailed tests because we are interested in whether the observed value is different to the expected value.\n- One tailed tests are used when we are specifically interested in whether the observed value is greater or less than the expected value. Examples are in quality control and production processes.\n\n\n# Tails\n\n- Let say we would like to test whether a sample or population behaves according to a known or expected value. For example, ENVX1002 students have a mean heart rate of 70 beats per minute $(c)$. We could test the relationship between our population or sample mean $(\\mu)$ or $(y)$ and our expected mean $(c)$.\n\n- $H_0:\\mu=c$ *Two tailed*\n- $H_1:\\mu \\ne c$ *Two tailed*\n\n- $H_0:\\mu \\ge c$ *One tailed - greater than*\n- $H_1:\\mu < c$ *One tailed - greater than*\n\n- $H_0:\\mu \\le c$ *One tailed - less than*\n- $H_1:\\mu > c$ *One tailed - less than*\n  \n- We usually use the first (two tailed) form unless we are specifically interested in testing whether the difference is in one direction or the means of two variables are equivalent (i.e. equivalence testing in production)\n- Initially we are assuming that the population/sample is normally distributed. Later we will look at what to do when this is not the case.\n\n\n# Type error??\n\n**Type I error**\n\n- False positive: we reject H0  when it is true \n\n**Type II error**\n\n- False negative: we accept H0  when it is false\n\n![](images/error.png)\n\n**Note:** Decreasing the significance level ($\\alpha$) to say 0.01 will increase the probability of type II error\n\n\n# Sample size and power\n\n**Sample size & Power**\n\n- While we won't do these calculations, it is important to think about sample size and power. For example, when comparing two means we can estimate the sample size required using the following \n\n$n = \\left( \\frac{(Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot (SD_1^2 + SD_2^2)}{\\delta^2} \\right)$\n\n- $n$ represents the required sample size for each group.\n- $Z_{\\alpha/2}$ denotes the critical value of the normal distribution at the desired significance level ($\\alpha$). For example, for a 95% confidence level ($\\alpha = 0.05$,$Z_{\\alpha/2}=1.96$.\n- $Z_{\\beta}$ is the Z-score corresponding to the desired power of the test $(1 - \\beta)$. For 80% power $(\\beta=0.2)$, you would use the Z-score that corresponds to 80% in the normal distribution table.\n- $SD_1$ and $SD_2$ are the standard deviations of the two groups being compared.\n- $\\delta$ is the minimum difference in means between the two groups that the study is designed to detect, also known as the effect size.\n\n\n# Sample size and power\n\n**Sample size & Power**\n\nIn a nut shell...\n\n**High Power:** If a test has high power (close to 1), it means that the probability of committing a Type II error is low. This indicates that the test is very likely to detect an effect or difference when one exists.\n\n**Low Power:** Conversely, if a test has low power, the probability of committing a Type II error is high. This means the test is less likely to detect an effect or difference, even if one exists.\n\n**Sample size:** Generally power increases as sample size increases, but so does time and costs!\n\n\n# The t-distribution and degrees of freedom?\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nThe T-distribution has the following properties:\n\n-\tIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\n\n-\tThe area under the curve = 1, as is the case for all continuous probability distributions.\n\n-\tThe probability density function is defined by three parameters, the mean $\\mu$, the standard deviation $\\sigma$ and the sample size $n$. (The normal distribution only has two parameters)\n\n-\tThe exact shape of the t distribution depends on the quantity called degrees of freedom, df. The df = n – 1 for any t distribution.\n\n:::\n\n::: {.column width=\"50%\"}\n\n![Comparing the shapes of the Student’s T and the Z (normal) curve](images/tdist.png)\n\n:::\n\n::::\n\n\n# Example 1: One sample t-test\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![Generated using DALL.E 3](images/DALL·E 2024-02-11 10.31.33_hippie.png)\n\n:::\n\n::: {.column width=\"50%\"}\n\n<span style=\"color: blue;\">**H**</span>ypothesis\n\nSay our hippie friend here has seen the light after a helping of mung beans and wishes to test whether ENVX1002 student average heart rates differ from the global average of 70 bpm (beats per minute). \n\n**State the Null and Alternate hypothesis!**\n\n- $H_0=70 \\text{ bpm}$\t(Null hypothesis)\n\n- $H_1 \\ne 70 \\text{ bpm}$\t(Alternate hypothesis)\n\n:::\n\n::::\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**A**</span>ssumptions\n\n- This time we are testing a data set where the **variance is unknown**.\n- A key assumption for the one-sample t-test is that the sample is \"reasonably\" normally distributed.\n- Why do we need to test the assumption of normality?\n- Think about if our distribution is right skewed, then our *mean* may not reflect the central tendency of our sample population.\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**A**</span>ssumptions\n\n**Small Samples:** With small samples (typically n < 30), even slight deviations from normality can significantly impact the t-test's validity because the t-test relies heavily on the assumption of normality in this scenario. However, if the data are symmetrically distributed and don't have outliers, the t-test can be robust to mild violations of normality. Note that the t-test adjusts for the sample size, so the larger the sample, the less important the normality assumption becomes.\n\n**Large Samples:** For larger samples (n > 30), the Central Limit Theorem (CLT) suggests that the distribution of the sample means tends to be normal, regardless of the population distribution. This means that for sufficiently large samples, the t-test can be robust to violations of normality. *However, extreme skewness, heavy tails, or outliers can still affect the test's performance*.\n\n\n# Example 1: One sample t-test\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<span style=\"color: blue;\">**A**</span>ssumptions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Assuming 'skewed_sample' is your skewed data. For this example, let's simulate it\nset.seed(0) # For reproducibility\nskewed_sample <- rlnorm(100, meanlog = 0, sdlog = 1) - exp(0.5) # Skewed data, centered around 0\n\n# Create a data frame for ggplot\ndata <- data.frame(value = skewed_sample)\n\n# Plot\np <- ggplot(data, aes(x = value)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = \"lightgreen\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Skewed Distributed Sample\", x = \"Value\", y = \"Density\") +\n  theme_minimal() +\n  geom_vline(aes(xintercept = mean(value)), color = \"blue\", linetype = \"dotted\") +\n  annotate(\"text\", x = mean(skewed_sample), y = 0.35, label = paste(\"Mean:\", round(mean(skewed_sample), 2)), vjust = -0.5)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(p)\n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**A**</span>ssumptions\n\n- Let's help out our hippie friend: We have asked students to measure their heart rate and randomly selected heart rates from 21 students (a *completely randomised design*) in our hippie friends ENVX1002 class. Note we will do this exercise in class!!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nheart_rate <- rnorm(21, mean = 70, sd = 10)\n```\n:::\n\n\n\n- We have a small sample size so often we use a *stem plot* or *box plot* to look at the distribution. Both look quite symmetrical!\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstem(heart_rate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 7\n  5 | 8\n  6 | 01122445559\n  7 | 11345\n  8 | 01\n  9 | 4\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(heart_rate)\n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**A**</span>ssumptions\n\n- We can also look at the 6 number summary and the histogram. The histogram looks quite symmetrical and the 6 number summary is quite symmetrical.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(heart_rate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  46.54   61.63   64.89   67.68   72.77   94.16 \n```\n\n\n:::\n:::\n\n\n\n- We can see that the mean and the median are quite similar. This is a good sign that the data is symmetrical and perhaps normally distributed. \n- We can also start to think how our student heart rate compares to the national average of 70 bpm.\n\n\n# Example: One sample t-test\n\n<span style=\"color: blue;\">**A**</span>ssumptions\n\n- We can also use a hypothesis test called the *Shapiro Test* and use a plot called a *qqplot* (\"Quantile Quantile plot\").\n- We can see that the p-value of the shapiro test > 0.05 - no strong evidence that the distribution is not normal.\n- We can see that the points on the Q-Q plot are reasonably straight and follow the \"line\". \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(heart_rate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  heart_rate\nW = 0.94441, p-value = 0.266\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: panel-tabset\n\n## R base graphics\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nqqnorm(heart_rate)\nqqline(heart_rate)\n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## ggplot2\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nggplot(mapping = aes(sample = heart_rate)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"Q-Q Plot using ggplot2\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## ggpubr\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggpubr)\nggqqplot(heart_rate, \n         main = \"Q-Q Plot using ggpubr\",\n         color = \"blue\") \n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n::::\n\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**T**</span>est statistic\n\n**t-test**\n\n- The formula is the same as the z-test but we use the t-distribution instead of the normal distribution and we use the sample standard deviation instead of the population standard deviation.\n\n$t = \\frac{\\bar{y} - \\mu}{s / \\sqrt{n}}$ population variance unknown (estimate $s^2$)\n\n- $\\bar{y}$ is the sample mean\n- $\\mu$ is the population mean\n- $s$ is the sample standard deviation\n- $n$ is the sample size\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**CI**</span> &\n<span style=\"color: blue;\">**T**</span>-Statistic & \n<span style=\"color: blue;\">**P**</span>-value\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\nLuckily for us we have a function in RStudio that does all of the above in one function!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(heart_rate,\n  mu = 70,\n  alternative = \"two.sided\",\n  conf.level = 0.95\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  heart_rate\nt = -1.0736, df = 20, p-value = 0.2958\nalternative hypothesis: true mean is not equal to 70\n95 percent confidence interval:\n 63.16242 72.19073\nsample estimates:\nmean of x \n 67.67657 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nWe can see the following output:\n\n- the t-statistic = -1.0736\n- the degrees of freedom = 20 $(n-1)$\n- the p-value = 0.2958\n- the 95% confidence interval = (63.16242 72.19073)\n- the sample mean = 67.67657\n\n:::\n\n::::\n\n\n# Example 1: One sample t-test\n\n<span style=\"color: blue;\">**C**</span>onclusion\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![Ceated using DALL.E 3](images/happy_mung.png)\n\n:::\n\n::: {.column width=\"50%\"}\n\nLet's revisit our hippie friend's hypothesis\n\n- $H_0=70 \\text{ bpm}$\t(Null hypothesis)\n\n- $H_1 \\ne 70 \\text{ bpm}$\t(Alternate hypothesis)\n\n- The p-value is 0.2958 which is not less than 0.05. We retain null hypothesis. \n\n- There is no strong evidence to indicate that the mean heart rate of ENVX1002 students is different to the global mean of 70 bpm. We can be 95% confident that the true mean heart rate of 63 and 72 (0 d.p.) beats per minute. Our global mean lies within this interval. Therefore we are petty confident that the class average heart rate is similar to the global average.\n\n:::\n\n::::\n\n\n# Confidence intervals\n\n- A confidence interval is a range of values, derived from a sample, that is likely to contain the value of an unknown population parameter.\n- Confidence are important and many people prefer them to formal hypothesis tests because they give a range of values rather than a single value.\n- There are two types we are going to look at:\n  - Confidence interval where we **know** the population variance\n  - Confidence interval where we **do not know** the population variance \n\n\n# Confidence intervals - known variance\n\n- Assuming our sample comes from a normal population, we can calculate the confidence interval for the mean of a population when the population **variance is known**. An example may be in manufacturing where we know the variance of a process.\n- The formula for the confidence interval is:\n\n$CI = \\bar{y} \\pm Z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}$\n\nfor the 95% confidence interval, $\\alpha = 0.05$ and $Z_{\\alpha/2} = 1.96$ i.e. 2 standard errors from the mean.\n\n\n# Confidence intervals - known variance\n\n- $\\bar y$  is a random variable with a sampling distribution. \n- Because there is an infinite number of values of  $\\bar y$, there is an infinite number of intervals of the form  $CI = \\bar{y} \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}$. \n- The probability statement says that 95% of these intervals will actually include $\\mu$ between the limits. \n- For any one interval,  $CI = \\bar{y} \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}$, we say that we are 95% confident that $\\mu$ lies between these limits. \n\n\n# Confidence intervals - known variance\n\n**Example**\n\nSay we have measured the weights of 6 chocolate bars from Florry's chocolate factory. The readings were:\n\n48   43   45   51   49   40\n\nThe mean for these six values is  $\\bar y = 46$g. Let’s assume that the bar weights follow a normal distribution and that $\\sigma = 1$g These assumptions allow us to calculate a 95% z-based confidence interval:\n\nSo the 95% CI for the current example is \n\n$CI = 46 \\pm 1.96 \\cdot \\frac{1}{\\sqrt{6}}$ or \n$CI = 46 \\pm 0.80$ or $CI = (45.20, 46.80)$\n\nSo we are 95% confident that the true mean weight of the chocolate bars is between 45.20 and 46.80g. \n\n- We will calculate confidence intervals with **unknown variance** in later on.\n\n\n# Confidence intervals - Unknown variance\n\n- We can also calculate a confidence interval when we do not know the population variance.\n- The formula for the confidence interval is:   \n$CI = \\bar{y} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}$ \n\n- For the 95% confidence interval, $\\alpha = 0.05$ and $t_{\\alpha/2} = 2.093$ (for 20 degrees of freedom) \n- The degrees of freedom is $n-1$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqt(0.975, df = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.085963\n```\n\n\n:::\n:::\n\n\n\n\n# Confidence intervals - Unknown variance\n\n- Another interesting result to contemplate can be demonstrated through the following simulation of 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration ($\\mu g / l$) assumed to be $\\sim N(0.3, 0.12)$. For each sample the 95% confidence interval calculated: \n\n![Depiction of confidence intervals from each of 100 simulated samples](images/CI_sim.png)\n\n\n# Confidence intervals - Unknown variance\n\n- Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 ($\\mu g / l$). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\n- We have to accept that 5% of the time we will not capture the true mean.\n- We can widen the confidence interval to 99% so we are more confident but this can increase the chance of a type II error. \n\n\n# Example 2: One sample test\n\nWhat happens when the assumptions are not met?\n\n1. We can try and transform the data to make it more normal (and equal variance for later)\n2. We can use a non-parametric test (later)\n\n\n# Example 2: Data transformations \n\nMy data is not normal :( what can I do?\n  - Sometimes we can transform our data to make it more normal.\n  - Let's look at an example:\n    - Total nitrogen (TN) levels @ Wallacia in western Sydney on the Nepean River\n    - According to the ANZECC guidelines, the maximum acceptable level of TN in an lowland river is 500 $\\mu g/L$ and for an upland river = 250 $\\mu g/l$. (see Table 3.3.2 in Australian and New Zealand Guidelines for Fresh and Marine Water Quality)\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nTN <- read.csv(\"data/TN_Wallacia.csv\")\nstr(TN)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t29 obs. of  1 variable:\n $ TN: int  1020 1120 1170 920 920 1010 850 910 800 710 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(TN$TN, na.rm = TRUE) # na.rm=TRUE if we have missing values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 855.8621\n```\n\n\n:::\n\n```{.r .cell-code}\nmedian(TN$TN, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 800\n```\n\n\n:::\n:::\n\n\n\n\n# Example 2: State Hypothesis\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- $H_0: \\mu = 500$ $\\mu g/l$ (Null hypothesis)\n- $H_1: \\mu \\ne 500$ $\\mu g/l$ (Alternate hypothesis)\n\n:::\n\n::: {.column width=\"50%\"}\n\n![Grose River - Floris van Ogtrop](images/stream1.png)\n\n:::\n::::\n\n\n# Example 2: Check assumptions\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Histogram\np1 <- ggplot(TN, aes(x = TN)) +\n  geom_histogram(\n    fill = \"blue\", color = \"black\",\n    bins = 7\n  ) +\n  xlab(\"Total Nitrogen (ug/L)\") +\n  theme_minimal()\n\n# Boxplot\np2 <- ggplot(TN, aes(x = TN)) +\n  geom_boxplot(fill = \"blue\") +\n  xlab(\"Total Nitrogen (ug/L)\") +\n  theme_minimal()\n\n# Q-Q plot\np3 <- ggplot(TN, aes(sample = TN)) +\n  stat_qq() +\n  stat_qq_line(col = \"red\") +\n  theme_minimal()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Arrange the plots in a 2x2 layout\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n:::\n::::\n\n\n# Example 2: Check assumptions\n\n**Shapiro-Wilk normality test**\n\n- For smaller sample sizes, use the Shapiro-Wilk test (say < 50)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(TN$TN)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  TN$TN\nW = 0.92582, p-value = 0.04293\n```\n\n\n:::\n:::\n\n\n\n- We see P<0.05 so we **reject** the null hypothesis that the data is normally distributed.\n- For really large samples (>>30) the central limit theorem suggests that the distribution of the sample means tends to be normal, regardless of the population distribution. This means that for sufficiently large samples, the t-test can be robust to violations of normality. However, extreme skewness, heavy tails, or outliers can still affect the test's performance.\n\n\n# Example 2: Transformations\n\nFor right (positive) skewed data, we can use a \n\n- $1/x$ inverse transformation for highly skewed data.\n- $\\log_{10}$ or $\\log_e$ transformation for very skewed data. \n- $\\sqrt{}$ square root transformation for moderately skewed data.\n  \nFor left (negative) skewed data (which is rare to find), we can \"try\" a\n\n- $x^2$ or $x^3$ transformation.\n\n\n# Example 2: Transformations & Recheck assumptions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's try a log10 transformation\n# We start by creating a new column in our data frame called log10_TN\n# We then take the log10 of the TN column and store it in the new column\nTN$log10_TN <- log10(TN$TN)\n```\n:::\n\n\n\n\n**Geometric mean**\n\nNote that now we are looking at the geometric mean as opposed the the arithmatic mean which was 855.86 in our case\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n10^mean(TN$log10_TN)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 770.0666\n```\n\n\n:::\n:::\n\n\n\n\n# Example 2: Recheck assumptions\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Histogram\np1 <- ggplot(TN, aes(x = log10_TN)) +\n  geom_histogram(\n    fill = \"blue\", color = \"black\",\n    bins = 7\n  ) +\n  xlab(\"Log10 Total Nitrogen (ug/L)\") +\n  theme_minimal()\n\n# Boxplot\np2 <- ggplot(TN, aes(x = log10_TN)) +\n  geom_boxplot(fill = \"blue\") +\n  xlab(\"log10 Total Nitrogen (ug/L)\") +\n  theme_minimal()\n\n# Q-Q plot\np3 <- ggplot(TN, aes(sample = log10_TN)) +\n  stat_qq() +\n  stat_qq_line(col = \"red\") +\n  theme_minimal()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Arrange the plots in a 2x2 layout\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-05_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n:::\n::::\n\n\n# Example 2: Recheck assumptions\n\n**Shapiro-Wilk normality test**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(TN$log10_TN)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  TN$log10_TN\nW = 0.97279, p-value = 0.6375\n```\n\n\n:::\n:::\n\n\n\n- From our previous slide, our data seems to be quite symmetrical\n- Now P>0.05 so we **fail to reject** the null hypothesis that the data is normally distributed.\n- We can now proceed with our t-test using the log transformed data.\n- **NOTE:** *We will also need to transform the hypothesised mean of 500 to the log10 scale.*\n\n\n# Example 2: T-test & P-Value\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(TN$log10_TN,\n  mu = log10(500),\n  alternative = \"two.sided\",\n  conf.level = 0.95\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  TN$log10_TN\nt = 4.8768, df = 28, p-value = 3.884e-05\nalternative hypothesis: true mean is not equal to 2.69897\n95 percent confidence interval:\n 2.807748 2.965309\nsample estimates:\nmean of x \n 2.886528 \n```\n\n\n:::\n:::\n\n\n\n- The p-value is <0.001 (3.d.p.) which is less than 0.05. Therefore, we reject the null hypothesis. \n- There is strong evidence that the mean value of TN in Nepean River is different to 500 ug/L. \n- Can we say something about the direction??\n\n\n# Example 2: Confidence intervals\n\n- We need to back transform the confidence interval to the original scale.\n- To back-transform the confidence interval we can use the following:\n\n$10^{CI_{low}}$ and $10^{CI_{high}}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n10^(2.81)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 645.6542\n```\n\n\n:::\n\n```{.r .cell-code}\n10^(2.97)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 933.2543\n```\n\n\n:::\n:::\n\n\n\n- This means that we are 95% confident that the geometric mean (back-transformed mean) of the sample is between approximately 646 and 933 ug/l. This is higher than the hypothesised mean.\n\n\n# Example 2: Conclusion\n\n1. The data was log10 transformed to meet the assumptions of the t-test.\n2. We have strong evidence that the mean value of Total Nitrogen in Nepean River is different to 500 ug/L.\n3. We are 95% confident that the geometric mean (back-transformed mean) of the sample is between approximately 646 and 933 ug/l.\n4. Looking at the confidence interval, we can say that the mean value of TN in Nepean River is significantly greater than 500 ug/L.\n\n\n# Example 3: One tailed test\n\n- We can also use a one tailed test if we are specifically interested in whether the observed value is greater or less than the expected value.\n- In this case we only want to know if the TN concentration is greater than the ANZECC guidelines of 500 $\\mu g/l$.\n\n- $H_0: \\mu = 500$ $\\mu g/l$ (Null hypothesis)\n- $H_1: \\mu > 500$ $\\mu g/l$ (Alternate hypothesis)\n\n\n# Example 3: One tailed test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(TN$log10_TN,\n  mu = log10(500),\n  alternative = \"greater\",\n  conf.level = 0.95\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  TN$log10_TN\nt = 4.8768, df = 28, p-value = 1.942e-05\nalternative hypothesis: true mean is greater than 2.69897\n95 percent confidence interval:\n 2.821104      Inf\nsample estimates:\nmean of x \n 2.886528 \n```\n\n\n:::\n:::\n\n\n\n- P < 0.001 (3.d.p.) so we reject the null hypothesis.\n- Because we are doing a one tailed test we can now conclude that the mean value of TN in Nepean River is greater than 500 ug/L.\n- Take a look at the confidence interval and the p-value. What do you notice when comparing to the two tailed test?\n\n\n# Example 3: One tailed test\n\n[Ruxton and Neuhäuser 2010](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210X.2010.00014.x)\n\n![](images/one_tail.png)\n1. Explain why the expected effect is in one direction. Or why the effect in one direction is of more interest than the other?\n2. Explain why a large difference in the opposite direction is not of importance. \n\n# Muddy notes\n\nQuestions? Confused? **Use this QR code and 1-2 minutes to leave a note for me.**\n\n![](images/muddyqr.png)\n\n# References\n\n- Quinn G. P. & Keough M. J. (2002) Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK.\n- Logan, M. (2010). Biostatistical design and analysis using R a practical guide. Hoboken, N.J., Wiley-Blackwell.\n- Fox, G. A., S. Negrete-Yankelevich, and V. J. Sosa. (2015). Ecological statistics: contemporary theory and application. Oxford University Press, USA\n- Ruxton, G.D. and Neuhäuser, M., 2010. When should we use one-tailed hypothesis testing? Methods in Ecology and Evolution, 1 (2), 114-117.\n\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/",
    "supporting": [
      "lecture-05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}