{
  "hash": "50888fa7c29af45ceb0f8e7ca7dcc6b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 10 -- Simple Linear Regression\nauthor: Si Yang Han\nformat: ochre-revealjs\nscrollable: true\n---\n\n## Module overview\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n- [Week 9. Describing Relationships]{style=\"color: #D0D3D4;\"}\n    + [Correlation (calculation, interpretation)]{style=\"color: #D0D3D4;\"}\n    + [Regression (model structure, model fitting]{style=\"color: #D0D3D4;\"}\n    + [What/when/why/how]{style=\"color: #D0D3D4;\"}\n\n- **Week 10. Simple Linear Regression**\n    + Can we use the model?(assumptions, hypothesis testing)\n    + How good is the model?(interpretation, model fit)\n    \n- [Week 11. Multiple Linear Regression]{style=\"color: #D0D3D4;\"}\n    + [Multiple Linear Regression (MLR) modelling]{style=\"color: #D0D3D4;\"}\n    + [Assumptions, interpretation and the principle of parsimony]{style=\"color: #D0D3D4;\"}\n\n- [Week 12. Nonlinear Regression]{style=\"color: #D0D3D4;\"}\n    + [Common nonlinear functions]{style=\"color: #D0D3D4;\"}\n    + [Transformations]{style=\"color: #D0D3D4;\"}\n\n## Last week...\n\n- Correlation $r$: a measure of the strength and direction of the linear relationship between two variables\n- Is there a moderate to strong *causal* relationship?\n  \n. . .\n\n### Simple linear regression modelling\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$ \n\n\n> Basically, a deterministic straight line equation $y=c+mx$, with added random variation that is normally distributed\n\n$$ Y = c + mx + \\epsilon $$\n\n\n## Fitting the line \n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$ \n\n$$ Y = c + mx + \\epsilon $$\n\nHow do we fit a line to data if data are \"noisy\"?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx <- 1:10\ny <- 2 * x + rnorm(10, 0, 2)\n# generate y with predicted values\ny_pred <- 2 * x\ndf <- data.frame(x, y)\n\np1 <- ggplot(df, aes(x, y_pred)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  labs(x = \"x\", y = \"y\", title = \"A\")\n\np2 <- ggplot(df, aes(x, y)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"royalblue\") +\n  labs(x = \"x\", y = \"y\", title = \"B (How do we fit this?)\")\n\nlibrary(patchwork)\np1 + p2 + plot_layout(ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-2-1.png)\n:::\n:::\n\n\n## Usage of least squares\n\n- **Student's t-test** (indirectly)\n- **linear regression**\n  \n. . .\n\n- **nonlinear regression (logistic, polynomial, exponential, etc.)**\n- analysis of variance (ANOVA)\n- generalised linear model\n- principle component analysis\n- machine learning models\n- etc...\n\n\n## Galton's data revisited\n\n- Galton's data on the heights of parents and their children.\n- Is there a relationship between the heights of parents and their children?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(HistData)\ndata(Galton)\nfit <- lm(child ~ parent, data = Galton)\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  labs(x = \"Parent height (inches)\", y = \"Child height (inches)\")\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n**How did we end up with the line in the plot above?**\n\n## How do we analytically fit a line?\n\nWe calculate slope ($\\beta_1$):\n\n$$ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} $$\nThen substitute below to get the intercept ($\\beta_0$):\n\n$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$\nImagine a dataset with a million points - this would be computationally taxing.\n\n## How do we numerically fit a line?\n\n- *Minimise* the sum of the squared residuals via trial and error\n- Most common method used via computers (gradient-descent)\n- Can be done by hand (but not recommended)\n\n$$\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)}\\color{black})^2$$ \n\n![[Source](https://github.com/Enchufa2/ls-springs)](images/leastsquares.gif){fig-align=\"center\"}\n\n## Fitting a linear model in R\n\nIs there a relationship between the heights of parents and their children?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(child ~ parent, data = Galton)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n```\n\n\n:::\n:::\n\n\n\n$$ \\widehat{child} = 23.9 + 0.646 \\cdot parent$$\n\n. . .\n\nDo we trust our model? How good is the model? How can we interpret the results?\n\n## Steps for Regression\n\n1.  [Understand the variables]{style=\"color: #D0D3D4;\"}\n2.  [Explore data]{style=\"color: #D0D3D4;\"}\n3.  [Fit model]{style=\"color: #D0D3D4;\"}\n4.  Check assumptions\n5.  Assess model fit\n6.  Interpret output\n\n# Assumptions\n\n> The data **must** meet certain criteria for linear regression, which we often call *assumptions*.\n\n## Assumptions - LINE\n\n- **L**inearity. The relationship between $y$ and $x$ is linear.\n- **I**ndependence. The errors $\\epsilon$ are independent.\n- **N**ormal. The errors $\\epsilon$ are normally distributed.\n- **E**qual Variance of errors $\\epsilon$. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.\n\n. . .\n\n### Why do we care?\n\n- If the assumptions are met, then we can be confident that the model is a good representation of the data.\n- If they are *not* met, the results are still presented, but our interpretation of the model is likely to be flawed.\n  - Hypothesis test results are unreliable\n  - Standard error is unreliable\n  - Poor estimates of coefficients = poor predictions\n\n## How do we check the assumptions?\n\nRecall that the linear model is a **deterministic straight line equation** $y = c + mx$ plus some **random noise** $\\epsilon$:\n\n$$ Y_i = \\beta_0 + \\beta_1 x + \\epsilon $$\n\n- **If the only source of variation in $y$ is $\\epsilon$, then we can check our assumptions by just looking at the residuals $\\hat{\\epsilon}$.**\n\n. . .\n\n:::{.callout-tip}\nAll but the independence assumption can be assessed using diagnostic plots. R will not warn you if the assumptions are not met. It is up to you to check them!\n:::\n\n## How do we get the residuals?\n\n- Fit the model!\n- Residuals need to be calculated from the model, not from the raw data.\n- In R, these values are stored automatically.\n\n$$ \\color{firebrick}{\\hat{\\epsilon_i}} = \\color{royalblue}{y_i} - \\color{forestgreen}{\\hat{y_i}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# simulate example data\nset.seed(340)\nx <- runif(8, 0, 30)\ny <- 5 * x + rnorm(8, 0, 40)\ndf <- data.frame(x, y)\n\n# fit linear model, add residual vertical lines as arrows\nmod <- lm(y ~ x, data = df)\np1 <- ggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_segment(aes(xend = x, yend = fitted(mod)),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    color = \"royalblue\"\n  ) +\n  labs(x = \"x\", y = \"y\")\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  annotate(\"text\",\n    x = 6.3, y = -6, size = 7,\n    label = expression(hat(epsilon[i])), colour = \"royalblue\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = 25, size = 7,\n    label = expression(hat(y[i])), colour = \"forestgreen\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = -36, size = 7,\n    label = expression(y[i]), colour = \"firebrick\"\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-5-1.png)\n:::\n:::\n\n\n## Another way to look at residuals\n\n![](images/residual.jpg){fig-align=\"center\"}\nFor a given 'x', there is a range of 'y' values that are possible.\n\n- e.g. For a given parent height, there is a range of child heights that are possible.\n\n# Checking assumptions\n\nlinearity | normality | equal variance | outliers\n\n## 1-step\n\n- **Residuals vs. Fitted**: check for linearity, equal variance.\n- **Q-Q Residuals**: check for normality.\n- **Scale-Location**: check for equal variance (standardised).\n- **Residuals vs. Leverage**: check for outliers (influential points).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # need to do this to get 4 plots on one page\nplot(fit)\n```\n:::\n\n\n---\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-7-1.png)\n:::\n:::\n\n\n## Assumption: Linearity\n\n- Residuals vs. fitted plot looks at the relationship between the residuals and the fitted values.\n- If the relationship is linear:\n  - Residuals should be randomly scattered around the horizontal axis.\n  - The red line should be reasonably straight.\n- Could also look at a scatterplot of x and y!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit, which = 1)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-8-1.png)\n:::\n:::\n\n\n## Examples\n\n![](images/residual_vs_fitted.png){fig-align=\"center\"}\n<br>\n\nLinear Models with R (Faraway 2005, p59)\n\n## Assumption: Normality\n\n- Q-Q plot looks at the distribution of the residuals against a normal distribution function (the dotted line).\n- Sometimes, a histogram is still useful to see the shape of the distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(fit, which = 2)\nhist(rstandard(fit))\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-9-1.png)\n:::\n:::\n\n\n## Assumption: Normality\n\n- If **normally distributed**, the points should follow the red line.\n- Deviation from the red line is common in the tails (i.e. the ends), but not in the middle.\n\n. . .\n\n### Tips\n\n- **Light-tailed**: small variance in residuals, resulting in a narrow distribution.\n- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution.\n- **Left-skewed** (n shape): more data falls to the left of the mean.\n- **Right-skewed** (u shape): more data falls to the right of the mean.\n\n:::{.callout-tip}\nLeft or right-skewed? Look at where the tail points. \n:::\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(915)\nx <- rnorm(100)\ny <- 2 + 5 * x + rchisq(100, df = 2)\ndf <- data.frame(x, y)\nfit_eg <- lm(y ~ x, data = df)\npar(mfrow = c(1, 2))\nplot(fit_eg, which = 2)\nhist(rstandard(fit_eg))\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-10-1.png)\n:::\n:::\n\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(100)\ny <- 2 + 5 * x + rchisq(100, df = 3) * -1\ndf <- data.frame(x, y)\nfit_eg <- lm(y ~ x, data = df)\npar(mfrow = c(1, 2))\nplot(fit_eg, which = 2)\nhist(rstandard(fit_eg))\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-11-1.png)\n:::\n:::\n\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(100)\ny <- 2 + 5 * x + rnbinom(100, 10, .5)\ndf <- data.frame(x, y)\nfit_eg <- lm(y ~ x, data = df)\npar(mfrow = c(1, 2))\nplot(fit_eg, which = 2)\nhist(rstandard(fit_eg))\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-12-1.png)\n:::\n:::\n\n\n# Assumption: Equal Variances\n\n## Equal variances\n\n- Look at the **scale-location plot**.\n- If variances are equal, the points should be randomly scattered around the horizontal axis.\n- The red line should be more or less horizontal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit, which = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-13-1.png)\n:::\n:::\n\n\n## Equal variances\n\n- If variances are not equal we *may* see:\n  - A funnel shape, where the points are more spread out at the ends than in the middle. Sometimes also called \"fanning\".\n  - Patterns in the scale-location plot, such as a curve or a wave, indicating that the variance is changing.\n- Look at the red line for a general trend, **but don't depend on it too much**.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(915)\nx <- rnorm(100)\ny <- 2 + 5 * x^2 + rchisq(100, df = 2)\ndf <- data.frame(x, y)\nfit_eg <- lm(y ~ x, data = df)\nplot(fit_eg, which = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-14-1.png)\n:::\n:::\n\n\n## Outliers\n\n- **Leverage** is a measure of how far away the predictor variable is from the mean of the predictor variable.\n- The Residuals vs Leverage plot shows the relationship between the residuals and the leverage of each point.\n- **Cook's distance** is a measure of how much the model would change if a point was removed.\n\n---\n\nIn general, points with **high leverage** and **high Cook's distance** are considered outliers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit, which = 5)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n--- \n\n### Example of an influential outlier\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(30)\ny <- 2 + 2 * x\n\ny[25] <- y[25] + 40 # Add an outlier\n\nfit_eg <- lm(y ~ x)\n\npar(mfrow=c(1,2))\nplot(fit_eg, which = 5)\n\nplot(x, y)\nabline(fit_eg, col = \"red\")\n\nfit_eg2 <- lm(y[-25] ~ x[-25])\nabline(fit_eg2, col = \"blue\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-16-1.png)\n:::\n:::\n\n\nPoints that exceed the dashed line (which appears once they approach the Cook's distance), are likely to influence the model. These outliers should be removed. \n\ne.g. this is of a perfect line with one extreme outlier -- the line of best fit deviates because of a single point!\n\n## What can we do if the assumptions aren't met? {auto-animate=\"true\"} \n\n### It depends...\n\n...which assumption is not met and the type of data i.e. circumstances.\n\n- If data is **non-linear**, try a transformation of the response variable $y$, from light to extreme:\n  - root: $\\sqrt{y}$ or $\\sqrt{y+1}$ if $y$ contains zeros\n  - log: $\\log(y)$ or $\\log(y+1)$ if $y$ contains zeros\n  - inverse: $\\frac{1}{y}$ or $\\frac{1}{y+1}$ if $y$ contains zeros\n\n- If residuals are **not normally distributed**, try a transformation of the response variable $y$ first, otherwise transform the predictor variable $x$. Both can be done at the same time.\n\n- If **equal variances** assumption is not met, same as above.\n- If **outliers** are present, try removing them, or transforming the response variable $y$.\n\n## What if transformation doesn't work?\n\nIf the assumptions are still not met after trying the above, you can try:\n\n- Using a different type of regression e.g. logistic regression, non-linear regression\n- Using a different model e.g. machine learning.\n- Using a non-parametric test.\n\n## Back to Galton - model assumptions are met\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-17-1.png)\n:::\n:::\n\n\nNow what?\n\n# Model Assessment and Interpretation\n\n> Hypothesis? How good is the model? What can we understand about the relationship between `child` and `parent`?\n\n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"1-6\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- `Call`: the model formula\n- `Residuals`: distribution of the residuals\n\n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"8-11\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n::: {.fragment .fade-out}\n- `Coefficients`: a summary table of the coefficients, their standard errors, t-values, and p-values.\n- `(Intercept)/Estimate`: the **y-intercept**, or the mean response when all predictors are 0.\n- `parent/Estimate`: the **slope** coefficient - i.e. the change in the **mean** of the response for a **one-unit increase in the predictor**.\n:::\n\n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"8-11\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- `Pr`: the p-value\n  - `(Intercept)/Pr`: the p-value of the y-intercept is not meaningful.\n  - `parent/Pr`: is `parent` a significant predictor to the model?\n  \n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"10-11\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- We can also use the `Estimate` values to write the equation of the regression line:\n$$ \\widehat{child} = 23.94153 + 0.64629 \\cdot parent$$\n\n- For every one-inch increase in the parent height, the child height is predicted to increase by 0.64629 inches.\n- e.g. if a parent is 70 inches, how tall will the child be? $23.94153 + 0.64629 \\cdot 70 = 68.5$ inches (174 cm).\n\n\n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"15-15\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- `Residual standard error`: the standard deviation of the residuals. \n  -  Interpretation: the average amount that the response will *deviate* from the true regression line.\n-  `degrees of freedom`: the number of observations minus the number of parameters being estimated. Used in hypothesis testing and calculating the standard error of the regression coefficients.\n\n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"16-16\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- `Multiple R-squared`: the proportion of variance (0-1) explained by the model (for simple linear regression).\n- `Adjusted R-squared`: the proportion of variance (0-1) explained by the model, adjusted for the number of predictors (for multiple linear regression).\n- Ranges from 0 to 1; R^2^ = 1 is a perfect fit.\n- \"The proportion of variance in the response that is explained by `parent`: 21.05%.\"\n\n## Interpreting the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-line-numbers=\"17-17\"}\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- `F-statistic`: the ratio of the variance explained by predictors, and the residual variance (variance not explained by predictors).\n  - Also known as the partial F-test between the full model and the intercept-only (null) model.\n- `p-value`: the probability that the F-statistic is greater than the observed value under the null hypothesis.\n  - In a simple linear regression, the p-value for the slope coefficient is the same as the p-value for the F-statistic.\n  \n## Hypothesis testing\n\nHow does our null ($H_0: \\beta_1=0$) model compare to the linear ($H_0: \\beta_1 \\neq 0$) model?\n\n> In simple linear regression, the p-value for the slope coefficient is the same as the p-value for the F-statistic.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nnull_model <- Galton %>%\n  lm(child ~ 1, data = .) %>%\n  augment(Galton)\nlin_model <- Galton %>%\n  lm(child ~ parent, data = .) %>%\n  augment(Galton)\nmodels <- bind_rows(null_model, lin_model) %>%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-25-1.png)\n:::\n:::\n\n\n## What are we testing?\n\n- The **null model** is a model with no predictors, i.e. $y = \\beta_0 + \\epsilon$\n- The **alternative model** is a linear model with one predictor, i.e. $y = \\beta_0 + \\beta_1 x + \\epsilon$\n- We use the t-test to compare the two models:\n\n$$ t = \\frac{estimate - 0}{Standard\\ error} = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} $$ where $SE(\\hat{\\beta}_1)$ is the standard error of the slope estimate:\n\n$$ SE(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} $$\n\n:::{.callout-note}\nIf the model is **not significant**, then the null model (i.e. mean) is better. If the model **is significant**, then the linear model is better.\n:::\n\n## Reporting {auto-animate=\"true\"}\n\n> Refresher on earlier steps\n\nGalton collected data on the average height of both parents and their children. There wasa moderate, positive, linear relationship ($r$ = 0.46) between parent and child height.\n\n> Can we rely on our model results? Assumptions and hypothesis?\n\nWe fitted a linear model to predict child height from parent height, and model assumptions were met. The model was statistically significant (R^2^ = 0.21, F(1, 926) = 246.84, p < .001), hence the null hypothesis was rejected in favour of the linear model. The effect of parent height is statistically significant and positive ($\\beta$ = 0.65, t(926) = 15.71, p < .001).\n\n> Equation? Inference?\n\nFor every one-inch increase in parent height, child height is predicted to increase by 0.65 inches ($ \\widehat{child} = 23.94 + 0.65 \\cdot parent$). The average height of both parents explains 21.05% of the variance in child height -- there is an effect but there are clearly more factors at play.\n\n## Fun fact\n\n- Galton's key finding was that children of tall parents are not as tall, children of short parents are not as short.\n\n$$ \\widehat{child} = 23.94153 + 0.64629 \\cdot parent$$\n\n  - e.g. if average parent height is 60 inches (152 cm), how tall will the child be? $23.94153 + 0.64629 \\cdot 60 = 62.7$ inches (159 cm).\n  - e.g. if average parent  height is 75 inches (191 cm), how tall will the child be? $23.94153 + 0.64629 \\cdot 75 = 72.4$ inches (184 cm).\n\n- The height of children appeared to **regress** towards the population mean, i.e. the concept of **regression to the mean**\n- Hence the [Galton](https://sites.ualberta.ca/~dwiens/stat575/misc%20resources/regression%20to%20the%20mean.pdf) is credited with coining the term **regression** (and also correlation, percentile, median, etc.)\n\n# Let's practice\nCan we predict the weight of an alligator from its length?\n[Download data ⬇](https://canvas.sydney.edu.au/courses/46921/pages/week-10-lectures-linear-functions?module_item_id=1752336)\n\n![](images/alligator.jpg){width=30%}\n\nPhoto by <a href=\"https://unsplash.com/@eyedealstuff?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Shelly Collins</a>\n\n## Explore\n\nRead the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl) # load the readxl package\n\nalligator <- read_excel(path = \"data/ENVX1002_Lecture_wk10_data.xlsx\", \n  sheet = \"Alligator\") # read in the data\n```\n:::\n\n\nWhat does the data look like?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(alligator)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [25 × 2] (S3: tbl_df/tbl/data.frame)\n $ Length: num [1:25] 58 61 63 68 69 72 72 74 74 76 ...\n $ Weight: num [1:25] 28 44 33 39 36 38 61 54 51 42 ...\n```\n\n\n:::\n:::\n\n\n## Plot\n\n::: {.panel-tabset}\n## Using base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x = alligator$Length, y = alligator$Weight, \n  xlab = \"Length (cm)\", ylab = \"Weight (kg)\")\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-28-1.png)\n:::\n:::\n\n\n## Using `ggplot2`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2) # load the ggplot2 package\nggplot(data = alligator, aes(x = Length, y = Weight)) +\n  geom_point() +\n  labs(x = \"Length (cm)\", y = \"Weight (kg)\")\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-29-1.png)\n:::\n:::\n\n\n:::\n\n## Plot residual diagnostics\n\nTo check assumptions, we need to fit the model first, then plot the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(formula = Weight ~ Length, data = alligator)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-30-1.png)\n:::\n:::\n\n\n## Check assumptions\n\n### Is the relationship linear?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit, which = 1)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-31-1.png)\n:::\n:::\n\n\nIf the linearity assumption is not met, there is no reason to validate the model since it is no longer suitable for the data.\n\n## Dealing with non-linearity: transform the data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(patchwork)\n\np1 <- ggplot(data = alligator, aes(x = Length, y = Weight)) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"Weight (kg)\", title = \"Original\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np2 <- ggplot(data = alligator, aes(x = Length, y = sqrt(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"sqrt[Weight (kg)]\", title = \"Square root\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np3 <- ggplot(data = alligator, aes(x = Length, y = log(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"log[Weight (kg)]\", title = \"Natural log\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np4 <- ggplot(data = alligator, aes(x = Length, y = log10(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"log10[Weight (kg)]\", title = \"Log base 10\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np1 + p2 + p3 + p4\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-32-1.png)\n:::\n:::\n\n\n## Natural log transformation -- Check assumptions again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(formula = log(Weight) ~ Length, data = alligator)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](lecture-10_files/figure-typst/unnamed-chunk-33-1.png)\n:::\n:::\n\n\n## Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Weight) ~ Length, data = alligator)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.289266 -0.079989  0.000933  0.102216  0.288491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.335335   0.131394   10.16 5.63e-10 ***\nLength      0.035416   0.001506   23.52  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1493 on 23 degrees of freedom\nMultiple R-squared:  0.9601,\tAdjusted R-squared:  0.9583 \nF-statistic:   553 on 1 and 23 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n- `Length` is a statistically significant predictor of `log(Weight)` (p < .001)\n- The model explains a statistically significant and large proportion (96%) of variance (R^2^ = 0.96, F(1, 23) = 553, p < .001)\n- For every 1 cm increase in `Length`, `log(Weight)` increases by a value of 0.0354, `Weight` increases by $e^{0.0354}$ **times** and `Weight` increases by *approximately* 3.54%\n\n## Percent change with $ln$ transformation\n\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute $e$ below for 10 or any other base), but the **quick approximation only works with natural log transformations**. \n\nIf $y$ has been transformed with a natural log (`log(y)`), for a one-unit increase in $x$ the **percent change in $y$** (not `log(y)`) is calculated with:\n\n$$\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)$$\n\nIf $\\beta_1$ is small (i.e. $-0.25 < \\beta_1 < 0.25$), then: $e^{\\beta_1} \\approx 1 + \\beta_1$. So $\\Delta y \\% \\approx 100 \\cdot \\beta_1$.\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\n|   β   |   Exact $(e^{\\beta} - 1)$%   |   Approximate $100 \\cdot \\beta$   |\n|-----:|-----------------:|------------------:|\n| -0.25 |                      -22.13  |                               -25 |\n| -0.1  |                       -9.52  |                               -10 |\n|  0.01 |                        1.01  |                                 1 |\n|  0.1  |                       10.52  |                                10 |\n|  0.25 |                       28.41  |                                25 |\n|  0.5  |                       64.87  |                                50 |\n|   2   |                      638.91  |                               200 |\n\n:::\n:::{.column width=\"50%\"}\n\n- **$y$ transformed**: a one-unit increase in $x$ is *approximately* a $\\beta_1$% change in $y$.\n- **$x$ transformed**: a 1% increase in $x$ is *approximately* a $0.01 \\cdot \\beta_1$ change in $y$.\n- **Both $x$ and $y$ transformed**: a 1% increase in x is *approximately* a $\\beta_1$% change in y.\n\n:::\n:::\n\n## Summary\n### Workflow\n\n1.  **Understand the variables**: Which is the response variable? Is there a reason to believe a causal relationship?\n\n2.  **Explore data**: How many observations? Summary statistics? Scatterplot and correlation?\n\n3.  **Fit model**\n\n4.  **Check assumptions**: Remember - it's about the residuals! If assumptions fail, try transforming and return to Step 3. If assumptions *still* fail, consider another model and return to Step 3.\n  \n5.  **Assess model fit**: Hypothesis test, significance, F-statistic, p-value. R^2^, how much model variation was explained by the model.\n  \n6.  **Interpret output**: 'For every one-unit increase in x, y increases by $\\beta_1$ units...' and any additional research/insight.\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "supporting": [
      "lecture-10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}