{
  "hash": "7c2c27eebb2c2dfdf044df80e422df43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 9 -- Describing relationships\nauthor: Si Yang Han\nformat: soles-revealjs\nembed-resources: true\n---\n\n\n\n\n\n\n## About me\n\n::: {layout-ncol=2}\n- Research topics: spatial modelling and mapping, precision agriculture, winter grains\n- Timeline at USYD\n  - BSc (Hons) in Agricultural Science\n  - PhD in Digital Agriculture\n  - Postdoc in Spatial Modelling\n  - Associate Lecturer in Agricultural Data Science\n\n![Faba beans at Trangie](images/hello_world.jpeg)\n:::\n\n## Learning Outcomes\n\n- LO1. Demonstrate proficiency in utilizing R and Excel to effectively explore and describe data sets in the life sciences.\n\n- [LO2. Evaluate and interpret different types of data in the natural sciences by visualising probability distributions and calculating probabilities using RStudio and Excel.]{style=\"color: #D0D3D4;\"}\n\n- [LO3. Apply parametric and non-parametric statistical inference methods to experimental data using RStudio and effectively interpret and communicate the results in the context of the data.]{style=\"color:#D0D3D4;\"}\n\n- LO4. Apply both linear and non-linear models to describe relationships between variables using RStudio and Excel, demonstrating creativity in developing models that effectively represent complex data patterns.\n\n- [LO5. Articulate statistical and modelling results clearly and convincingly]{style=\"color:black;\"} [in both written reports and oral presentations, working effectively as an individual and collaboratively in a team, showcasing the ability to convey complex information to varied audiences.]{style=\"color:#D0D3D4;\"}\n\n<!---\ntherefore, want students how to fit a regression model and be able to interpret \n- Want to make sure we are using it for the right thing (linear relationship)\n- want to make sure we are fitting it properly\n- Want to then be able to interpret the output\n--->\n\n## Module overview\n\n- **Week 9. Describing Relationships**\n    + Correlation (calculation, interpretation)\n    + Regression (model structure, model fitting\n    + What/when/why/how\n\n- [Week 10. Simple Linear Regression]{style=\"color: #D0D3D4;\"}\n    + [Can we use the model?(assumptions, hypothesis testing)]{style=\"color: #D0D3D4;\"}\n    + [How good is the model?(interpretation, model fit)]{style=\"color: #D0D3D4;\"}\n    \n- [Week 11. Multiple Linear Regression]{style=\"color: #D0D3D4;\"}\n    + [Multiple Linear Regression (MLR) modelling]{style=\"color: #D0D3D4;\"}\n    + [Assumptions, interpretation and the principle of parsimony]{style=\"color: #D0D3D4;\"}\n\n- [Week 12. Nonlinear Regression]{style=\"color: #D0D3D4;\"}\n    + [Common nonlinear functions]{style=\"color: #D0D3D4;\"}\n    + [Transformations]{style=\"color: #D0D3D4;\"}\n\n## Module overview\n\n- **Week 9. Describing Relationships**\n    + Correlation (calculation, interpretation)\n    + Regression (model structure, model fitting\n    + What/when/why/how\n\n## Example - Galton's Data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(HistData)\nhead(Galton)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  parent child\n1   70.5  61.7\n2   68.5  61.7\n3   65.5  61.7\n4   64.5  61.7\n5   64.0  61.7\n6   67.5  62.2\n```\n\n\n:::\n:::\n\n\n\n\n\n\n<br>\n\n- 928 children of 205 pairs of parents\n- Average height of both parents and child measured in inches\n- Size classes were binned (hence data looks discrete)\n\n## Example - Galton's Data\n\nWe can visually inspect the relationship between the two variables using a scatterplot:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n    geom_point(alpha = .2, size = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n... but this is not a very good way to assess the strength of the relationship between the two variables.\n\nIs the relationship:\n\n  - Linear?\n  - Positive or negative?\n  - Weak, moderate or strong?\n\n# Correlation\n\n## Correlation {auto-animate=\"true\"}\n\nThe **correlation coefficient** is a number between -1 and 1 that describes the relationship between two **continuous** variables.\n\n- **Direction**: \n  - Positive -- both variables increase together\n  - Negative -- one variable increases as the other decreases\n- **Strength**: \n  - -1 &rarr; perfect negative relationship\n  - 0 &rarr; no relationship\n  - 1 &rarr; perfect positive relationship\n  - Common terms: *weak* (~0.1--0.3), *moderate* (~0.4--0.6) or *strong* (~0.7--1.0) -- useful but subjective!\n\n## Correlation\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\nPearson's Correlation (*r*) Formula:\n\n$$ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n\nCovariance divided by the product of the standard deviations.\n\n> Karl Pearson developed the **correlation coefficient** in 1800s (based on the work by Francis Galton)\n\n:::\n:::{.column width=\"50%\"}\n![](images/pearson.jpg){out.width=\"50%\"}\n:::\n:::\n\nPearson's *r* is:\n\n- The most commonly used correlation coefficient\n- For normally distributed data only (parametric)\n- For linear relationships only (i.e. a straight line)\n\n## Luckily we have Excel and R\n\n- Excel: `=CORREL()` formula, or use the Analysis Toolpak\n- R: `cor()` function\n  - By default this calculates Pearson's correlation coefficient\n  - Can also calculate other types of correlation (`cor(x, y, method = \"spearman\")`)\n\n---\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate synthetic data\nset.seed(123)\nrainfall <- rnorm(100, mean = 50, sd = 10)\nplant_growth <- rainfall + rnorm(100, mean = 0, sd = 5)\n\n# Calculate correlation coefficient\ncorrelation_coef <- cor(rainfall, plant_growth)\n\n# Plot data using ggplot2\nlibrary(ggplot2)\np1 <- ggplot(data = data.frame(rainfall, plant_growth), aes(x = rainfall, y = plant_growth)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(\n        title = \"Positive correlation\",\n        x = \"Rainfall\", y = \"Crop Yield\",\n        subtitle = paste(\"Pearson's r: \", round(correlation_coef, 2))\n    )\n\n# Generate synthetic data\nset.seed(123)\ninsulin <- rnorm(100, mean = 50, sd = 10)\nblood_glucose <- 100 - insulin + rnorm(100, mean = 0, sd = 5)\n\n# Calculate correlation coefficient\ncorrelation_coef <- cor(insulin, blood_glucose)\n\n# Plot data using ggplot2\np2 <- ggplot(data = data.frame(insulin, blood_glucose), aes(x = insulin, y = blood_glucose)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(\n        title = \"Negative correlation\",\n        x = \"Insulin\", y = \"Blood Glucose\",\n        subtitle = paste(\"Pearson's r: \", round(correlation_coef, 2))\n    )\nlibrary(patchwork)\n\n# Set the seed for reproducibility\nset.seed(1249)\n\n# Generate the data\nx <- rnorm(50, mean = 50, sd = 10)\ny <- rnorm(50, mean = 100, sd = 20)\ndf1 <- data.frame(x, y) # combine the two variables into a dataframe\ncorrelation_coef <- cor(x, y) # calculate the correlation coefficient\n\n# Scatter plot\np3 <- ggplot(df1, aes(x, y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(\n        title = \"Weak/no correlation\",\n        x = \"Shoe Size\", y = \"IQ\",\n        subtitle = paste(\"Pearson's r: \", round(correlation_coef, 2))\n    )\n\np1 + p3 + p2\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n(Note: not real data)\n\nA **weak** or **nonexistent** relationship is one where the correlation coefficient is close to 0.\n\nA **moderate** relationship falls closer to 0.5.\n\nA **strong** relationship is one where the correlation coefficient is close to -1 or 1.\n\nThe exact values are subjective and vary between fields of study.\n\n## Example - Galton's Data\n\n::: {.columns}\n::: {.column width=\"30%\"}\n![Galton's Results](images/galton_corr.jpg)\n:::\n::: {.column width=\"60%\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n    geom_point(alpha = .2, size = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: \n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(Galton$parent, Galton$child)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4587624\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIs the relationship:\n\n  - Linear? [**Seems to be**]{style=\"color: firebrick\"}\n  - Positive or negative? [**Positive**]{style=\"color: firebrick\"}\n  - Weak, moderate or strong? [**Moderate**]{style=\"color: firebrick\"}\n\nIn words: \"There is a moderate positive linear relationship between the height of parents and the height of their children.\"\n\n## Anscombe's Quartet\n\nA set of four datasets that are nearly identical in simple descriptive statistics but have very different distributions.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n\n# Compute Pearson, Spearman, and Kendall correlations for each set\ncor_values <- anscombe %>%\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\") %>%\n  group_by(set) %>%\n  summarise(\n    pearson = cor(x, y, method = \"pearson\"))\n\n# Create the plot with the correlation values added as text annotations\nanscombe %>%\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\") %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  facet_wrap(~set, ncol = 4) +\n  # Add text annotations for Pearson, Spearman, and Kendall correlations\n  geom_text(data = cor_values, aes(x = 8, y = 4, label = \n    paste(\"Pearson: \", round(pearson, 2))),\n    inherit.aes = FALSE, size = 3.5, color = \"black\", hjust = 0)\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nCorrelation coefficients are **not reliable** in inferring the 'type' of relationship between variables -- we must visualise.\n\n## Datasaurus Dozen\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(datasauRus)\nggplot(datasaurus_dozen, aes(x = x, y = y)) +\n    geom_point(size = .5, alpha = .3) +\n    geom_smooth(method = \"lm\", se = TRUE) +\n    facet_wrap(~dataset, ncol = 6)\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n*All of these data have a correlation coefficient close to zero!*\n\n## Monotonic vs linear relationships\n\n- **Monotonic**: a relationship that is consistently increasing or decreasing\n- **Linear**: a relationship that is increasing or decreasing at a *constant rate* i.e. a straight line\n\nPearson's correlation is only for linear relationships.\n\n**Spearman's rank** and **Kendall's tau** are correlation coefficients for all monotonic relationships.\n\n- Works with non-parametric data\n- More 'conservative' i.e. values can be smaller in magnitude, but more robust against outliers\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\nx_length <- length(x)\n\n# Create the data for each relationship\ny_straight <- 2 * x + 5\ny_exponential <- exp(x)\ny_log <- logb(x+1, base = 2)\ny_sigmoid <- 1 / (1 + exp(-x))\ny_quadratic <- -1 * (x - 5)^2 + 100\ny_polynomial <- x^3 - 4 * x^2 + 3 * x + 2\n\n# Function to scale y-values to a specific range [0, 100]\nscale_to_range <- function(y_values, new_min = 0, new_max = 100) {\n  old_min <- min(y_values)\n  old_max <- max(y_values)\n  scaled_values <- (y_values - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n  return(scaled_values)\n}\n\n# Apply scaling to each relationship\ny_straight_scaled <- scale_to_range(y_straight)\ny_exponential_scaled <- scale_to_range(y_exponential)\ny_log_scaled <- scale_to_range(y_log)\ny_sigmoid_scaled <- scale_to_range(y_sigmoid)\ny_quadratic_scaled <- scale_to_range(y_quadratic)\ny_polynomial_scaled <- scale_to_range(y_polynomial)\n\n# Combine the data into a tibble\ndata <- tibble(\n  x = rep(x, 6),\n  y = c(y_straight_scaled, y_exponential_scaled, y_log_scaled, y_sigmoid_scaled, y_quadratic_scaled, y_polynomial_scaled),\n  type = rep(c(\"Straight Line\", \"Exponential\", \"Log Curve\", \"Sigmoid Curve\", \"Quadratic Line\", \"Polynomial\"), each = x_length)\n)\n\n# Calculate the correlation coefficients for each relationship type\ncor_values <- data %>%\n  group_by(type) %>%\n  summarise(\n    pearson = cor(x, y, method = \"pearson\"),\n    spearman = cor(x, y, method = \"spearman\"),\n    kendall = cor(x, y, method = \"kendall\")\n  )\n\n# Plot the data and add correlation coefficients\nggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  facet_wrap(~type, scales = 'free') +\n  geom_text(data = cor_values, aes(x = 3, y =25, label = \n                                    paste(\"Pearson: \", round(pearson, 2), \n                                          \"\\nSpearman: \", round(spearman, 2),\n                                          \"\\nKendall: \", round(kendall, 2))),\n            inherit.aes = FALSE, size = 4, color = \"black\", hjust = 0) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Example - `iris` Dataset\n\n![](images/iris_example.png)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Example - `iris` Dataset\n\nWhich correlation coefficient do we use?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris[,-5])\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Example - `iris` Dataset\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(iris[,-5]) |> round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n```\n\n\n:::\n:::\n\n\n\n\n\n\nCorrelation analysis **identifies** variables that have a strong linear relationship **quickly and easily**.\n\n- We want to *predict* the `Petal.Width` of an iris flower\n- Which variables would be *good predictors*? \n  - Either `Petal.Length` and `Sepal Length` would be good predictors\n- Which variables are likely to present issues for *model fitting*?\n  - e.g. `Petal.Length` and `Sepal Length` are strongly correlated - including both would cause multicollinearity -- (L11 MLR)\n\n## Correlation $\\neq$ causation\n\n[Spurious correlations](https://www.tylervigen.com/spurious-correlations): a relationship between two variables does not imply that one causes the other.\n\n![](images/correlation_causation.jpg){fig-align=\"center\"}\n\n## What comes after correlation?\n\n- We have data with two or more numerical variables\n- We conduct correlation analysis to describe *possible* linear relationships\n  - Fast, easy, interpretable, and widely used\n\nBut, we can't infer causation:\n- Is there *reason() to expect a relationship between the two variables?\n- Do you have a *hypothesis* about the relationship between the two variables?\n\nIf we have a hypothesis about the relationship between two variables, we can use **regression analysis** to test it.\n\n\n# Regression modelling\n\n- Regression is a *statistical* method to fit a model to data\n- In linear regression this is a straight line that best fits the data -- i.e. line of best fit\n\n## Why regression?\n\n### Describe the relationship between two variables\nWhat is the relationship between a response variable $Y$ and a predictor variable $x$?\n\nCommon Terms:\n$Y$ = response, independent variable, target, outcome etc.\n$x$ = predictor, dependent variable, feature, input etc.\n\n### Explain the relationship between two variables\nHow much variation in $Y$ can be explained by a relationship with $x$?\n\n### Predict the value of a response variable\nWhat is the value of $Y$ for a given value of $x$?\n\nOften we can easily measure/obtain $x$ but not $Y$, so we need to *predict* $Y$ from $x$.\n\n## A gateway to the world of modelling\n\nMany types of regression models exist:\n\n- **Simple linear regression (one predictor i.e. $x$)**\n- Multiple linear regression (more than one predictor)\n- Non-linear regression, using functions such as polynomials, exponentials, logarithms, etc.\n\n. . .\n\nAsking ChatGPT for help with the next slide:\n\n> Using R code, can you generate some data that is useful to demonstrate simple linear regression, multiple linear regression, polynomial, exponential and logarithmic regressions in ggplot2?\n\n> Sure! Here's an example code that generates a sample dataset and visualizes it using ggplot2 library in R.\n\n## Visualising regression models\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(136)\nx <- 1:100\ny <- x^2 + rnorm(100, sd = 100)\n\n# Plot the data and regression lines\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n    ggtitle(\"Regression Models\") +\n    geom_point(alpha = .2) +\n    xlab(\"X\") +\n    ylab(\"Y\") +\n    ylim(-6000, 15000)\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Visualising regression models\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(136)\nx <- 1:100\ny <- x^2 + rnorm(100, sd = 100)\n\n# Define regression functions\nslr <- function(x, y) {\n    mod <- lm(y ~ x)\n    return(list(\n        data.frame(x = x, y = predict(mod), model_type = \"Simple Linear Regression\"),\n        paste(\"y =\", round(coefficients(mod)[[2]], 2), \"x +\", round(coefficients(mod)[[1]], 2))\n    ))\n}\n\nmlr <- function(x, y, z) {\n    mod <- lm(y ~ x + z)\n    return(list(\n        data.frame(x = x, z = z, y = predict(mod), model_type = \"Multiple Linear Regression\"),\n        paste(\"y =\", round(coefficients(mod)[[3]], 2), \"x +\", round(coefficients(mod)[[2]], 2), \"z +\", round(coefficients(mod)[[1]], 2))\n    ))\n}\n\npoly_reg <- function(x, y, degree) {\n    mod <- lm(y ~ poly(x, degree, raw = TRUE))\n    x_new <- seq(min(x), max(x), length.out = 100)\n    y_new <- predict(mod, newdata = data.frame(x = x_new))\n    return(list(\n        data.frame(x = x_new, y = y_new, model_type = paste(\"Polynomial Regression (\", degree, \")\", sep = \"\")),\n        paste(paste(\"x^\", degree, sep = \"\"), \":\", paste(round(coefficients(mod), 2), collapse = \" + \"))\n    ))\n}\n\nexp_reg <- function(x, y) {\n    mod <- lm(log(y) ~ x)\n    x_new <- seq(min(x), max(x), length.out = 100)\n    y_new <- exp(predict(mod, newdata = data.frame(x = x_new)))\n    return(list(\n        data.frame(x = x_new, y = y_new, model_type = \"Exponential Regression\"),\n        paste(\"y =\", round(exp(coefficients(mod)[[2]]), 2), \"* e^(\", round(coefficients(mod)[[1]], 2), \"x\", \")\")\n    ))\n}\n\nlog_reg <- function(x, y) {\n    mod <- lm(y ~ log(x))\n    x_new <- seq(min(x), max(x), length.out = 100)\n    y_new <- predict(mod, newdata = data.frame(x = x_new))\n    return(list(\n        data.frame(x = x_new, y = y_new, model_type = \"Logarithmic Regression\"),\n        paste(\"y =\", round(coefficients(mod)[[2]], 2), \"* log(x) +\", round(coefficients(mod)[[1]], 2))\n    ))\n}\n\n# Create regression line dataframes and equations\nreg_data <- list(slr(x, y), mlr(x, y, rnorm(100, sd = 10)), poly_reg(x, y, 3), exp_reg(x, y), log_reg(x, y))\nreg_eqs <- sapply(reg_data, function(x) x[[2]])\n\n# Plot the data and regression lines\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n    lapply(seq_along(reg_data), function(i) geom_line(data = reg_data[[i]][[1]], aes(x, y, color = reg_data[[i]][[1]]$model_type), linewidth = 1.4)) +\n    ggtitle(\"Regression Models\") +\n    geom_point(alpha = .2) +\n    xlab(\"X\") +\n    ylab(\"Y\") +\n    scale_color_discrete(name = \"Model Type\") +   # This line adds the title to the legend\n    ylim(-6000, 15000) +\n    theme(legend.position = \"right\")  # Modify this to display the legend (default position is 'right')\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Example: climate change modelling\n\n![](images/anomalies.jpg){fig-align=\"center\"}\n\n[Source: https://science2017.globalchange.gov/chapter/4/](https://science2017.globalchange.gov/chapter/4/)\n\n## Example: COVID-19 transmission modelling\n\n![](images/covid.jpg){fig-align=\"center\"}\n\n[Source: https://www.nature.com/articles/s41598-021-84893-4/figures/1](https://www.nature.com/articles/s41598-021-84893-4/figures/1)\n\n## How does regression work?\n\n![Adrien-Marie Legendre](images/legendre.jpg)\n![Carl Friedrich Gauss](images/gauss.jpg)\n![Francis Galton](images/galton.jpg)\n\nAdrien-Marie Legendre, Carl Friedrich Gauss, Francis Galton\n\n:::{.callout-note}\nMany other people contributed to the development of regression analysis, but these three are the most well-known.\n:::\n\n## How does regression work? {auto-animate=\"true\"}\n\n- **Method of least squares** first theorised by Adrien-Marie Legendre in 1805\n- **Technique of least squares** first used by Carl Friedrich Gauss in 1809 (to fit a parabola to the orbit of the asteroid Ceres)\n- **Model fitting** first published by Francis Galton in 1886 (predicting the height of a child from the height of the parents)\n\n## Least squares\n\n:::{.columns}\n:::{.column width=\"40%\"}\nWhen we fit a line, there is error between the observed and predicted values.\n\n$$ \\color{firebrick}{\\hat{\\epsilon_i}} = \\color{royalblue}{y_i} - \\color{forestgreen}{\\hat{y_i}} $$\nThe **method of least squares** fits a line of best fit by minimising the sum of the squared errors. \n\n$$\\color{firebrick} \\sum_{i=1}^n ({\\hat{\\epsilon_i}})^2$$\n:::\n:::{.column width=\"60%\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# simulate example data\n\nset.seed(340)\nx <- runif(8, 0, 30)\ny <- 5 * x + rnorm(8, 0, 40)\ndf <- data.frame(x, y)\n\n# fit linear model, add residual vertical lines as arrows\nmod <- lm(y ~ x, data = df)\np1 <- ggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_segment(aes(xend = x, yend = fitted(mod)),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    color = \"royalblue\"\n  ) +\n  labs(x = \"x\", y = \"y\")\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  annotate(\"text\",\n    x = 6.3, y = -6, size = 7,\n    label = expression(hat(epsilon[i])), colour = \"royalblue\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = 25, size = 7,\n    label = expression(hat(y[i])), colour = \"forestgreen\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = -36, size = 7,\n    label = expression(y[i]), colour = \"firebrick\"\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n:::\n:::\n\n:::{.callout-note}\n\nThe error is squared so positive and negative errors do not cancel each other out.\n\n:::\n\n## Least squares {auto-animate=\"true\"}\n\n![](images/ss_error_pair.jpg){out.width=\"80%\"}\nWhich line fits the data better?\n\n## How does a computer fit a line of best fit?\n\n![](images/leastsquares.gif){fig-align=\"center\"}\n\nThe line is fitted again and again until the squared error stabilises. A computer can do this very quickly!\n\n# Simple linear regression\n\n> The goal is to fit the line of best fit between a numerical response and a numerical predictor.\n\n## Simple linear regression model {auto-animate=\"true\"}\n\nWe want to relate the response $Y$ to a predictor $x$ for $i$ number of observations: \n\n$$Y_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}$$\n\nwhere\n\n$$\\epsilon_i \\sim N(0, \\sigma^2)$$\n\n- $Y_i$, the *response*, is an observed value of the dependent variable.\n- $\\beta_0$, the *constant*, is the y-intercept and is **fixed**.\n- $\\beta_1$ is the population *slope* parameter, and like $\\beta_0$, is also **fixed**.\n- $\\epsilon_i$ is the error associated with predictions of $y_i$, and unlike $\\beta_0$ or $\\beta_1$, it is *not fixed*.\n\n:::{.callout-note}\n$\\epsilon_i$ is generally associated with the **residual** ($observed - predicted$). **True error** occurs during data collection (e.g. faulty instruments, selection bias, etc.) and often immeasurable.\n:::\n\n## Different wordings\n\nDifferent ways to think about the response:\n\n  - Response = [Prediction]{style=\"color: royalblue\"} + [Error]{style=\"color: red\"}\n  - Response = [Signal]{style=\"color: royalblue\"} + [Noise]{style=\"color: red\"}\n  - Response = [Model]{style=\"color: royalblue\"} + [Unexplained]{style=\"color: red\"}\n  - Response = [Deterministic]{style=\"color: royalblue\"} + [Random]{style=\"color: red\"}\n  - Response = [Explainable]{style=\"color: royalblue\"} + [Everything else]{style=\"color: red\"}\n  - Y = f(x)\n  - Dependent variable = f(Independent variable)\n\n## Model fitting\n\nTwo approaches; analytical and numerical: \n\n- **Analytical**:  equation(s) used directly to find solution\n- **Numerical**:  computer uses “random guesses” to find set of parameters to that minimises objective function, in this case residual sum of squares\n\n## Analytical: Slope, $\\beta_1$\n\n<!-- $$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$ -->\n\n$$ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} $$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Calculate slope from df\nbeta1 <- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) /\n  sum((df$x - mean(df$x))^2)\n# beta0 <- mean(df$y) - beta1 * mean(df$x)\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linetype = 2) +\n  # label the line\n  annotate(\"text\",\n    x = 15, y = 65, size = 7,\n    label = expression(beta[1]), colour = \"firebrick\"\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n<!-- $$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$ -->\n\n## Analytical: Intercept\n\n$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# calculate mean y from df\nybar <- mean(df$y)\nxbar <- mean(df$x)\nbeta0 <- ybar - beta1 * xbar\n\np1 + geom_vline(xintercept = xbar, linetype = \"dashed\", color = \"slateblue\") +\n  geom_hline(yintercept = ybar, linetype = \"dashed\", color = \"slateblue\") +\n  # label the lines\n  annotate(\"text\",\n    x = 25, y = ybar * 0.8, size = 7,\n    label = expression(bar(y)), colour = \"slateblue\"\n  ) +\n  annotate(\"text\",\n    x = xbar * 1.05, y = 150, size = 7,\n    label = expression(bar(x)), colour = \"slateblue\"\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linetype = 2) +\n  # extend the geom_smooth line to intercept x=0\n  geom_segment(aes(x = xbar, y = ybar, xend = 0, yend = beta0),\n    color = \"firebrick\", linetype = 2\n  ) +\n  # label the slope line\n  annotate(\"text\",\n    x = 15, y = 65, size = 7,\n    label = expression(beta[1]), colour = \"firebrick\"\n  ) +\n  # add a dot at the intercept\n  geom_point(aes(x = 0, y = beta0), color = \"seagreen\", size = 3) +\n  # label the intercept\n  annotate(\"text\",\n    x = 0, y = beta0 * 1.4, size = 7,\n    label = expression(beta[0]), colour = \"seagreen\"\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n# Example: back to Galton's data\n\nWhat can we understand about the relationship between `child height` and `parent height`?\n\n> Let's do this in R - much easier\n\n## Linearity check\n\nRecap - there appears to be a linear relationship between `child height` and `parent height` (plot). This is a moderately positive relationship (correlation).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n    geom_point(alpha = .2, size = 3)\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ncor(Galton$parent, Galton$child)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4587624\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Hypothesis testing\n\n- The **null hypothesis** for a linear model: $H_0: \\beta_1=0$\n  - If there is no slope ($\\beta_1=0$) then $y = \\beta_0$ (the mean)\n  - i.e. is the mean ($\\bar{y}$) a better fit than a linear model?\n- The **alternative hypothesis** for a linear model: $H_0: \\beta_1 \\neq 0$\n  - i.e. the estimate from the linear model ($\\hat{y}$) fits the data better than the mean ($\\bar{y}$)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nnull_model <- Galton %>%\n  lm(child ~ 1, data = .) %>%\n  broom::augment(Galton)\nlin_model <- Galton %>%\n  lm(child ~ parent, data = .) %>%\n  broom::augment(Galton)\nmodels <- bind_rows(null_model, lin_model) %>%\n  mutate(model = rep(c(\"Null Model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null Model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR Model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")\n```\n\n::: {.cell-output-display}\n![](lecture-09_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Simple linear regression\n\nWe fit the model in one simple line of code: `fit <- lm(child ~ parent, data = Galton)`\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(child ~ parent, data = Galton)\n```\n:::\n\n\n\n\n\n\n. . .\n\nAnd then we can use `summary()` to get a summary of the model:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Summary\n\nCorrelation is a measure of the relationship between two numerical variables between -1 and 1.\n\n  - A correlation coefficient measures the *strength* and *direction* of the relationship.\n  - Pearson's correlation is for linear relationships, Spearman's/Kendall's correlation is for monotonic relationships.\n  - Correlation $\\neq$ causation.\n  \n  \nRegression is an analysis that models the relationship between a dependent variable and independent variable(s).\n\n  - The most common method for regression is least squares, which minimises the sum of the squared residuals.\n  - Simple linear regression fits a straight line between two variables.\n  \n> Next week: interpreting results and assumptions for simple linear regression.\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/",
    "supporting": [
      "lecture-09_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}