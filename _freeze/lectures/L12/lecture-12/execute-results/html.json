{
  "hash": "4b9bf7b794e9d1dac45beea47eca053d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 12 -- Nonlinear regression\nauthor: Si Yang Han\nformat:\n  soles-revealjs:\n    logo: ../muddyqr.png\n    # footer: \"Scan the QR code to provide feedback on this lecture\"\nembed-resources: true\nscrollable: true\n---\n## Module overview\n\n\n::: {.cell}\n<style type=\"text/css\">\n.reveal .slide-logo {\n  height: 8rem !important;\n  max-height: 8rem !important;\n  bottom: -2px;\n  right: 10px;\n}\n</style>\n:::\n\n\n\n\n\n- [Week 9. Describing Relationships]{style=\"color: #D0D3D4;\"}\n    + [Correlation (calculation, interpretation)]{style=\"color: #D0D3D4;\"}\n    + [Regression (model structure, model fitting]{style=\"color: #D0D3D4;\"}\n    + [What/when/why/how]{style=\"color: #D0D3D4;\"}\n\n- [Week 10. Simple Linear Regression]{style=\"color: #D0D3D4;\"}\n    + [Can we use the model?(assumptions, hypothesis testing)]{style=\"color: #D0D3D4;\"}\n    + [How good is the model?(interpretation, model fit)]{style=\"color: #D0D3D4;\"}\n    \n- [Week 11. Multiple Linear Regression]{style=\"color: #D0D3D4;\"}\n    + [Multiple Linear Regression (MLR) modelling]{style=\"color: #D0D3D4;\"}\n    + [Assumptions, interpretation and the principle of parsimony]{style=\"color: #D0D3D4;\"}\n\n- **Week 12. Nonlinear Regression**\n    + Common nonlinear functions\n    + Transformations\n\n## Regressions \n\n### Simple linear regression\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$\n\nIdeal for predicting a continuous response variable from a single predictor variable: *\"How does $y$ change as $x$ changes, when the relationship is linear?\"*\n\n\n### Multiple linear regression \n\n$$ Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + \\epsilon_i $$\n\n*\"How does $y$ change as $x_1$, $x_2$, ..., $x_k$ change?\"*\n\n\n:::{.fragment}\n### Nonlinear regression\n$$ Y_i = f(x_i, \\beta) + \\epsilon_i $$\n:::\n\n:::{.fragment}\nwhere $f(x_i, \\beta)$ is a nonlinear function of the parameters $\\beta$: \"How do we model a change in $y$ with $x$ when the relationship is nonlinear?\"\n:::\n\n# Nonlinear regression\n\n![](images/gauss.jpg)\n![](images/netwon.jpg)\n\n> Carl Friedrich Gauss (1777-1855) and Isaac Newton (1642-1726)\n> Gauss-Newton approach to nonlinear regression is most commonly used\n\n\n## Nonlinear relationships\n\nLinear relationships are simple to interpret since the rate of change is constant.\n\n**\"As one changes, the other changes at a constant rate.\"**\n\nNonlinear relationships often involve exponential, logarithmic, or power functions.\n\n**\"As one changes, the other changes at a rate that is *not proportional* to the change in the other.**\n\n## Dealing with nonlinearity\n\n### Transformations\n\nOften, a nonlinear relationship may be transformed into a linear relationship by applying a transformation to the response variable or the predictor variable(s).\n\n- **Logarithmic**: $y = \\log(x)$ \n- **Exponential**: $y = e^x$\n- **Square-root**: $y = \\sqrt{x}$\n- **Inverse**: $y = \\frac{1}{x}$\n\n:::{.fragment}\n- Usually works when $y$ changes [monotically](https://en.wikipedia.org/wiki/Monotonic_function) with $x$.\n- More interpretable and easier to fit.\n:::\n\n## Nonlinear relationships: exponents\n\n- $x^2$ is the *square* of $x$.\n- $x^3$ is the *cube* of $x$.\n- $x^a$ is x raised to the *power* of $a$.\n\n> In a relationship where $y$ is a function of $x^a$, as $x$ increases, $y$ increases nonlinearly at a rate that depends on the value of $x$ and $a$ ($\\frac{dy}{dx} = ax^{a-1}$).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plot a simulation of above in ggplot2\nset.seed(123)\ntibble(x = seq(0, 10, by = 0.2), y = x^2) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(x = \"x\", y = \"y\") +\n  ggtitle(expression(y == x^2)) +\n  theme(plot.title = element_text(size = 40, face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Nonlinear relationships: logarithms\n\n- $log_e(x)$ is the *natural logarithm* of $x$.\n- $log_{10}(x)$ is the *common logarithm* of $x$.\n- $log_a(x)$ is the *logarithm* of $x$ to the base $a$.\n\n**Interpretation:**\n\n- If $\\log_a(y) = x$: as $x$ increases, the value of $y$ increases by $y = a^x$.\n- If $y = \\log_a(x)$: as $x$ increases, the value of $y$ increases by $y = \\log_a(x)$. As $y$ increases, the value of $x$ increases by $x = a^y$.\n\n## Exponents and logarithms\n\n|       | Exponents        | Logarithms       |\n|-------|:----------------:|:----------------:|\n| **Definition** | If $a^n = b$, $a$ is the base, $n$ is the exponent, and $b$ is the result. | If $\\log_a b = n$, $a$ is the base, $b$ is the result, and $n$ is the logarithm (or the exponent in the equivalent exponential form). |\n| **Example** | $2^3 = 8$ | $\\log_2 8 = 3$ |\n| **Interpretation** | $2$ raised to the power of $3$ equals $8$. | The power to which you must raise $2$ to get $8$ is $3$. |\n| **Inverse** | The logarithm is the inverse operation of exponentiation. | The exponentiation is the inverse operation of logarithm. |\n| **Properties** | $(a^n)^m = a^{n \\cdot m}$, $a^n \\cdot a^m = a^{n+m}$, $\\frac{a^n}{a^m} = a^{n-m}$ | $\\log_a(b \\cdot c) = \\log_a b + \\log_a c$, $\\log_a\\left(\\frac{b}{c}\\right) = \\log_a b - \\log_a c$, $\\log_a(b^n) = n \\cdot \\log_a b$ |\n\n\n:::{.callout-note}\nFor your understanding, not examinable.\n:::\n\n\n# Common nonlinear functions\n\n$f(x_i, \\beta)$\n\n## Exponential decay relationship\n\nResponse variable *decreases* and approaches limit as predictor variable increases.\n\n$$ y = a \\cdot e^{-bx} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(429) # set seed\n# Simulate data:\ndecay <- tibble(\n  predictor = seq(0,10, by = 0.2),\n  response = abs(exp(-0.5*predictor) + rnorm(length(predictor), mean = 1, sd = 0.1)))\n\nggplot(data = decay, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\nExamples: radioactive decay, population decline, chemical reactions.\n\n## Asymptotic relationship\n\nResponse variable *increases* and approaches a limit as the predictor variable increases.\n\n$$ y = a + b(1 - e^{-cx}) $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(442) # set seed\n# Simulate data:\nasymptotic = tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100*(1-exp(-0.5*predictor)) + rnorm(length(predictor), mean = 0, sd = 10))\n\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\nExamples: population growth, enzyme kinetics.\n\n## Logistic relationship\n\nAn S-shaped relationship, where the response variable is at first exponential, then asymptotic.\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(450)\n# Simulate data:\nlogistic <- tibble(predictor = seq(0, 10, by = 0.2), \n  response = 10 + abs(300 * (1 / (1 + exp(-0.8 * (predictor - 5)))) + rnorm(length(predictor), mean = 0, sd = 10)))\n\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\nExamples: growth of bacteria, disease spread, species growth.\n\n## Polynomial relationship\n\nResponse variable changes in a variety of ways as the predictor variable changes. Also known as 'curvilinear'.\n\n$$ y = a + bx + cx^2 + dx^3 + ... $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Set seed for reproducibility\nset.seed(529)\n# Simulate data:\ncurvilinear <- tibble(predictor = seq(0, 30, length.out = 50), \n  response = 50 * (1 - (predictor - 15)^2 / 225) + rnorm(length(predictor), mean = 0, sd = 5))\n\nggplot(data = curvilinear, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\nExamples: food intake, drug dosage, exercise.\n\n# Transformations\n> How far can we go?\n\n## Transformations: exponential decay\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = decay,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = decay, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: exponential decay\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = decay)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = decay)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Transformations: asymptotic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = asymptotic,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = asymptotic, \n       aes(x = log(predictor), y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: asymptotic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = asymptotic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = asymptotic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Transformations: logistic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = logistic,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = logistic, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: logistic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = logistic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = logistic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Transformations: polynomial relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = curvilinear,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = curvilinear, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: polynomial relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = curvilinear)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = curvilinear)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Did the transformations work?\n\n- To a *certain* extent...\n- **Problems**:\n  - Relationships typically do not meet the linear assumption, but seem \"ok\" for other assumptions.\n  - Poor fit to the data (over or underfitting in some areas).\n  - Difficult to interpret the results.\n\n\n![](images/fit.jpg)\n\n\n## Nonlinear regression\n\n- A way to model complex (nonlinear) relationships.\n  - i.e. phenomena that arise in the natural and physical sciences e.g. biology, chemistry, physics, engineering.\n- At least *one* predictor is not linearly related to the response variable.\n- Unique/specific shape - apply only if you are sure of the relationship, e.g. asymptotic, quadratic.\n\n\n## Performing nonlinear regression\n\n- **Polynomial regression**: still linear in the parameters and a good place to start.\n- **Nonlinear regression**: use the `nls()` function to fit the following nonlinear models:\n  - Exponential growth\n  - Exponential decay\n  - Logistic\n\n# Polynomial regression\n> A special case of multiple linear regression used to model nonlinear relationships.\n\n## Model\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i $$\n\nwhere $k$ is the degree of the polynomial.\n\n- The model is still linear in the parameters $\\beta$ and can be fitted using least squares.\n- Instead of multiple predictors, we have multiple *terms* of the same predictor (same $x$).\n- Only the *highest-order term* is tested for significance.\n- Can still be fit using `lm()`.\n- The more complex, the less likely it follows a true biological relationship...\n\n. . .\n\n<br>\n\n### Adding polynomial terms\n- Linear: $y = \\beta_0 + \\beta_1 x$\n- Quadratic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n- Cubic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$\n- Each level increases the power of the predictor by 1.\n\n# Polynomial fitting\nUsing the `asymptotic` data\n\n## The data\n\nSee Slide 11 for the relationship and mathematical expression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (linear)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_fit <- lm(response ~ predictor, asymptotic)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\", size = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (`poly(degree = 2)`)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly2_fit <- lm(response ~ poly(predictor, 2), asymptotic)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\", size = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (`poly(degree = 3)`)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly3_fit <- lm(response ~ poly(predictor, 3), asymptotic)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\") +\n  geom_line(aes(y = predict(poly3_fit)), color = \"seagreen\", size = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (`poly(degree = 10)`)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_10 x_i^{10} + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly10_fit <- lm(response ~ poly(predictor, 10), asymptotic)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n\n\nTable: Comparison of R^2^ of Polynomial Models\n\n|Model  |    R2|\n|:------|-----:|\n|Linear | 0.570|\n|Poly2  | 0.820|\n|Poly3  | 0.872|\n|Poly10 | 0.862|\n\n\n:::\n:::\n\n\n:::{.callout-note}\nWe use adjusted R^2^ for polynomials - extra terms, extra complexity, so extra penalty.\n:::\n\n## Limitations\n\n- Meaning of the coefficients is not always clear.\n- Extrapolation can be *dangerous*.\n- Extra terms can lead to overfitting and are difficult to interpret:\n- Parsimony: is the most complex term (highest power) significant? If not, use a lower power.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(poly10_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = response ~ poly(predictor, 10), data = asymptotic)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1659  -8.6908  -0.0494   8.8003  16.4012 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             79.818      1.552  51.426  < 2e-16 ***\npoly(predictor, 10)1   159.368     11.084  14.378  < 2e-16 ***\npoly(predictor, 10)2  -106.939     11.084  -9.648 5.37e-12 ***\npoly(predictor, 10)3    48.570     11.084   4.382 8.28e-05 ***\npoly(predictor, 10)4   -19.411     11.084  -1.751   0.0876 .  \npoly(predictor, 10)5     1.193     11.084   0.108   0.9148    \npoly(predictor, 10)6    -2.769     11.084  -0.250   0.8040    \npoly(predictor, 10)7    -1.343     11.084  -0.121   0.9042    \npoly(predictor, 10)8    -4.009     11.084  -0.362   0.7195    \npoly(predictor, 10)9    -2.851     11.084  -0.257   0.7984    \npoly(predictor, 10)10    5.769     11.084   0.520   0.6056    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.08 on 40 degrees of freedom\nMultiple R-squared:  0.8897,\tAdjusted R-squared:  0.8621 \nF-statistic: 32.26 on 10 and 40 DF,  p-value: 4.846e-16\n```\n\n\n:::\n:::\n\n\n\n### Still:\n\n- Easy to fit: just add polynomial terms to the model.\n- Simple to perform: use `lm()`.\n\n\n# Nonlinear fitting\n\n## Fitting a nonlinear model\n\nIf you have some understanding of the underlying relationship (e.g. mechanistic process) between the variables, you can fit a nonlinear model.\n<br>\n\n:::{.fragment}\n\n### Mathematical expression\n\n$$ Y_i = f(x_i, \\beta) + \\epsilon_i $$\n\nwhere $f(x_i, \\beta)$ is a nonlinear function of the parameters $\\beta$.\n\n- $Y_i$ is the continuous response variable.\n- $x_i$ is the vector of predictor variables.\n- $\\beta$ is the vector of unknown parameters.\n- $\\epsilon_i$ is the random error term (residual error).\n:::\n\n## Assumptions\n\nLike the linear model, the nonlinear model assumes *INE*:\n\n- Error terms are independent (**Independence**).\n- Error terms are normally distributed (**Normality**).\n- Error terms have equal/constant variance (**Homoscedasticity**).\n\nBasically:\n\n$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\n\n. . .\n\nLike all other models we have seen, we focus on the residuals to assess the model fit, since the residuals are the only part of the model that is random.\n\n## Estimating the model parameters\n\n:::{.columns}\n:::{.column width=\"50%\"}\n- The parameters are estimated using the **method of least squares**.\n- For nonlinear models, a nonlinear optimization algorithm is used to find the best fit, rather than ordinary least squares:\n  - [Gauss-Newton algorithm](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm)\n  - [Levenberg-Marquardt algorithm](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm)\n- This can only be performed iteratively and depends on a \"best guess\" of the parameters *as a start*.\n  - **i.e. we need to provide a starting point for a nonlinear least squares algorithm to begin**.\n\n:::\n:::{.column width=\"50%\"}\n\n![](images/gauss-newton.gif)\n\nSource: [Wikipedia](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm)\n\n:::\n:::\n\n\n## Two methods in R\n\n:::{.columns}\n:::{.column width=\"50%\"}\n\nUse `nls()` function in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnls(formula, data, start)\n```\n:::\n\n\n- `formula`: a formula object, response variable ~ predictor variable(s).\n- `data`: a data frame containing the variables in the model (response, predictor).\n- `start`: a named list of starting values for the parameters in the model.\n\n:::\n:::{.column width=\"50%\"}\n\nSelf-starting functions: `SSexpf()`, `SSasymp()`, `SSlogis()`, etc.\n\n- Self-starting functions estimate the starting values for you.\n- Named after the models they fit.\n- Existing functions have pre-set formulas.\n- Can define own functions but more complex than `nls()`.\n\n:::\n:::\n\n# Example: Fitting an exponential model\n\n## With `nls()`\n\n$y=y_0e^{kx}$\n\nwhere\n\n-   $y$ is the response and $x$ is the predictor\n-   $y_0$ is the value of $y$ when $x = 0$\n-   $k$ is the rate of change\n\n$k$ can be estimated with the equation $slope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}$, but usually a value of 1 is a good starting point.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\ngrowth <- tibble(\n  predictor = seq(0,10, by = 0.2),\n  response = abs(exp(0.5*predictor) + rnorm(length(predictor), mean = 1, sd = 5)))\n\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n## First guess\n\nBased on the plot, we can estimate $y_0 ~ 0$ and $k = 1$. Because of the equation, $y=y_0e^{kx}$, $y_0$ cannot be 0!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_exponential <- nls(response ~ y0*exp(k*predictor), data = growth, \n  start = list(y0 = 0.1, k = 1))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() + \n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_exponential)), color = \"red\", size = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n## Check assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nlstools)\nresids <- nlsResiduals(fit_exponential)\nplot(resids)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n- These plots determine if the residuals are normally distributed and have equal variance\n- *Normal QQ* looks good\n- *Residuals vs fitted* and *Standardized Residuals* even spread but slight fanning.\n- With *Autocorrelation* we want random scatter around 0 -- this indicates independence. Harder to meet with time-series data.\n- Nonlinear models typically should meet assumptions because they are fitted specifically to the data.\n\n## Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_exponential)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ y0 * exp(k * predictor)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \ny0   1.1694     0.1291   9.059 4.82e-12 ***\nk    0.4847     0.0121  40.057  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.409 on 49 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 1.204e-06\n```\n\n\n:::\n:::\n\n\n- The model is significant since the p-value is less than 0.05 for all parameters.\n- If this were real data (e.g. population growth), the parameters themselves e.g. rate of change, are useful\n- The parameterised model is:\n\n$$ y = 1.17 \\cdot e^{-0.484x} $$\nThe R-squared value is not reported for nonlinear models as the sum of squares is not partitioned into explained and unexplained components. You can use the **residual standard error** and plots instead to compare between models.\n\n## A really bad guess\n\nWhat if we don't estimate our parameters very well? R will either give an error or get there eventually.\n\nNote the parameters and residual standard error are the same as the previous slide - but the `Number of iterations to convergence` is higher.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_exponential <- nls(response ~ y0*exp(k*predictor), data = growth, \n  start = list(y0 = 50, k = 1.5)) # totally bogus numbers\n\nsummary(fit_exponential)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ y0 * exp(k * predictor)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \ny0   1.1694     0.1291   9.059 4.82e-12 ***\nk    0.4847     0.0121  40.057  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.409 on 49 degrees of freedom\n\nNumber of iterations to convergence: 28 \nAchieved convergence tolerance: 1.982e-06\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip}\nIf an error pops up, try different starting values - the **rate of change** is most likely the problem.\n:::\n\n## Fitting the model with `SSexpf()`\n\n- `SSexpf()` is from the `nlraa` package.\n- It has the same formula as above -- different names for parameters ($y_0$ = $a$, $k$ = $c$) but we can re-define them to anything we want\n- Reaches the same result but with less effort.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nlraa)\nfit_exponential_ss <- nls(response ~ SSexpf(predictor, y0, k), data = decay) \nsummary(fit_exponential_ss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ SSexpf(predictor, y0, k)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \ny0  1.65486    0.04699   35.22  < 2e-16 ***\nk  -0.06527    0.00590  -11.06 6.33e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1471 on 49 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 1.048e-06\n```\n\n\n:::\n:::\n\n\n# Example: Fitting an asymptotic model\n\n## The equation\n\n- There are multiple equations for asymptotic models, this is the equation that `SSasymp()` (base R) uses:\n\n$$ y = Asym + (R_0-Asym) \\cdot e^{-e^{lrc} \\cdot x} $$\n\n- $R_0$ is value of $y$ when $x = 0$.\n- $Asym$ is the upper limit: the maximum value of $y$.\n- $lrc$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-41-1.png){width=50%}\n:::\n:::\n\n\nSome plausible estimates -- $R_0 = 0$, $Asym = 100$, $lrc = 0.8$.\n\n## Fit model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit_asymptotic <- nls(response ~ Asym + (R0-Asym)*(exp(-exp(lrc)*predictor)), data = asymptotic, \n#   start = list(R0 = 0, Asym = 100, lrc = 0.8))\n\nfit_asymptotic <- nls(response ~ SSasymp(predictor, Asym, R0, lrc), data = asymptotic)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_asymptotic)), color = \"red\", size = 2)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-43-1.png){width=960}\n:::\n:::\n\n\n## Check assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nlstools)\nresids <- nlsResiduals(fit_asymptotic)\nplot(resids)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n\n## Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_asymptotic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ SSasymp(predictor, Asym, R0, lrc)\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nAsym  98.5204     2.2852  43.113  < 2e-16 ***\nR0   -14.5176     6.6416  -2.186  0.03374 *  \nlrc   -0.4626     0.1134  -4.079  0.00017 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 48 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 3.197e-07\n```\n\n\n:::\n:::\n\n\n- The model is significant since the p-value is less than 0.05 for all parameters.\n- If this were real data (e.g. population growth), the parameters themselves e.g. rate of change, are useful\n- The parameterised model is:\n\n$$ y = 98.5 + (-14.5-98.5) \\cdot e^{-e^{-0.463} \\cdot x}  $$\n# Example: fitting a logistic model\n\n## The equation\n\nThere are multiple equations for logistic models, but they all have an 'S' or sigmoid shape. The equation that `SSlogis()` (base R) assumes $y$ is positive and uses:\n\n$$ y = \\frac{Asym}{1+e^{\\frac{xmid-x}{scal}}} $$\nwhere\n\n- $Asym$ is the upper limit: the maximum value of $y$.\n- $xmid$ is the value of $x$ when $y$ is halfway between the lower and upper limits.\n- $scal$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_hline(yintercept = 300, linetype = \"dashed\") +\n  geom_vline(xintercept = 5, linetype = \"dashed\") +\n  # label the lines above\n  annotate(\"text\", x = 0, y = 300, label = \"Asym\", size = 8, vjust = 1.5) +\n  annotate(\"text\", x = 5, y = 100, label = \"xmid\", size = 8, hjust = -1) +\n  ## plot the rate\n  geom_segment(aes(x = 2.5, y = 60, xend = 6, yend = 250), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  # label the rate\n  annotate(\"text\", x = 4, y = 180, label = \"scal\", size = 8, colour = \"red\", hjust = -1)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-46-1.png){width=50%}\n:::\n:::\n\n\nSome starting values would be $Asym = 300$, $xmid = 5$, $scal = 1$.\n\n## Fit model\n\nEstimating the parameters or using the self-starting function `SSlogis()` gives a near-identical result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit_logistic <- nls(response ~ Asym/(1+exp((xmid-predictor)/scal)), data = logistic, \n#    start = list(Asym = 300, xmid = 5, scal = 1))\n\nfit_logistic <- nls(response ~ SSlogis(predictor, Asym, xmid, scal), data = logistic)\n\nresids <- nlsResiduals(fit_logistic)\nplot(resids)\n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-47-1.png){width=960}\n:::\n:::\n\n\n## Interpretation\n\n`SSlogis()` guessed the parameters on the first try.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_logistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ SSlogis(predictor, Asym, xmid, scal)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \nAsym 310.64727    4.62579   67.16   <2e-16 ***\nxmid   4.92715    0.07142   68.99   <2e-16 ***\nscal   1.34877    0.05418   24.90   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.22 on 48 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 6.626e-07\n```\n\n\n:::\n:::\n\n\n- The model is significant since the p-value is less than 0.05 for all parameters.\n- If the model visually fits well and relationship has reasoning (parameter significance not always important).\n- The parameterised model is:\n\n$$ y = \\frac{310}{1+e^{\\frac{4.93-x}{1.35}}} $$\n\n# How do we know which model is better? (Advanced)\nNote: this is non-examinable content but might be useful for your project.\n\n## Example: polynomial regression\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyr)\n\n# Create a new data frame with predictor values and model predictions\npredictions <- data.frame(\n  predictor = asymptotic$predictor,\n  Linear = predict(lin_fit),\n  Poly_2 = predict(poly2_fit),\n  Poly_3 = predict(poly3_fit),\n  Poly_10 = predict(poly10_fit)\n)\n\n# Reshape the data to long format\npredictions_long <- predictions %>%\n  pivot_longer(cols = -predictor, names_to = \"Model\", values_to = \"response\")\n\n# Plot the data\nggplot(predictions_long, aes(x = predictor, y = response, color = Model)) +\n  geom_point(data = asymptotic, aes(x = predictor, y = response), inherit.aes = FALSE) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  scale_color_brewer(palette = \"Spectral\") \n```\n\n::: {.cell-output-display}\n![](lecture-12_files/figure-revealjs/unnamed-chunk-49-1.png){width=960}\n:::\n:::\n\n\n## Prediction quality\n\nWe can use prediction quality metrics to compare the fits.\n\n- [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n[Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n  - Useful for comparing model fits.\n  - Has a penalty for more predictors\n- Residual standard error, residual sum of squares (`deviance(mod)`), [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\nand [mean absolute error (MAE)](https://en.wikipedia.org/wiki/Mean_absolute_error).\n  - Essentially the difference between observed and predicted (residuals).\n  - RMSE penalises larger residuals.\n\n## AIC and BIC\n\nUse the `broom` package to extract the AIC and BIC values from the model fits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n# collect all polynomial fits into a single tibble using glance\npoly_fits <- tibble(\n  model = c(\"linear\", \"poly2\", \"poly3\", \"poly10\"),\n  fit = list(lin_fit, poly2_fit, poly3_fit, poly10_fit)) %>%\n  mutate(glance = map(fit, glance)) %>%\n  unnest(glance) %>%\n  select(model, AIC, BIC)\npoly_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 Ã— 3\n  model    AIC   BIC\n  <chr>  <dbl> <dbl>\n1 linear  453.  459.\n2 poly2   409.  416.\n3 poly3   392.  402.\n4 poly10  402.  425.\n```\n\n\n:::\n:::\n\n\n- The smaller the AIC or BIC, the better the fit compared to other models.\n\n## Calculate RMSE and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npredictions <- data.frame(\n  observed = asymptotic$response,\n  Linear = predict(lin_fit),\n  Poly_2 = predict(poly2_fit),\n  Poly_3 = predict(poly3_fit),\n  Poly_10 = predict(poly10_fit)\n)\n\nerrors <- predictions %>%\n  pivot_longer(cols = -observed, names_to = \"Model\", values_to = \"Predicted\") %>%\n  group_by(Model) %>%\n  summarise(\n    RMSE = sqrt(mean((observed - Predicted)^2)),\n    MAE = mean(abs(observed - Predicted))\n)\n\nknitr::kable(errors, digits=2, caption = \"Comparison of RMSE and MAE for different models\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Comparison of RMSE and MAE for different models\n\n|Model   |  RMSE|   MAE|\n|:-------|-----:|-----:|\n|Linear  | 19.38| 15.17|\n|Poly_10 |  9.82|  8.57|\n|Poly_2  | 12.30|  9.88|\n|Poly_3  | 10.25|  8.83|\n\n\n:::\n:::\n\n\n- From the results, the polynomial to the degree of 10 has the lowest error - but visually we know it is overfitting, and the cubic polynomial is more parsimonius.\n- We can say the model has a prediction error of 10.25 units (RMSE) and 8.83 units (MAE).\n\n:::{.callout-note}\nBoth the RMSE and MAE measure error on the same scale as the response variable. e.g. if the response variable is in kg, the error will be in kg.\n:::\n\n## Summary\n\n::::{.columns}\n:::{.column width=\"70%\"}\n- With nonlinear relationships, there are three possible approaches:\n   1. **Linearise** the relationship by transforming: \n      - Fit: easy\n      - Interpret: difficult\n   2. Add **polynomial** terms: \n      - Fit: easy\n      - Interpret: difficult\n   3. Fit the model using a **nonlinear** algorithm:\n      - Fit: difficult\n      - Interpret: easy\n\n:::\n\n:::{.column width=\"30%\"}\n\n![](images/brain.jpg){width=75%}\n\n:::\n::::\n\n. . .\n\n- Nonlinear models:\n  - Useful for modelling more complex relationships. Require some understanding of the underlying relationship and equations.\n  - Mainly for prediction rather than interpreting relationships.\n  - Self-starting functions have limited pre-defined formulas.\n  - Assumptions **INE**.\n \n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/",
    "supporting": [
      "lecture-12_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}