{
  "hash": "59032543ea55f199874772def714dfcd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Topic 6 -- Two-sample *t*-tests\nauthor: Floris van Ogtrop\nformat: ochre-revealjs\n---\n\n# Testing differences in means\n## Recap: we have one sample {auto-animate=\"true\"}\n\n::: fragment\n**One-sample** *t*-test: compare the sample of data to a *fixed* value of interest (e.g. a hypothesised value, or a population mean).\n:::\n\n::: fragment\n### Examples\n\n-   *Is the mean height of students in ENVX different from the population mean of 170 cm?*\n-   *Is the mean heart rate of students in ENVX different from the population mean of 70 bpm*\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(patchwork)\nlibrary(ggplot2)\n\nset.seed(108)\nheights <- rnorm(100, mean = 170, sd = 10)\nheart <- rnorm(100, mean = 70, sd = 10)\n\n# heights\np1 <- ggplot(data.frame(heights), aes(x = heights)) +\n  geom_histogram(aes(y = after_stat(density)),\n    binwidth = 5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(heights)), \n    color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Height of students\") +\n  theme_classic()\n\n\n# heart rate\np2 <- ggplot(data.frame(heart), aes(x = heart)) +\n  geom_histogram(aes(y = after_stat(density)), \n    binwidth = 5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(heart)), \n    color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Heart rate\") +\n  theme_classic()\n\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-1-1.png)\n:::\n:::\n\n\n:::\n\n\n\n\n## What if we want to compare a sample of data to *another* sample?\n\n::: fragment\n### Examples\n\n-   *Is the mean height of students in ENVX1002 different from ENVX2001?*\n-   *Is the mean heart rate of students in ENVX1002 different from ENVX2001?*\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(129)\n## Generate data for 2 groups\nheights_all <- data.frame(\n  group = rep(c(\"ENVX1002\", \"ENVX2001\"), each = 100),\n  heights = c(rnorm(100, mean = 165, sd = 7), \n    rnorm(100, mean = 185, sd = 9)))\nheart_all <- data.frame(\n  group = rep(c(\"ENVX1002\", \"ENVX2001\"), each = 100),\n  heart_rates = c(rnorm(100, mean = 70, sd = 10), \n    rnorm(100, mean = 72, sd = 5)))\n\n## Plot\np3 <- ggplot(heights_all, aes(x = heights, fill = group)) +\n  geom_histogram(aes(y = ..density..), \n    binwidth = 5, color = \"black\", alpha = .2) +\n  geom_vline(aes(xintercept = 170), \n    color = \"red\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 185),\n    color = \"blue\", linetype = \"dashed\") +\n  ggtitle(\"Height of students (cm)\") +\n  theme_classic()\np4 <- ggplot(heart_all, aes(x = heart_rates, fill = group)) +\n  geom_histogram(aes(y = ..density..), \n    binwidth = 5, color = \"black\", alpha = .2) +\n  geom_vline(aes(xintercept = 70), \n    color = \"red\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 72), \n    color = \"blue\", linetype = \"dashed\") +\n  ggtitle(\"Heart rates (bpm)\") +\n  theme_classic()\n\np3 + p4\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-2-1.png)\n:::\n:::\n\n:::\n\n\n\n# Two-sample *t*-test\n## Comparing two samples: visualisation\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n::: fragment\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np5 <- ggplot(heights_all, aes(x = group, y = heights, fill = group)) +\n  geom_boxplot(alpha = .2) +\n  ggtitle(\"Height of students (cm)\") +\n  theme_classic()\np6 <- ggplot(heart_all, aes(x = group, y = heart_rates, fill = group)) +\n  geom_boxplot(alpha = .2) +\n  ggtitle(\"Heart rates (bpm)\") +\n  theme_classic()\n\np5 + p6\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-4-1.png)\n:::\n:::\n\n:::\n\n\n\n## Some considerations: the boxplot\n\n::: fragment\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-5-1.png)\n:::\n:::\n\n:::\n\n::: fragment\n- Trade-off between being able to see the distribution of the data and being able to compare between groups.\n- The *recommended* approach when comparing two or more groups of data in most cases.\n:::\n\n\n## \n\n![][1]\n\n  [1]: images/redbull.png\n\n\n\n# Does Red Bull increase the heart rate of students?\n## Data\n\n*A simulated example* (data is not real):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nredbull <- data.frame(\n  group = c(rep(\"redbull\", 12), rep(\"control\", 12)), \n  heart_rate = c(72, 88, 72, 88, 76, 75, 84, 80, 60, 96, 80,  84, 84, 76, 68, 80, 64, 62, 74, 84, 68, 96, 80, 64))\n```\n:::\n\n\n::: fragment\n**Experimental design**: two groups of students selected at random, *without* replacement, from the ENVX1002 cohort.\n\n-   `redbull` group: students who consumed 250 ml of Red Bull.\n-   `control` group: students who consumed 250 ml of water (control group).\n\nHeart rate in beats per minute (bpm) was measured *20 minutes after consumption*.\n:::\n\n::: fragment\n### Structure of data\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(redbull)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t24 obs. of  2 variables:\n $ group     : chr  \"redbull\" \"redbull\" \"redbull\" \"redbull\" ...\n $ heart_rate: num  72 88 72 88 76 75 84 80 60 96 ...\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n# HATPC\n\nHypothesis \\| Assumptions \\| Test \\| P-value \\| Conclusion\n\n## Hypothesis\n\nFor a two-sample *t*-test, the null hypothesis is that the means of the two groups are equal, and the alternative hypothesis is that the means are different.\n\n$$H_0: \\mu_{\\text{redbull}} = \\mu_{\\text{control}}$$\n$$H_1: \\mu_{\\text{redbull}} \\neq \\mu_{\\text{control}}$$\n\n::: fragment\n*Compare this to the one-sample *t*-test, where the null hypothesis is that the sample mean is equal to a fixed value:*\n$$H_0: \\mu = \\mu_0$$\n$$H_1: \\mu \\neq \\mu_0$$\n:::\n\n\n\n## Assumptions\nThe assumptions of the two-sample *t*-test include:\n\n1.  **Normality**: the data are normally distributed.\n2.  **Homogeneity of variance**: the variances of the two groups are equal.\n\n::: fragment\n### Why are these assumptions important?\n\n::: incremental\n\n- Since the *t*-test compares the means of two groups, normality ensures that the means are the *best estimate* of the population means.\n- Equal variances indicates that the two groups have similar \"noise\" influencing their means, except for the \"treatment\" effect.\n  - In the Red Bull example, this means that the range of heart rate values in students for both groups is similar, except for the effect of consuming Red Bull.\n:::\n:::\n\n\n# Assumption: normality\nHistogram | QQ-plot | Shapiro-Wilk test\n\n\n\n## Normality: histogram\n\n- We visually inspect the distribution of the data using histograms, generally for **each group**.\n- Look out for: symmetry, skewness, and multimodality.\n- Hard to visualise when n (sample size is small)\n\n\n::: panel-tabset\n\n## R base graphics\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 2))\nhist(redbull$heart_rate[redbull$group == \"redbull\"], \n  main = \"Red Bull group\", xlab = \"Heart rate (bpm)\", \n  col = \"lightblue\", border = \"black\")\nhist(redbull$heart_rate[redbull$group == \"control\"],\n  main = \"Control group\", xlab = \"Heart rate (bpm)\", \n  col = \"lightblue\", border = \"black\")\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-8-1.png)\n:::\n:::\n\n\n## ggplot2\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nggplot(redbull, aes(x = heart_rate, fill = group)) +\n    geom_histogram(aes(y = ..density..),\n        binwidth = 12, color = \"black\", alpha = .5\n    ) +\n    facet_wrap(~group) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-9-1.png)\n:::\n:::\n\n\n## ggpubr\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggpubr)\ngghistogram(redbull, x = \"heart_rate\", fill = \"group\", binwidth = 6) +\n    facet_wrap(~group) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-10-1.png)\n:::\n:::\n\n\n:::\n\n::: fragment\nConclusion: *The data appear to be normally distributed, but it is better to confirm this with a QQ-plot.* \n:::\n\n\n\n## Normality: QQ-plot\n\n- The qq-plot is a graphical method to *specifically* assess the normality of the data. Again, we look at the data for **each group**.\n- Look out for: deviations from the straight line.\n\n::: panel-tabset\n\n## R base graphics\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 2))\nqqnorm(redbull$heart_rate[redbull$group == \"redbull\"],\n    main = \"Red Bull group\", xlab = \"Theoretical quantiles\",\n    ylab = \"Sample quantiles\", col = \"blue\"\n)\nqqline(redbull$heart_rate[redbull$group == \"redbull\"], col = \"red\")\n\nqqnorm(redbull$heart_rate[redbull$group == \"control\"],\n    main = \"Control group\", xlab = \"Theoretical quantiles\",\n    ylab = \"Sample quantiles\", col = \"blue\"\n)\nqqline(redbull$heart_rate[redbull$group == \"control\"], col = \"red\")\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-11-1.png)\n:::\n:::\n\n\n## ggplot2\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nggplot(redbull, aes(sample = heart_rate)) +\n    stat_qq() +\n    stat_qq_line() +\n    facet_wrap(~group) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-12-1.png)\n:::\n:::\n\n\n## ggpubr\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggpubr)\nggqqplot(redbull$heart_rate) +\n    facet_wrap(~ redbull$group) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-13-1.png)\n:::\n:::\n\n\n:::\n\n::: fragment\nConclusion: *The data appear to be normally distributed.*\n:::\n\n\n\n## Normality: formal test\n\n- Use the Shapiro-Wilk test which tests the null hypothesis that the data are normally distributed.\n- This test is sensitive to deviations from normality in the tails of the distribution, and is suitable for small sample sizes (about 5 to 50 observations).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nshapiro.test(redbull$heart_rate[redbull$group == \"redbull\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  redbull$heart_rate[redbull$group == \"redbull\"]\nW = 0.97459, p-value = 0.9524\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nshapiro.test(redbull$heart_rate[redbull$group == \"control\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  redbull$heart_rate[redbull$group == \"control\"]\nW = 0.93733, p-value = 0.4643\n```\n\n\n:::\n:::\n\n\n::: fragment\nConclusion: *p*-values are greater than 0.05, so we do not reject the null hypothesis of normality. The data are normally distributed.\n:::\n\n\n\n## What if the normality assumption is violated?\n\n- The *t*-test is robust to deviations from normality, especially for large sample sizes due to the Central Limit Theorem.\n- If the sample size is small, consider using a non-parametric test (e.g. the Wilcoxon rank-sum test): **next week**\n- Alternatively, transform the data: **later**\n\n\n\n# Assumption: homogeneity of variance\nBoxplots | Formal tests \n\n\n\n## Equal variances: boxplot\n\n- We visually inspect the spread of the data using boxplots, generally for **each group**.\n- Look out for: differences in spread, outliers, and symmetry.\n\n::: panel-tabset\n\n## R base graphics\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 2))\nboxplot(heart_rate ~ group,\n    data = redbull,\n    main = \"Heart rate\", xlab = \"Group\", ylab = \"Heart rate (bpm)\"\n)\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n## ggplot2\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nggplot(redbull, aes(x = group, y = heart_rate, fill = group)) +\n    geom_boxplot(alpha = .2) +\n    labs(x = \"Group\", y = \"Heart rate (bpm)\") +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-16-1.png)\n:::\n:::\n\n\n## ggpubr\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggpubr)\nggboxplot(redbull,\n    x = \"group\", y = \"heart_rate\",\n    fill = \"group\", alpha = .2\n) +\n  labs(x = \"Group\", y = \"Heart rate (bpm)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-17-1.png)\n:::\n:::\n\n\n:::\n\n::: fragment\nConclusion: *The spread of the data appears to be similar between the two groups.*\n:::\n\n\n\n## Equal variances: formal tests\n\n- Bartlett's and Levene's tests may be used to test the null hypothesis that the variances of the groups are equal.\n- These tests are sensitive to deviations from normality (Levene's test is less so compared to Bartlett's), and are suitable for small sample sizes.\n\n::: panel-tabset\n\n## Levene's test\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: carData\n```\n\n\n:::\n\n```{.r .cell-code}\nleveneTest(heart_rate ~ group, data = redbull)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1   0.289 0.5962\n      22               \n```\n\n\n:::\n:::\n\n\n## Bartlett's test\n\n::: {.cell}\n\n```{.r .cell-code}\nbartlett.test(heart_rate ~ group, data = redbull)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  heart_rate by group\nBartlett's K-squared = 0.075121, df = 1, p-value = 0.784\n```\n\n\n:::\n:::\n\n\n:::\n\n::: fragment\nConclusion: *p*-values are greater than 0.05, so we do not reject the null hypothesis of equal variances. The variances of the two groups are equal.\n:::\n\n\n\n\n\n\n## What if the equal variance assumption is violated?\n\nSome debate exists on what to do, but choices include:\n\n- Use the **Welch's *t*-test**, which is robust to unequal variances: *coming up next*\n- **Transform** the data to stabilise the variances: **later**\n- Perform a non-parametric test: **next week**\n\n\n\n## The Welch's *t*-test\n\n- The Welch's *t*-test is a modification of the two-sample *t*-test that **does not assume equal variances**.\n- Also applicable when the **sample sizes are unequal**.\n\n### Why not use the Welch's *t*-test all the time?\n\n- Ongoing debate on whether to use the Welch's *t*-test or the Student's *t*-test when the variances are equal. \n- The Welch's *t*-test is generally considered more robust, and **is the default in R's `t.test()` function**.\n- You can still use the Student's *t*-test by setting `var.equal = TRUE` in the `t.test()` function.\n\n\n## Are the assumptions of normality and homogeneity of variance met?\nWhen reporting in journals, it is common to simply state that the assumptions were met and what tests were used to confirm them, without showing the exact results of the tests!\n\n::: fragment\n### Example 1\n\n> *The assumptions of normality and homogeneity of variance were met for the data (Shapiro-Wilk test, $p > 0.05$; Levene's test, $p > 0.05$). Thus, we performed a two-sample *t*-test...*\n:::\n\n::: fragment\n### Example 2\n\n> *Visual inspection of the histograms, QQ-plots, and boxplots showed that the data met the assumptions of both normality and homogeneity of variance. Thus, we performed a two-sample *t*-test..*\n:::\n\n::: fragment\n**For your lab reports, you should show the results of the tests (because we want to check your work!)**\n:::\n\n\n\n# P-value and Conclusion\n## Performing the *t*-test\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(heart_rate ~ group, data = redbull, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  heart_rate by group\nt = -1.1365, df = 22, p-value = 0.268\nalternative hypothesis: true difference in means between group control and group redbull is not equal to 0\n95 percent confidence interval:\n -12.947117   3.780451\nsample estimates:\nmean in group control mean in group redbull \n             75.00000              79.58333 \n```\n\n\n:::\n:::\n\n\nResults indicate that the means of the two groups are **not** significantly different (p = 0.27).\n\n::: fragment\n### Compare with the Welch's *t*-test\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nt.test(heart_rate ~ group, data = redbull)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  heart_rate by group\nt = -1.1365, df = 21.845, p-value = 0.2681\nalternative hypothesis: true difference in means between group control and group redbull is not equal to 0\n95 percent confidence interval:\n -12.950568   3.783902\nsample estimates:\nmean in group control mean in group redbull \n             75.00000              79.58333 \n```\n\n\n:::\n:::\n\n:::\n\n\n\n## Conclusion\nDifferences in heart rate we not statistically significant between the Red Bull and control groups (t~22~ = -1.1, p = 0.27) indicating that Red Bull did not significantly increase the heart rate of students sampled from ENVX1002.\n\n\n\n\n# The paired *t*-test\n## Are the two sample independent?\nWhen testing if two samples are different from each other, we need to consider two possible scenarios:\n\n::: fragment\n-   **Independent samples**: The samples are drawn from two different populations, **or** the samples are not related to each other -- **independent groups**.\n-   **Related samples**: The samples are drawn from the same population, **and/or** the samples are related to each other -- **repeated measures** or **matched pairs**.\n:::\n\n::: fragment\nIf the samples are related, a **paired *t*-test** is more appropriate than a two-sample *t*-test as it accounts for the relationship between the samples that could confound the results.\n:::\n\n\n\n## Paired *t*-test\n### Experimental design (what if?)\n#### Before\n\n> Two groups of students selected at random, *without* replacement, from the ENVX1002 cohort.\n\n::: fragment\n\n#### Paired design\n**The same student** was used in a before/after experiment, where the heart rate was measured before and after consuming 250ml of Red Bull. Twelve (12) students were selected at random from the ENVX1002 cohort.\n\n::: incremental\n- Data is no longer independent, as the same student is measured twice.\n- The student now confounds the results, as the heart rate of the same student is likely to be **correlated** even without consuming Red Bull.\n- Total number of students is now 12, not 24.\n- **Let's assume the data collected are exactly the same.**\n:::\n:::\n\n\n\n## Hypothesis\nFor a paired *t*-test, the null hypothesis is that the mean difference between the two groups is zero, and the alternative hypothesis is that the mean difference is different from zero.\n\n$$H_0: \\mu_{\\text{diff}} = 0$$\n$$H_1: \\mu_{\\text{diff}} \\neq 0$$\n\n::: fragment\n*Compare this to the two-sample *t*-test, where the null hypothesis is that the means of the two groups are equal:*\n$$H_0: \\mu_{\\text{redbull}} = \\mu_{\\text{control}}$$\n$$H_1: \\mu_{\\text{redbull}} \\neq \\mu_{\\text{control}}$$\n\n:::\n\n\n\n## Assumptions of the paired *t*-test\n\n- The assumption of the paired *t*-test is that the differences between the two groups are normally distributed.\n- There is no assumption of equal variances, as the paired *t*-test is a one-sample *t*-test on the differences.\n  - Another way to think about it is that since the data are paired, the variance of the differences is the same for both groups.\n\n\n## Performing the paired *t*-test\n\nThere are two ways.\n\n:::: columns\n::: column\n::: fragment\n### Method 1: Calculate the differences, then perform a one-sample *t*-test using `t.test()`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndiff <- redbull$heart_rate[redbull$group == \"redbull\"] - \n  redbull$heart_rate[redbull$group == \"control\"]\nt.test(diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  diff\nt = 1.6578, df = 11, p-value = 0.1256\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.501628 10.668294\nsample estimates:\nmean of x \n 4.583333 \n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n::: column\n::: fragment\n### Method 2: Use the `t.test()` function with the `paired = TRUE` argument\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# t.test(heart_rate ~ group, data = redbull, \n#   paired = TRUE)\n```\n:::\n\n\n:::\n:::\n::::\n\n::: fragment\nThe results for both methods are identical; the mean difference is not significantly different from zero (p = 0.13).\n:::\n\n\n# When assumptions of the *t*-test are violated\n\n## Recap: Assumptions of the two-sample *t*-test\n\n### With independent samples:\n\n- **Normality**: the data are normally distributed\n- **Homogeneity of variance** (equal variances): the variances of the two groups are equal\n\n### With paired samples:\n\n- **Normality**: the differences between the paired samples are normally distributed\n- Equal variances is implied\n\n## If we analyse the data anyway...\n\nThe *t*-test:\n\n- may provide incorrect results as **mean and variance calculations depend on normally distributed data**.\n- may be **less powerful** (i.e., less likely to detect a true difference).\n- may be **biased** (i.e., systematically over- or under-estimating the true difference).\n\n# Don't throw the data away...\n\n## What can we do?\n\nThe *t*-test is quite robust to violations of normality, especially when the sample size is large. However, the assumption of equal variances is more critical – we cannot simply depend on large sample sizes to \"fix\" the problem.\n\nOptions include:\n\n- **Transform** the data to normalise the data and/or scale the variance\n- Use a **Welch's *t*-test** or a **Welch's ANOVA** (limited cases)\n- Use a **non-parametric test**, such as the **Mann-Whitney U test** or **Wilcoxon signed-rank test** (paired samples) -- however, these tests have *less power* than the *t*-test i.e. less likely to detect a true difference.\n\n\n# Ants - a foraging biomass study\n\n![*Formica rufa*, the horse ant - native to Eurasia.](images/formica_rufa.jpg) {fig-align=\"left\"}\n\n## Is the food collected by ants different between two sites?\n\n### Data structure\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.2.0     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ lubridate 1.9.5     ✔ tibble    3.3.1\n✔ purrr     1.2.1     ✔ tidyr     1.3.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nants <- read.csv(\"data/ants.csv\") %>%\n  mutate(Tree = factor(Tree))\n\nglimpse(ants)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 54\nColumns: 2\n$ Food <dbl> 11.9, 33.3, 4.6, 5.5, 6.2, 11.0, 24.3, 20.7, 5.7, 12.6, 10.2, 4.7…\n$ Tree <fct> Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Ro…\n```\n\n\n:::\n:::\n\n\nWe want to compare the mean biomass of food, collected by ants between the two sites in **dry weight (mg) of prey, divided by the total number of ants leaving the tree in 30 minutes**.\n\n## Visualising the data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\np_ants <-\n  ggplot(ants, aes(x = Tree, y = Food)) +\n  geom_boxplot() +\n  ylab(\"Biomass of food (mg per ant)\") +\n  theme_minimal()\n\np_ants\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-25-1.png)\n:::\n:::\n\n\nDoes this data meet the assumptions of the two-sample *t*-test?\n\n## Checking assumptions\n\nWe have some idea that the data may not be normally distributed, but are not quite sure. So let's check using the Q-Q plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(ants, aes(sample = Food)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Tree) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-26-1.png)\n:::\n:::\n\n\n::: incremental\n- Curvature of the data points away from the line indicates non-normality.\n- Boxplots (previous slide) suggest equal variances.\n- **Let's transform the data.**\n:::\n\n\n\n## Picking a transformation\n\nWe need to consider the **type of data** and the **shape of its distribution** when choosing a transformation. These can be assessed using:\n\n::: incremental\n- **Histograms** and **Q-Q plots** to assess normality - DONE\n- **Box plots** to assess homogeneity of variance - DONE\n- **Skewness** and **kurtosis** to assess the shape of the distribution - NEXT\n:::\n\n\n## Skewness\n\nThe degree of asymmetry in the data distribution when compared to a normal distribution.\n\n::: fragment\n- Represented by the **skewness coefficient** ($\\gamma_1$) and can be positive, negative, or zero.\n- Skewness values between **-0.5 and 0.5** are considered acceptable (fairly symmetrical).\n- **Negative** skewness indicates a *left*-skewed distribution, while **positive** skewness indicates a *right*-skewed distribution.\n- Above 1 or below -1, the distribution is considered ***highly* skewed**.\n:::\n\n## Example: skewness\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(patchwork)\n\nx <- seq(0, 1, length.out = 100)\n\n# Calculate the density of the Beta distribution at these points\ndata1 <- data.frame(x = x, y = dbeta(x, 5, 2), dist = \"Negative (left) skewed\")\ndata2 <- data.frame(x = x, y = dbeta(x, 5, 5), dist = \"Symmetrical\")\ndata3 <- data.frame(x = x, y = dbeta(x, 2, 5), dist = \"Positive (right) skewed\")\ndata <- rbind(data1, data2, data3) %>%\n  mutate(dist = factor(dist, levels = c(\"Negative (left) skewed\", \"Symmetrical\", \"Positive (right) skewed\")))\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_line(color = \"skyblue\") +\n  geom_area(fill = \"skyblue\", alpha = 0.4) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  facet_wrap(~dist) +\n  ylab(\"density\") +\n  xlab(\"\")\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-27-1.png)\n:::\n:::\n\n\n## Kurtosis\n\nUsed to describe the extreme values (outliers) in the distribution versus the tails.\n\n::: fragment\n- **High kurtosis (>3)** indicates a distribution with **heavy tails** and a **peaked centre**. When this happens, we should investigate the data for outliers.\n- **Low kurtosis (<3)** indicates a distribution with **light tails** and a **flat centre**. There are fewer to no outliers in the data.\n:::\n\n\n## Example: kurtosis\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(moments)\n\n# Generate data\nset.seed(123)\nx1 <- seq(-10, 10, length.out = 1000)\nx2 <- seq(-5, 5, length.out = 1000)\ndata1 <- data.frame(x = x1, y = dt(x1, df = 1), dist = \"High Kurtosis\")\ndata2 <- data.frame(x = x2, y = dt(x2, df = 10), dist = \"Low Kurtosis\")\ndata <- rbind(data1, data2) %>%\n  mutate(dist = factor(dist, levels = c(\"High Kurtosis\", \"Low Kurtosis\")))\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_line(color = \"skyblue\") +\n  geom_area(fill = \"skyblue\", alpha = 0.4) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  facet_wrap(~dist, scales = \"free_x\") +\n  ylab(\"density\") +\n  xlab(\"\")\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-28-1.png)\n:::\n:::\n\n\n\n## Skewness and kurtosis in the ants data\n\nWith experience we can \"eyeball\" the data, but we can also calculate the skewness and kurtosis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nants %>%\n  group_by(Tree) %>%\n  summarise(skewness = skewness(Food), kurtosis = kurtosis(Food))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  Tree     skewness kurtosis\n  <fct>       <dbl>    <dbl>\n1 Rowan       1.04      3.15\n2 Sycamore    0.807     2.63\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-30-1.png)\n:::\n:::\n\n\n::: fragment\nFrom the results we can see that both sites have a **positive skewness**. Site `Rowan` has high kurtosis.\n:::\n\n\n\n# Data transformation\n\n\n\n## Workflow\n\n::: incremental\n1. Check the data for normality and homogeneity of variance (i.e. **test assumptions**). \n2. If the assumptions are violated, consider **transforming the data**.\n3. **Repeat** checks on assumptions. If assumptions are **met**, proceed with the *t*-test on the transformed scale. *Otherwise, use a different transformation or consider using a non-parametric test.*\n4. Interpret the statistical results and **back-transform the results** to the original scale (optional but recommended) to aid interpretation.\n:::\n\n\n## Picking a transformation\n\n::: fragment\n### For positive skewness\n\n- **Square root** transformation: $\\sqrt{x}$ for skewness between 0.5 and 1 and kurtosis < 3.\n- **Logarithmic** transformation: $\\log(x)$ for skewness > 1 and kurtosis < 3.\n- **Reciprocal** transformation: $\\frac{1}{x}$ for skewness > 1 and kurtosis > 3 (quite extreme).\n:::\n\n\n::: fragment\n### For negative skewness\n- This is rare as most biological data are positively skewed. However, you can try the **square** $x^2$ or **cube** $x^3$ transformation.\n- If negatively skewed data contains zeros, consider using the log transform and adding a constant to the data before transformation e.g. $\\log(x + 1)$.\n:::\n\n::: fragment\n::: callout-note\nThere is also the **Box-Cox transformation** which informs us of the best transformation to apply to the data without the need to check skewness and kurtosis. This method is not covered in this unit, but you can read more about it [here](https://r-coder.com/box-cox-transformation-r/) (the simple R version) or [here](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation) (more detailed mathematical explanation).\n:::\n:::\n\n## How do we check if the transformation worked?\n\nWe need to apply the transformation to the entire dataset and check the Q-Q plot again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nants$Food_log <- log(ants$Food)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# compare the Q-Q plot before and after transformation\npfood <- ggplot(ants, aes(sample = Food)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Tree) +\n  ggtitle(\"Before transformation\") +\n  theme_classic()\n\npfoodlog <- ggplot(ants, aes(sample = Food_log)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Tree) +\n  ggtitle(\"After transformation\") +\n  theme_classic()\n\npfood / pfoodlog\n```\n\n::: {.cell-output-display}\n![](lecture-06_files/figure-typst/unnamed-chunk-32-1.png)\n:::\n:::\n\n\n## Checking skewness and kurtosis after transformation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nants %>%\n  group_by(Tree) %>%\n  summarise(skewness = skewness(Food_log), kurtosis = kurtosis(Food_log))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  Tree      skewness kurtosis\n  <fct>        <dbl>    <dbl>\n1 Rowan    0.271         1.77\n2 Sycamore 0.0000457     1.86\n```\n\n\n:::\n:::\n\n\n## Performing the *t*-test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- t.test(Food_log ~ Tree, data = ants, var.equal = TRUE)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  Food_log by Tree\nt = -2.0521, df = 52, p-value = 0.04521\nalternative hypothesis: true difference in means between group Rowan and group Sycamore is not equal to 0\n95 percent confidence interval:\n -0.732858080 -0.008203447\nsample estimates:\n   mean in group Rowan mean in group Sycamore \n              2.287756               2.658287 \n```\n\n\n:::\n:::\n\n\n\n::: fragment\n### How do we interpret the results?\n\nEvidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045).\n:::\n\n\n## Back-transforming the results\n\n- For power transformations, we can back-transform the results to the original scale using the inverse function.\n- Log transformations are a bit tricky as the inverse function is the exponential function.\n  - For the natural log transformation which is `log()` in R, the inverse function is the exponential function: $e^x$.\n  - For the base 10 log transformation which is `log10()` in R, the inverse function is $10^x$.\n\n## Interpretation\n\n### Back-transforming mean values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrowan <- exp(fit$estimate[[1]]) # mean biomass from the Rowan site\nbsycamore <- exp(fit$estimate[[2]]) # mean biomass from the Sycamore site\n\n# check the ratio\nbsycamore / browan\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.448503\n```\n\n\n:::\n:::\n\n\n> Evidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045).\n\nThe mean biomass of food collected by ants from the Sycamore site (14.3 mg) is 1.4 times greater than the mean biomass of food collected by ants from the Rowan site (9.9 mg).\n\n### Back-transforming confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nant_ci <- exp(fit$conf.int)\nant_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4805336 0.9918301\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n\n\n## Comparing to a test without transformation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- t.test(Food ~ Tree, data = ants, var.equal = TRUE)\nfit2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  Food by Tree\nt = -1.9217, df = 52, p-value = 0.06013\nalternative hypothesis: true difference in means between group Rowan and group Sycamore is not equal to 0\n95 percent confidence interval:\n -10.4916030   0.2267678\nsample estimates:\n   mean in group Rowan mean in group Sycamore \n              12.27143               17.40385 \n```\n\n\n:::\n:::\n\n\n:::: columns\n::: column\n- Original mean values: \n  - Rowan = 12.3 mg\n  - Sycamore = 17.4 mg\n- Log-transformed mean values: \n  - Rowan = 2.3 lg(mg)\n  - Sycamore = 2.7 lg(mg)\n- Back-transformed mean values: \n  - **Rowan = 9.9 mg**\n  - **Sycamore = 14.3 mg**\n  \n\nThe original mean values are based on the arithmetic mean, while the log-transformed mean values are based on the geometric mean. The geometric mean is more appropriate for skewed data.\n\n:::\n\n::: column\n- Original 95% confidence interval: \n  - -10.5 to 0.2 mg\n- Log-transformed 95% confidence interval:\n  - 0.5 to 1 lg(mg)\n- Back-transformed 95% confidence interval:\n  - **1.6 to 2.7 mg**\n\nThe influence of kurtosis on the 95% confidence interval is evident when comparing the original and back-transformed confidence intervals, as the log transform reduces the effect of outliers on the data.\n:::\n::::\n\n\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template] and is licensed under a [Creative Commons Attribution 4.0 International License].\n\n[SOLES Quarto reveal.js template]: https://github.com/usyd-soles-edu/soles-revealjs\n[Creative Commons Attribution 4.0 International License]: http://creativecommons.org/licenses/by/4.0/\n\n\n\n## References\n\n- Quinn G. P. & Keough M. J. (2002) Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK.\n- Logan, M. (2010). Biostatistical design and analysis using R a practical guide. Hoboken, N.J., Wiley-Blackwell.\n \n",
    "supporting": [
      "lecture-06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}