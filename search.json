[
  {
    "objectID": "lectures/L08/lecture-08.html#non-normal-data",
    "href": "lectures/L08/lecture-08.html#non-normal-data",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Non-normal data",
    "text": "Non-normal data\nWhere data does not meet the assumptions of parametric tests, we have two options:\n\nTransform the data, and continue with parametric tests; or\nUse non-parametric “equivalents” of parametric tests at the cost of power and loss of information.\n\n\n\nA third option exists.\n\n\n\nUse computer intensive, randomisation-based methods to test hypotheses, called resampling techniques.\nThese methods include randomisation (or permutation) tests and bootstrap.\nRetains estimates of effect size and confidence intervals."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#model-based-inferential-techniques",
    "href": "lectures/L08/lecture-08.html#model-based-inferential-techniques",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Model-based inferential techniques",
    "text": "Model-based inferential techniques\n\n\nTraditionally, inferential statistics is based on mathematical approximations and assumptions about how data is obtained.\nBased on knowledge that “randomness” somehow obeys certain patterns in nature which can be reliably described by probability distributions.\nUses probability theory to draw approximate conclusions about these patterns when we observe data."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#resampling-techniques-1",
    "href": "lectures/L08/lecture-08.html#resampling-techniques-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Resampling techniques",
    "text": "Resampling techniques\n\n\nBased on the idea that we can use the data itself to estimate the distribution of the test statistic or parameter of interest.\nThese methods are model-free and distribution-free.\nRequires comparatively higher computational power, but nowadays it is not a problem – any modern personal computer can handle it."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#randomisation-or-bootstrap",
    "href": "lectures/L08/lecture-08.html#randomisation-or-bootstrap",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Randomisation or Bootstrap?",
    "text": "Randomisation or Bootstrap?\n\nThey are not the same:\n\n\n\n\nRandomisation\nGenerate a distribution of the test statistic under the null hypothesis by randomly sub-sampling the data, without replacement. Can be used to estimate a p-value.\n\n\n\n\nBootstrap\nGenerate a distribution of the parameter of interest (e.g. mean) by resampling the data with replacement1. Can be used to estimate confidence intervals.\n\n\n\n\n\n\nBasically, shuffle the data and see what happens.\n\n\n\n\n\nBasically, create alternative versions of the data and see what happens.\n\n\n\n\nAlso known as hallucination as it creates alternative versions of the data."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#why-would-these-techniques-work",
    "href": "lectures/L08/lecture-08.html#why-would-these-techniques-work",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Why would these techniques work?",
    "text": "Why would these techniques work?\nAt the core of the resampling approach is the idea that the observed data is a random sample from a larger population.\n\n\nIf the sampled data is truly representative of the population…\nThen, if we infinitely resample from the sample itself, we should be able to somewhat approximate the distribution of the test statistic under the null hypothesis, or parameter of interest (will show example later)."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#example-comparing-two-groups",
    "href": "lectures/L08/lecture-08.html#example-comparing-two-groups",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Example: comparing two groups",
    "text": "Example: comparing two groups\n\n\nSuppose we have two samples (groups) and we want to test if the mean scores1 are different.\nUnder the null hypothesis that there is no difference between the groups, the two sets of scores will have the same distribution.\nThus, we can pool the scores and reassign them to the two groups, since any score is equally likely to belong in either group, i.e. the scores are exchangeable.\n\n\nBasically any measure of interest."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#steps",
    "href": "lectures/L08/lecture-08.html#steps",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Steps",
    "text": "Steps\n\n\nPool the scores from both groups into a single dataset.\nRandomly reassign the scores to two groups.\nCalculate the test statistic of interest, in this case the t-test statistic.\nRepeat steps 2 and 3 many times to generate a distribution of the test statistic under the null hypothesis.\nCompare the observed test statistic to the randomised distribution to calculate a p-value."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#data",
    "href": "lectures/L08/lecture-08.html#data",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Data",
    "text": "Data\nThe sleep dataset in R contains the average extra hours of sleep, compared to control, for 10 patients who were given two different drugs.\n\n\nRows: 20\nColumns: 3\n$ extra &lt;dbl&gt; 0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0.0, 2.0, 1.9, 0.8, …\n$ group &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ ID    &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#are-assumptions-of-normality-met",
    "href": "lectures/L08/lecture-08.html#are-assumptions-of-normality-met",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Are assumptions of normality met?",
    "text": "Are assumptions of normality met?\nWe picked a dataset where the assumptions are met, so that we can compare the results with the parametric test."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#calculating-the-t-test-statistic",
    "href": "lectures/L08/lecture-08.html#calculating-the-t-test-statistic",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Calculating the t-test statistic",
    "text": "Calculating the t-test statistic\nRecall that the test statistic for the two-sample t-test is:\n t = \\frac{observed\\ value - expected\\ value}{standard\\ error} \nWe could calculate it manually, but let’s just use the t.test() function in R since the function calculates the test statistic for us. For example, the observed test statistic for the sleep data is:\n\n\n        t \n-1.860813"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-1-pool-the-scores",
    "href": "lectures/L08/lecture-08.html#step-1-pool-the-scores",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 1: Pool the scores",
    "text": "Step 1: Pool the scores\nThe first step is to pool the data. The pooled data is:\n\n\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0  1.9  0.8  1.1  0.1 -0.1\n[16]  4.4  5.5  1.6  4.6  3.4\n\n\nWhere the first 10 scores are from the first group, and the next 10 scores are from the second group."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-2-randomly-reassign-the-scores",
    "href": "lectures/L08/lecture-08.html#step-2-randomly-reassign-the-scores",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 2: Randomly reassign the scores",
    "text": "Step 2: Randomly reassign the scores\n\n\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0  1.9  0.8  1.1  0.1 -0.1\n[16]  4.4  5.5  1.6  4.6  3.4\n\n\nNext, we randomly shuffle the pooled data and re-assign the first 10 scores to group 1, and the next 10 scores to group 2.\n\n\n [1] -1.2  2.0  3.7  0.1  0.8  1.6  0.8  1.9  0.0  3.4 -1.6 -0.2  1.1 -0.1  0.7\n[16]  4.4  5.5  3.4 -0.1  4.6"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-3-calculate-the-test-statistic",
    "href": "lectures/L08/lecture-08.html#step-3-calculate-the-test-statistic",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 3: Calculate the test statistic",
    "text": "Step 3: Calculate the test statistic\nWe’re not using the results from the t.test() function, but just extracting the test statistic.\n\n\n         t \n-0.4995608"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-4-repeat-many-times",
    "href": "lectures/L08/lecture-08.html#step-4-repeat-many-times",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 4: Repeat many times",
    "text": "Step 4: Repeat many times\nPutting it all together, we can write a function to obtain the test statistic:\nRepeat the function 10,000 times:"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-5-compare-the-observed-test-statistic",
    "href": "lectures/L08/lecture-08.html#step-5-compare-the-observed-test-statistic",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 5: Compare the observed test statistic",
    "text": "Step 5: Compare the observed test statistic\nFinally, we can compare the observed test statistic to the randomised distribution. This can be done by calculating the proportion of randomised test statistics that are more extreme than the observed test statistic.\n\n\n[1] 0.08\n\n\nHow does this compare to the parametric t-test?\nIf we round the p-values of both tests to two decimal places, we get:\n\n\n[1] 0.08\n\n\nAs we can see, the p-values are very similar. This is because the assumptions of the parametric test are met, so the results will be close even though the methods are different!"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#whats-the-difference-between-the-two-techniques",
    "href": "lectures/L08/lecture-08.html#whats-the-difference-between-the-two-techniques",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "What’s the difference between the two techniques?",
    "text": "What’s the difference between the two techniques?\n\n\n\nt-test\n\nThe parametric test compares the observed test statistic to a theoretical distribution that has fixed parameters.\n\n\nRandomisation test\n\nThe randomisation test compares the observed test statistic to a randomised distribution that is generated from the data itself.\n\n\n\n\n\n\n\nAssumes that the data is normally distributed.\n\n\n\nNo assumptions about the data distribution, but if the assumption were met, the results would be similar.\n\n\n\n\n\n\n\nP-value is calculated from the theoretical distribution.\n\n\n\nP-value is calculated from the simulated distribution."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#visualising-the-randomised-distribution",
    "href": "lectures/L08/lecture-08.html#visualising-the-randomised-distribution",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Visualising the randomised distribution",
    "text": "Visualising the randomised distribution\n\nWe can see that the observed test statistic is well within the distribution of the randomised test statistics (which is normally distributed)."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#example-estimating-the-mean",
    "href": "lectures/L08/lecture-08.html#example-estimating-the-mean",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Example: estimating the mean",
    "text": "Example: estimating the mean\n\n\nSuppose we have two samples (groups) and we want to estimate the difference in means, and the 95% confidence interval of the difference.\nWe can use the usual mathematical equation to calculate 95% CI, but if the data does not meet the assumption of normality, then the CI will be a bad estimate.\nInstead, we can use the bootstrap to estimate the 95% CI, which is based on the simulated distribution of the mean difference."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#steps-1",
    "href": "lectures/L08/lecture-08.html#steps-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Steps",
    "text": "Steps\n\n\nResample the data with replacement.\nCalculate the parameter of interest (e.g. mean) for each resample.\nRepeat steps 1 and 2 many times (N) to generate a distribution of the parameter of interest.\nCalculate the 95% confidence interval from the simulated distribution:\n\nThe mean of the distribution is the point estimate.\nThe 0.025 \\times Nth smallest mean is the lower bound of the 95% CI.\nThe 0.975\\times Nth smallest mean is the upper bound of the 95% CI."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#data-1",
    "href": "lectures/L08/lecture-08.html#data-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Data",
    "text": "Data\nThe BOD dataset in R contains the biochemical oxygen demand (mg/L) measurements of 6 samples over time.\n\n\nRows: 6\nColumns: 2\n$ Time   &lt;dbl&gt; 1, 2, 3, 4, 5, 7\n$ demand &lt;dbl&gt; 8.3, 10.3, 19.0, 16.0, 15.6, 19.8"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-1-resample-the-data",
    "href": "lectures/L08/lecture-08.html#step-1-resample-the-data",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 1: Resample the data",
    "text": "Step 1: Resample the data\nFrom the pooled original data:\n\n\n[1]  8.3 10.3 19.0 16.0 15.6 19.8\n\n\nWe sample() with replacement:\n\n\n[1] 15.6  8.3  8.3  8.3 16.0  8.3\n\n\nNoting that some scores will be repeated, and some will be missing."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-2-calculate-the-parameter-of-interest",
    "href": "lectures/L08/lecture-08.html#step-2-calculate-the-parameter-of-interest",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 2: Calculate the parameter of interest",
    "text": "Step 2: Calculate the parameter of interest\nThe parameter of interest is the mean value.\n\n\n[1] 10.8"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-3-repeat-many-times",
    "href": "lectures/L08/lecture-08.html#step-3-repeat-many-times",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 3: Repeat many times",
    "text": "Step 3: Repeat many times\nSince it’s a simple process, we can write a function to calculate the mean:\nThen repeat the function 10,000 times:"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-4-calculate-the-95-ci",
    "href": "lectures/L08/lecture-08.html#step-4-calculate-the-95-ci",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 4: Calculate the 95% CI",
    "text": "Step 4: Calculate the 95% CI\nThe 95% CI is calculated from the simulated distribution:\nPutting it together, the mean is 14.85131 with a 95% CI of [11.25, 18.13].\nHow does this compare to the parametric test?\nIf we use the t.test() function to calculate the 95% CI:\n\n\n\n    One Sample t-test\n\ndata:  BOD$demand\nt = 7.8465, df = 5, p-value = 0.0005397\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  9.973793 19.692874\nsample estimates:\nmean of x \n 14.83333"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#how-different-are-the-results",
    "href": "lectures/L08/lecture-08.html#how-different-are-the-results",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "How different are the results?",
    "text": "How different are the results?\n\n\n\nMethod\nMean\n95% CI\nCI size\n\n\n\n\nBootstrap\n14.85\n[11.25, 18.13]\n6.88\n\n\nParametric\n14.83\n[9.97, 19.69]\n9.72\n\n\n\n\n\nThe point estimates of the mean are almost identical.\nThe 95% CI of the mean are similar, but the bootstrap CI is non-symmetric - it represents the true distribution of the mean.\nThe size of the CI is smaller for the bootstrap method, indicating that the parametric method is overestimating the precision of the estimate."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#the-tidymodels-framework",
    "href": "lectures/L08/lecture-08.html#the-tidymodels-framework",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "The tidymodels framework",
    "text": "The tidymodels framework\n\nIt is clear that bootstrapping and randomisation tests are powerful tools for modern statistics, but they can be cumbersome to implement manually.\nInterestingly, modern data science prefers resampling techniques over traditional methods even when the assumptions are met, because they are more robust and provide more information.\nThe tidymodels framework in R contains the infer package which provides a simple interface to perform most parametric tests using resampling techniques by default!\n\n\n\n\n\n\n\nTLDR\n\n\nThe general trend in modern statistics is to use resampling techniques over traditional methods, even when the assumptions are met – and this is currently led by the tidymodels framework in R."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#using-infer",
    "href": "lectures/L08/lecture-08.html#using-infer",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Using infer",
    "text": "Using infer\nLet’s use the sleep dataset to demonstrate how to use the infer package to perform a randomisation test (also makes it easier to compare against manual method).\nThe infer package requires the user to use an expressive grammar to specify the analysis.\nSteps\n\nspecify() the response variable of interest, then\nhypothesise() the null hypothesis, then\ngenerate() the null distribution, and finally\ncalculate() the p-value."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#two-sample-t-test-using-infer",
    "href": "lectures/L08/lecture-08.html#two-sample-t-test-using-infer",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Two-sample t-test using infer",
    "text": "Two-sample t-test using infer\nFirst we need to calculate the observed test statistic so that we can compare it to the simulated distribution.\n\n\nResponse: extra (numeric)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -1.58\n\n\nThen we generate the null distribution and calculate the p-value:"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#what-are-the-differences",
    "href": "lectures/L08/lecture-08.html#what-are-the-differences",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "What are the differences?",
    "text": "What are the differences?\n\n\n\nMethod\nP-value\n\n\n\n\nManual\n0.082\n\n\ninfer\n0.084\n\n\nt.test()\n0.079\n\n\n\nAs we can see, the results are very similar because the assumptions of the parametric test were already met.\n\n\n\n\n\n\nNote\n\n\nTo calculate confidence intervals, use the get_ci() function as documented here."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#example-beetles",
    "href": "lectures/L08/lecture-08.html#example-beetles",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Example: beetles",
    "text": "Example: beetles\nThe beetle dataset was used in last week’s lecture to demonstrate the non-parametric Wilcoxon rank-sum test.\n\n\nRows: 45\nColumns: 2\n$ SIZE    &lt;chr&gt; \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\",…\n$ BEETLES &lt;dbl&gt; 256, 209, 0, 0, 0, 44, 49, 117, 6, 0, 0, 75, 34, 13, 0, 90, 0,…"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#assumption",
    "href": "lectures/L08/lecture-08.html#assumption",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Assumption",
    "text": "Assumption\nRecall that the data does not meet the assumptions of normality:"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#t-test-via-resampling-using-infer",
    "href": "lectures/L08/lecture-08.html#t-test-via-resampling-using-infer",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "T-test via resampling using infer",
    "text": "T-test via resampling using infer\nFirst, calculate the test statistic:\nThen generate the null distribution and calculate the p-value:\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.025"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#comparisons",
    "href": "lectures/L08/lecture-08.html#comparisons",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Comparisons",
    "text": "Comparisons\nLet’s compare the p-values from\n\na t-test (if we ignore violations of assumptions),\nthe wilcoxon rank-sum test, and\nthe infer randomisation test."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#comparisons-1",
    "href": "lectures/L08/lecture-08.html#comparisons-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Comparisons",
    "text": "Comparisons\nT-test\nWilcoxon rank-sum test\nRandomisation test"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#results",
    "href": "lectures/L08/lecture-08.html#results",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Results",
    "text": "Results\n\n\n\nMethod\nP-value\n\n\n\n\nT-test\n0.037\n\n\nWilcoxon\n0.075\n\n\ninfer\n0.025\n\n\n\n\nAs we can see, the p-values are quite different because the assumptions of the parametric test were violated.\nThe randomisation test is more robust and provides a more accurate estimate of the p-value than the Wilcoxon rank-sum test.\n\nHow to report results of randomisation test\nThe results of the randomisation test can be reported as follows:\n\nBeetle consumption was significantly different between small and large beetles (t = 2.19, R = 10000, p = 0.025)."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#recap-we-have-one-sample",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#recap-we-have-one-sample",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Recap: we have one sample",
    "text": "Recap: we have one sample\n\nOne-sample t-test: compare the sample of data to a fixed value of interest (e.g. a hypothesised value, or a population mean).\n\n\n\nExamples\n\nIs the mean height of students in this class different from the population mean of 170 cm?\nIs the food weight in a can of tuna different from its advertised weight of 250 g?"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#what-if-we-want-to-compare-a-sample-of-data-to-another-sample",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#what-if-we-want-to-compare-a-sample-of-data-to-another-sample",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "What if we want to compare a sample of data to another sample?",
    "text": "What if we want to compare a sample of data to another sample?\n\n\nExamples\n\nIs the mean height of students in this class different from the other class?\nIs the mean food weight in this batch of tuna different from the other batch?"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#comparing-two-samples-visualisation",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#comparing-two-samples-visualisation",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Comparing two samples: visualisation",
    "text": "Comparing two samples: visualisation"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#some-considerations-the-boxplot",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#some-considerations-the-boxplot",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Some considerations: the boxplot",
    "text": "Some considerations: the boxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-off between being able to see the distribution of the data and being able to compare between groups.\nThe recommended approach when comparing two or more groups of data in most cases."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#data",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#data",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Data",
    "text": "Data\nA simulated example (data is not real):\n\nExperimental design: two groups of students selected at random, without replacement, from the ENVX1002 cohort.\n\nredbull group: students who consumed 250 ml of Red Bull.\ncontrol group: students who consumed 250 ml of water (control group).\n\nHeart rate in beats per minute (bpm) was measured 20 minutes after consumption.\n\n\n\nStructure of data\n\n\n'data.frame':   24 obs. of  2 variables:\n $ group     : chr  \"redbull\" \"redbull\" \"redbull\" \"redbull\" ...\n $ heart_rate: num  72 88 72 88 76 75 84 80 60 96 ..."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#hypothesis",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#hypothesis",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor a two-sample t-test, the null hypothesis is that the means of the two groups are equal, and the alternative hypothesis is that the means are different.\nH_0: \\mu_{\\text{redbull}} = \\mu_{\\text{control}} H_1: \\mu_{\\text{redbull}} \\neq \\mu_{\\text{control}}\n\nCompare this to the one-sample t-test, where the null hypothesis is that the sample mean is equal to a fixed value: H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#assumptions",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#assumptions",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Assumptions",
    "text": "Assumptions\nThe assumptions of the two-sample t-test include:\n\nNormality: the data are normally distributed.\nHomogeneity of variance: the variances of the two groups are equal.\n\n\n\nWhy are these assumptions important?\n\n\nSince the t-test compares the means of two groups, normality ensures that the means are the best estimate of the population means.\nEqual variances indicates that the two groups have similar “noise” influencing their means, except for the “treatment” effect.\n\nIn the Red Bull example, this means that the range of heart rate values in students for both groups is similar, except for the effect of consuming Red Bull."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#normality-histogram",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#normality-histogram",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Normality: histogram",
    "text": "Normality: histogram\n\nWe visually inspect the distribution of the data using histograms, generally for each group.\nLook out for: symmetry, skewness, and multimodality.\n\n\nR base graphicsggplot2ggpubr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The data appear to be normally distributed, but it is better to confirm this with a QQ-plot."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#normality-qq-plot",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#normality-qq-plot",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Normality: QQ-plot",
    "text": "Normality: QQ-plot\n\nThe qq-plot is a graphical method to specifically assess the normality of the data. Again, we look at the data for each group.\nLook out for: deviations from the straight line.\n\n\nR base graphicsggplot2ggpubr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The data appear to be normally distributed."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#normality-formal-test",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#normality-formal-test",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Normality: formal test",
    "text": "Normality: formal test\n\nUse the Shapiro-Wilk test which tests the null hypothesis that the data are normally distributed.\nThis test is sensitive to deviations from normality in the tails of the distribution, and is suitable for small sample sizes (about 5 to 50 observations).\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  redbull$heart_rate[redbull$group == \"redbull\"]\nW = 0.97459, p-value = 0.9524\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  redbull$heart_rate[redbull$group == \"control\"]\nW = 0.93733, p-value = 0.4643\n\n\n\nConclusion: p-values are greater than 0.05, so we do not reject the null hypothesis of normality. The data are normally distributed."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#what-if-the-normality-assumption-is-violated",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#what-if-the-normality-assumption-is-violated",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "What if the normality assumption is violated?",
    "text": "What if the normality assumption is violated?\n\nThe t-test is robust to deviations from normality, especially for large sample sizes due to the Central Limit Theorem.\nIf the sample size is small, consider using a non-parametric test (e.g. the Wilcoxon rank-sum test): next week\nAlternatively, transform the data: tomorrow"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#equal-variances-boxplot",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#equal-variances-boxplot",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Equal variances: boxplot",
    "text": "Equal variances: boxplot\n\nWe visually inspect the spread of the data using boxplots, generally for each group.\nLook out for: differences in spread, outliers, and symmetry.\n\n\nR base graphicsggplot2ggpubr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The spread of the data appears to be similar between the two groups."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#equal-variances-formal-tests",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#equal-variances-formal-tests",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Equal variances: formal tests",
    "text": "Equal variances: formal tests\n\nBartlett’s and Levene’s tests may be used to test the null hypothesis that the variances of the groups are equal.\nThese tests are sensitive to deviations from normality (Levene’s test is less so compared to Bartlett’s), and are suitable for small sample sizes.\n\n\nLevene’s testBartlett’s test\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   0.289 0.5962\n      22               \n\n\n\n\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  heart_rate by group\nBartlett's K-squared = 0.075121, df = 1, p-value = 0.784\n\n\n\n\n\n\nConclusion: p-values are greater than 0.05, so we do not reject the null hypothesis of equal variances. The variances of the two groups are equal."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#what-if-the-equal-variance-assumption-is-violated",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#what-if-the-equal-variance-assumption-is-violated",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "What if the equal variance assumption is violated?",
    "text": "What if the equal variance assumption is violated?\nSome debate exists on what to do, but choices include:\n\nUse the Welch’s t-test, which is robust to unequal variances: coming up next\nTransform the data to stabilise the variances: tomorrow\nPerform a non-parametric test: next week"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#the-welchs-t-test",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#the-welchs-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "The Welch’s t-test",
    "text": "The Welch’s t-test\n\nThe Welch’s t-test is a modification of the two-sample t-test that does not assume equal variances.\nAlso applicable when the sample sizes are unequal.\n\nWhy not use the Welch’s t-test all the time?\n\nOngoing debate on whether to use the Welch’s t-test or the Student’s t-test when the variances are equal.\nThe Welch’s t-test is generally considered more robust, and is the default in R’s t.test() function.\nYou can still use the Student’s t-test by setting var.equal = TRUE in the t.test() function."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#are-the-assumptions-of-normality-and-homogeneity-of-variance-met",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#are-the-assumptions-of-normality-and-homogeneity-of-variance-met",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Are the assumptions of normality and homogeneity of variance met?",
    "text": "Are the assumptions of normality and homogeneity of variance met?\nWhen reporting in journals, it is common to simply state that the assumptions were met and what tests were used to confirm them, without showing the exact results of the tests!\n\n\nExample 1\n\nThe assumptions of normality and homogeneity of variance were met for the data (Shapiro-Wilk test, p &gt; 0.05; Levene’s test, p &gt; 0.05). Thus, we performed a two-sample t-test…\n\n\n\n\nExample 2\n\nVisual inspection of the histograms, QQ-plots, and boxplots showed that the data met the assumptions of both normality and homogeneity of variance. Thus, we performed a two-sample t-test..\n\n\n\nFor your lab reports, you should show the results of the tests (because we want to check your work!)"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#performing-the-t-test",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#performing-the-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Performing the t-test",
    "text": "Performing the t-test\n\n\n\n    Two Sample t-test\n\ndata:  heart_rate by group\nt = -1.1365, df = 22, p-value = 0.268\nalternative hypothesis: true difference in means between group control and group redbull is not equal to 0\n95 percent confidence interval:\n -12.947117   3.780451\nsample estimates:\nmean in group control mean in group redbull \n             75.00000              79.58333 \n\n\nResults indicate that the means of the two groups are not significantly different (p = 0.27).\n\n\nCompare with the Welch’s t-test\n\n\n\n    Welch Two Sample t-test\n\ndata:  heart_rate by group\nt = -1.1365, df = 21.845, p-value = 0.2681\nalternative hypothesis: true difference in means between group control and group redbull is not equal to 0\n95 percent confidence interval:\n -12.950568   3.783902\nsample estimates:\nmean in group control mean in group redbull \n             75.00000              79.58333"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#conclusion",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#conclusion",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Conclusion",
    "text": "Conclusion\nDifferences in heart rate we not statistically significant between the Red Bull and control groups (t22 = -1.1, p = 0.27) indicating that Red Bull did not significantly increase the heart rate of students sampled from ENVX1002."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#are-the-two-sample-independent",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#are-the-two-sample-independent",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Are the two sample independent?",
    "text": "Are the two sample independent?\nWhen testing if two samples are different from each other, we need to consider two possible scenarios:\n\n\nIndependent samples: The samples are drawn from two different populations, or the samples are not related to each other – independent groups.\nRelated samples: The samples are drawn from the same population, and/or the samples are related to each other – repeated measures or matched pairs.\n\n\n\nIf the samples are related, a paired t-test is more appropriate than a two-sample t-test as it accounts for the relationship between the samples that could confound the results."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#paired-t-test",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#paired-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Paired t-test",
    "text": "Paired t-test\nExperimental design (what if?)\nBefore\n\nTwo groups of students selected at random, without replacement, from the ENVX1002 cohort.\n\n\n\nPaired design\nThe same student was used in a before/after experiment, where the heart rate was measured before and after consuming 250ml of Red Bull. Twelve (12) students were selected at random from the ENVX1002 cohort.\n\n\nData is no longer independent, as the same student is measured twice.\nThe student now confounds the results, as the heart rate of the same student is likely to be correlated even without consuming Red Bull.\nTotal number of students is now 12, not 24.\nLet’s assume the data collected are exactly the same."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#hypothesis-1",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#hypothesis-1",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor a paired t-test, the null hypothesis is that the mean difference between the two groups is zero, and the alternative hypothesis is that the mean difference is different from zero.\nH_0: \\mu_{\\text{diff}} = 0 H_1: \\mu_{\\text{diff}} \\neq 0\n\nCompare this to the two-sample t-test, where the null hypothesis is that the means of the two groups are equal: H_0: \\mu_{\\text{redbull}} = \\mu_{\\text{control}} H_1: \\mu_{\\text{redbull}} \\neq \\mu_{\\text{control}}"
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#assumptions-of-the-paired-t-test",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#assumptions-of-the-paired-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Assumptions of the paired t-test",
    "text": "Assumptions of the paired t-test\n\nThe assumption of the paired t-test is that the differences between the two groups are normally distributed.\nThere is no assumption of equal variances, as the paired t-test is a one-sample t-test on the differences.\n\nAnother way to think about it is that since the data are paired, the variance of the differences is the same for both groups."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#performing-the-paired-t-test",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#performing-the-paired-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "Performing the paired t-test",
    "text": "Performing the paired t-test\nThere are two ways.\n\n\n\n\nMethod 1: Calculate the differences, then perform a one-sample t-test using t.test()\n\n\n\n    One Sample t-test\n\ndata:  diff\nt = 1.6578, df = 11, p-value = 0.1256\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.501628 10.668294\nsample estimates:\nmean of x \n 4.583333 \n\n\n\n\n\n\nMethod 2: Use the t.test() function with the paired = TRUE argument\n\n\n\nThe results for both methods are identical; the mean difference is not significantly different from zero (p = 0.13)."
  },
  {
    "objectID": "lectures/L06/lecture-06a-2s-ttest-1.html#references",
    "href": "lectures/L06/lecture-06a-2s-ttest-1.html#references",
    "title": "Topic 6 – Two-sample t-tests – Part I",
    "section": "References",
    "text": "References\n\nQuinn G. P. & Keough M. J. (2002) Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK.\nLogan, M. (2010). Biostatistical design and analysis using R a practical guide. Hoboken, N.J., Wiley-Blackwell."
  },
  {
    "objectID": "lectures/L06/index.html",
    "href": "lectures/L06/index.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Lecture 06a\nFull Screen | PDF\n\n\n\nLecture 06b\nFull Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L06 -- Two-sample *t*-tests"
    ]
  },
  {
    "objectID": "lectures/L01/index.html",
    "href": "lectures/L01/index.html",
    "title": "Lecture 01 – Introduction",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L01 -- Introduction"
    ]
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#overview",
    "href": "lectures/L07/lecture-07a-non-parametric.html#overview",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Overview",
    "text": "Overview\nParametric methods\nDepends on the assumption that the data is normally distributed with mean \\mu and standard deviation \\sigma ,e.g. t-test, ANOVA, linear regression.\n\n\nNon-parametric methods\nDo not make any assumptions about the distribution of the data.\nUses other properties e.g. ranking of the data, e.g. Wilcoxon signed-rank test, Mann-Whitney U test, Kruskal-Wallis test."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#rank-based-tests",
    "href": "lectures/L07/lecture-07a-non-parametric.html#rank-based-tests",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Rank-based tests",
    "text": "Rank-based tests\nGeneral idea\n\nRank the data e.g., from smallest to largest.\nReplace the data with their ranks.\nPerform the test on the ranks."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#its-kind-of-like-a-transformation",
    "href": "lectures/L07/lecture-07a-non-parametric.html#its-kind-of-like-a-transformation",
    "title": "Topic 7 – Non-parametric tests",
    "section": "It’s kind of like a transformation…",
    "text": "It’s kind of like a transformation…\n\nFor the Wilcoxon signed-rank test suppose we have the following data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample:\n12\n10\n8\n6\n4\n10\n8\n6\n10\n\n\n\n\n\n\n\nWe arrange the data in ascending order (similar values are given the same colour for illustration):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nordered:\n4\n6\n6\n8\n8\n10\n10\n10\n12\n\n\n\n\n\n\n\nThen, we rank the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nordered ranks:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\n\n\nFinally, ranks that are tied are given the average rank:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfinal rank:\n1\n2.5\n2.5\n4.5\n4.5\n7\n7\n7\n9\n\n\n\n\n\nThese ranks are then used to perform the test, instead of the original data."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#two-sample-t-test",
    "href": "lectures/L07/lecture-07a-non-parametric.html#two-sample-t-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Two-sample t-test",
    "text": "Two-sample t-test\nConsider two sets of identical data that compares between a group A and B, where one contains an outlier.\n\n\nData:\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\n7\n\n\n2\n8\n\n\n3\n9\n\n\n4\n10\n\n\n5\n11\n\n\n6\n12\n\n\n7\n13\n\n\n8\n14\n\n\n9\n15\n\n\n10\n16\n\n\n\n\n\n\n\n\nData with outlier:\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\n7\n\n\n2\n8\n\n\n3\n9\n\n\n4\n10\n\n\n5\n11\n\n\n6\n12\n\n\n7\n13\n\n\n8\n14\n\n\n9\n15\n\n\n10\n200"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#should-there-be-a-difference",
    "href": "lectures/L07/lecture-07a-non-parametric.html#should-there-be-a-difference",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Should there be a difference?",
    "text": "Should there be a difference?\nWithout the outlier, the data would have been normally distributed."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#outlier",
    "href": "lectures/L07/lecture-07a-non-parametric.html#outlier",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Outlier",
    "text": "Outlier\nThe same data, but with a single outlier in group B:"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#analysis",
    "href": "lectures/L07/lecture-07a-non-parametric.html#analysis",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Analysis",
    "text": "Analysis\nIf we perform t-tests on both data sets, we get the following results:\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by group\nt = -4.4313, df = 18, p-value = 0.0003224\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -8.844662 -3.155338\nsample estimates:\nmean in group A mean in group B \n            5.5            11.5 \n\n\nResults indicate that there is a statistically significant difference between the two groups (t18 = -4.4, p &lt; 0.05).\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by group\nt = -1.2882, df = 9.0461, p-value = 0.2297\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -67.21615  18.41615\nsample estimates:\nmean in group A mean in group B \n            5.5            29.9 \n\n\nResults indicate that the two groups are not significantly different (t18 = -2.1, p = 0.23).\n\n\nThe real difference between the two groups is obscured by the outlier. Type II error (false negative)?"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#when-to-use",
    "href": "lectures/L07/lecture-07a-non-parametric.html#when-to-use",
    "title": "Topic 7 – Non-parametric tests",
    "section": "When to use",
    "text": "When to use\n\nIf assumptions are met (normality, homogeneity of variance), use parametric tests as they are more powerful and efficient than non-parametric tests.\nIf the normality assumption is violated, transform the data and check for normality again (optional).\nNon-parametric tests are a good way to deal with circumstances in which parametric tests perform “poorly”."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#what-to-use",
    "href": "lectures/L07/lecture-07a-non-parametric.html#what-to-use",
    "title": "Topic 7 – Non-parametric tests",
    "section": "What to use",
    "text": "What to use\n\n\n\n\n\n\n\n\nParametric tests\nNon-parametric counterpart\n\n\n\n\nOne-sample t-test\nWilcoxon signed-rank test\n\n\nTwo-sample t-test\nMann-Whitney U test\n\n\nANOVA\nKruskal-Wallis test\n\n\nPearson's correlation\nSpearman's rank correlation\n\n\n\n\n\n\n\nAll of the non-parametric techniques above convert the data into ranks before performing the test.\n\n\n\n\n\n\n\nNote\n\n\nWe will focus on the Wilcoxon signed-rank test and the Mann-Whitney U test."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#overview-1",
    "href": "lectures/L07/lecture-07a-non-parametric.html#overview-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Overview",
    "text": "Overview\nThe Wilcoxon signed-rank test is a non-parametric test used to compare two related samples, matched pairs, or repeated measures on a single sample.\n\nIs an alternative to:\n\nOne-sample t-test\nPaired t-test"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#assumptions",
    "href": "lectures/L07/lecture-07a-non-parametric.html#assumptions",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nData comes from the same population\nData are randomly and independently sampled\n\n\n\nBasically, used in same situations as the one-sample or paired t-test, but when the data is not normally distributed but still symmetric."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#calculating-ranks",
    "href": "lectures/L07/lecture-07a-non-parametric.html#calculating-ranks",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Calculating ranks",
    "text": "Calculating ranks\nIf comparing two groups, the ranks are calculated as follows:\n\n\nCalculate the difference D between the two groups.\nRank the absolute values of the differences in ascending order.\nAssign the sign of the difference to the rank.\nSum the ranks for each group – zero differences are ignored.\n\n\n\n\n\n\n\n\n\nNote\n\n\nSee Slide 5 to recall how ranks are calculated, but we will show another example in the next slide."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#weight-gain",
    "href": "lectures/L07/lecture-07a-non-parametric.html#weight-gain",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Weight gain",
    "text": "Weight gain\nWe measured weight gain in chickens before and after a diet.\n\n\n\n\n\n\n\n\nchicken\nweight\nweight_after\n\n\n\n\n1\n2.5\n4.0\n\n\n2\n3.5\n5.0\n\n\n3\n3.5\n5.0\n\n\n4\n3.4\n4.6\n\n\n\n\n\n\n\nIs there a significant increase in weight gain after the diet?"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#rank-values",
    "href": "lectures/L07/lecture-07a-non-parametric.html#rank-values",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Rank values",
    "text": "Rank values\n\n\n\n\n\n\n\n\nchicken\nweight\nweight_after\nD\nSign\nrank\nSigned rank\n\n\n\n\n1\n2.5\n4.0\n1.5\n+\n3\n3\n\n\n2\n3.5\n5.0\n1.5\n+\n3\n3\n\n\n3\n3.5\n5.0\n1.5\n+\n3\n3\n\n\n4\n3.4\n4.6\n1.2\n+\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe order of the ranks is based on the absolute values of the differences; the signs are assigned afterward."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#hypothesis",
    "href": "lectures/L07/lecture-07a-non-parametric.html#hypothesis",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nIs there a significant increase in weight gain after the diet?\n\nH_0: \\mu_{\\text{before}} = \\mu_{\\text{after}} H_1: \\mu_{\\text{before}} &lt; \\mu_{\\text{after}}\n\nIn words:\n\nH_0: There is no difference in weight gain before and after the diet.\nH_1: There is an increase in weight gain after the diet."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#assumptions-1",
    "href": "lectures/L07/lecture-07a-non-parametric.html#assumptions-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\nWith so few data points, we may want to use a formal test to check for normality."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#assumptions-2",
    "href": "lectures/L07/lecture-07a-non-parametric.html#assumptions-2",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$weight_after - df$weight\nW = 0.62978, p-value = 0.001241\n\n\nResults indicate that the data significantly deviates from normality (W = 0.63, p &lt; 0.05). We will use the Wilcoxon signed-rank test."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#performing-the-test",
    "href": "lectures/L07/lecture-07a-non-parametric.html#performing-the-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Performing the test",
    "text": "Performing the test\nIn R\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  df$weight_after and df$weight\nV = 10, p-value = 0.04449\nalternative hypothesis: true location shift is greater than 0\n\n\nwhere V is the sum of the signed ranks.\n\nThe results indicate that there is a significant increase in weight gain after the diet (V = 10, p &lt; 0.05)."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#beetle-consumption-in-lizards",
    "href": "lectures/L07/lecture-07a-non-parametric.html#beetle-consumption-in-lizards",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Beetle consumption in lizards",
    "text": "Beetle consumption in lizards\nResearchers investigated differences in beetle consumption between two size classes of eastern horned lizard (Phrynosoma douglassi brevirostre)\n\nLarger class: adult females.\nSmaller class: adult males, yearling females.\n\n\nFocusing on just the smaller size class (for now) – it was hypothesised that this size class would eat a minimum of 100 beetles per day."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#hypothesis-1",
    "href": "lectures/L07/lecture-07a-non-parametric.html#hypothesis-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypothesis",
    "text": "Hypothesis\nDoes the average smaller size class lizard eat about 100 beetles per day?\nH_0: \\mu = 100 H_1: \\mu \\neq 100\nDataset Download\n\n\nRows: 45\nColumns: 2\n$ SIZE    &lt;chr&gt; \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\",…\n$ BEETLES &lt;dbl&gt; 256, 209, 0, 0, 0, 44, 49, 117, 6, 0, 0, 75, 34, 13, 0, 90, 0,…"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#first-check-assumptions",
    "href": "lectures/L07/lecture-07a-non-parametric.html#first-check-assumptions",
    "title": "Topic 7 – Non-parametric tests",
    "section": "First, check assumptions",
    "text": "First, check assumptions\n\nIs it normally distributed?"
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#run-the-test",
    "href": "lectures/L07/lecture-07a-non-parametric.html#run-the-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Run the test",
    "text": "Run the test\nThe Wilcoxon signed-rank test for one sample can be performed as follows:\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  .\nV = 92, p-value = 0.09755\nalternative hypothesis: true location is not equal to 100\n\n\n\nResults indicate that the average number of beetles consumed by the smaller size class lizard is not significantly different from 100 (V = 92, p = 0.1).\n\n\n\n\n\n\n\n\nImportant\n\n\nWe are unable to make a conclusion about effect size from non-parametric tests as the information is lost when the data is transformed into ranks."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#about",
    "href": "lectures/L07/lecture-07a-non-parametric.html#about",
    "title": "Topic 7 – Non-parametric tests",
    "section": "About",
    "text": "About\n\nA non-parametric test used to compare two independent samples similar to the two-sample t-test.\nLike the Wilcoxon signed-rank test, it uses ranks to perform the test and does not assume normality.\nIt is also more relaxed in that it does not assume symmetry in the distribution of the data – instead, it assumes that the two groups have the same shape/distribution."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#beetle-consumption-in-lizards-1",
    "href": "lectures/L07/lecture-07a-non-parametric.html#beetle-consumption-in-lizards-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Beetle consumption in lizards",
    "text": "Beetle consumption in lizards\nResearchers investigated differences in beetle consumption between two size classes of eastern horned lizard (Phrynosoma douglassi brevirostre)\n\nLarger class: adult females.\nSmaller class: adult males, yearling females.\n\nWe will now compare the number of beetles consumed by the larger and smaller size classes of lizards."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#hypotheses",
    "href": "lectures/L07/lecture-07a-non-parametric.html#hypotheses",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nAre the number of beetles consumed by the larger and smaller size classes of lizards different?\n\nLoosely speaking, because we are not assuming symmetry, the most appropriate summary statistic to use when comparing the two groups is the median.\nH_0: median_{\\text{larger}} = median_{\\text{smaller}} H_1: median_{\\text{larger}} \\neq median_{\\text{smaller}}\nMore accurately, we are testing for a difference in the distribution of the two groups."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#assumptions-3",
    "href": "lectures/L07/lecture-07a-non-parametric.html#assumptions-3",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\nData does not meet the normality assumption."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#test-statistic",
    "href": "lectures/L07/lecture-07a-non-parametric.html#test-statistic",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Test statistic",
    "text": "Test statistic\nThe same function wilcox.test() can be used to perform the Mann-Whitney U test.\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  BEETLES by SIZE\nW = 329, p-value = 0.07494\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nW is the sum of the ranks of the smaller group.\nThe “true location shift” is the median of the larger group minus the median of the smaller group.\n\n\nThe results indicate that the number of beetles consumed by the larger and smaller size classes of lizards is not significantly different (W = 329, p = 0.07)."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#transform-or-non-parametric",
    "href": "lectures/L07/lecture-07a-non-parametric.html#transform-or-non-parametric",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Transform, or non-parametric?",
    "text": "Transform, or non-parametric?\n\n\nAs usual, there is ongoing debate on whether to transform the data or use non-parametric tests, but the general consensus is to always prefer parametric tests and transformations when assumptions are met using those techniques.\n\ne.g. Parametric analysis of transformed data is more powerful than non-parametric analysis\n\nSome argue that non-parametric tests must be decided during experimental design and not when the data fails to meet the normality assumption: as the decision to rank data has implications on the interpretation of the results.\n\ne.g. Graphpad Advice: Don’t automate the decision to use a nonparametric test\n\nThe conventional wisdom is to transform the data and check for normality if the assumption is not met. If the data is still not normal, then use non-parametric tests (after considering the implications on interpretation).\n\nOr, use bootstrapping (next week!)."
  },
  {
    "objectID": "lectures/L07/lecture-07a-non-parametric.html#summary",
    "href": "lectures/L07/lecture-07a-non-parametric.html#summary",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Summary",
    "text": "Summary\n\nWilcoxon signed-rank test: alternative to the one-sample t-test and paired t-test.\nMann-Whitney U test: alternative to the two-sample t-test.\nAdvantages: Robust to outliers, skewness, and non-normality.\nDrawbacks: Less powerful than parametric tests when assumptions are met, provide no insight into the size of the effect."
  },
  {
    "objectID": "lectures/L09/index.html",
    "href": "lectures/L09/index.html",
    "title": "Lecture 09",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L09 -- Describing relationships"
    ]
  },
  {
    "objectID": "lectures/L12/lecture-12.html#regressions",
    "href": "lectures/L12/lecture-12.html#regressions",
    "title": "Topic 12 – Non-linear regression",
    "section": "Regressions",
    "text": "Regressions\nSimple linear regression\n Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \nIdeal for predicting a continuous response variable from a single predictor variable: “How does y change as x changes, when the relationship is linear?”\nMultiple linear regression\n Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + \\epsilon_i \n“How does y change as x_1, x_2, …, x_k change?”\n\n\nNonlinear regression\n Y_i = f(x_i, \\beta) + \\epsilon_i \n\n\nwhere f(x_i, \\beta) is a nonlinear function of the parameters \\beta: “How do we model a change in y with x when the relationship is nonlinear?”"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-a-nonlinear-model",
    "href": "lectures/L12/lecture-12.html#fitting-a-nonlinear-model",
    "title": "Topic 12 – Non-linear regression",
    "section": "Fitting a nonlinear model",
    "text": "Fitting a nonlinear model\nLinear relationships are simple to interpret since the rate of change is constant.\n\n“As one changes, the other changes at a constant rate.”\n\nNonlinear relationships often involve exponential, logarithmic, or power functions.\n\n“As one changes, the other changes at a rate that is not proportional to the change in the other.”"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-relationships-exponents",
    "href": "lectures/L12/lecture-12.html#nonlinear-relationships-exponents",
    "title": "Topic 12 – Non-linear regression",
    "section": "Nonlinear relationships: exponents",
    "text": "Nonlinear relationships: exponents\n\nx^2 is the square of x.\nx^3 is the cube of x.\nx^a is x raised to the power of a.\n\n\nIn a relationship where y is a function of x^a, as y increases, x increases at a rate that is equal to x to the power of a."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-relationships-logarithms",
    "href": "lectures/L12/lecture-12.html#nonlinear-relationships-logarithms",
    "title": "Topic 12 – Non-linear regression",
    "section": "Nonlinear relationships: logarithms",
    "text": "Nonlinear relationships: logarithms\n\nlog_e(x) is the natural logarithm of x.\nlog_{10}(x) is the common logarithm of x.\nlog_a(x) is the logarithm of x to the base a.\n\nInterpretation:\n\nIf \\log_a(y) = x: as x increases, y increases at a rate of y = a^x.\nIf y = \\log_a(x): as y increases, x also increases, at x = a^y."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#exponents-and-logarithms",
    "href": "lectures/L12/lecture-12.html#exponents-and-logarithms",
    "title": "Topic 12 – Non-linear regression",
    "section": "Exponents and logarithms",
    "text": "Exponents and logarithms\n\n\n\n\n\n\n\n\n\nExponents\nLogarithms\n\n\n\n\nDefinition\nIf a^n = b, a is the base, n is the exponent, and b is the result.\nIf \\log_a b = n, a is the base, b is the result, and n is the logarithm (or the exponent in the equivalent exponential form).\n\n\nExample\n2^3 = 8\n\\log_2 8 = 3\n\n\nInterpretation\n2 raised to the power of 3 equals 8.\nThe power to which you must raise 2 to get 8 is 3.\n\n\nInverse\nThe logarithm is the inverse operation of exponentiation.\nThe exponentiation is the inverse operation of logarithm.\n\n\nProperties\n(a^n)^m = a^{n \\cdot m}, a^n \\cdot a^m = a^{n+m}, \\frac{a^n}{a^m} = a^{n-m}\n\\log_a(b \\cdot c) = \\log_a b + \\log_a c, \\log_a\\left(\\frac{b}{c}\\right) = \\log_a b - \\log_a c, \\log_a(b^n) = n \\cdot \\log_a b"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#dealing-with-nonlinearity",
    "href": "lectures/L12/lecture-12.html#dealing-with-nonlinearity",
    "title": "Topic 12 – Non-linear regression",
    "section": "Dealing with nonlinearity",
    "text": "Dealing with nonlinearity\nTransformations\nOften, a nonlinear relationship may be transformed into a linear relationship by applying a transformation to the response variable or the predictor variable(s).\n\nLogarithmic: y = \\log(x)\nExponential: y = e^x\nSquare-root: y = \\sqrt{x}\nInverse: y = \\frac{1}{x}\n\n\n\nAll good when y changes monotically with x.\nWhat if relationship is not monotonic, or is more complex?"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#exponential-decay-relationship",
    "href": "lectures/L12/lecture-12.html#exponential-decay-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Exponential decay relationship",
    "text": "Exponential decay relationship\nResponse variable decreases and approaches limit as predictor variable increases.\n y = a \\cdot e^{-b_x} \n\nExamples: radioactive decay, population decline, chemical reactions."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#asymptotic-relationship",
    "href": "lectures/L12/lecture-12.html#asymptotic-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Asymptotic relationship",
    "text": "Asymptotic relationship\nResponse variable increases and approaches a limit as the predictor variable increases.\n y = a + b(1 - e^{-cx}) \n\nExamples: population growth, enzyme kinetics."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#logistic-relationship",
    "href": "lectures/L12/lecture-12.html#logistic-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Logistic relationship",
    "text": "Logistic relationship\nAn S-shaped relationship, where the response variable is at first exponential, then asymptotic.\n y = c + \\frac{d-c}{1+e^{-b(x-a)}} \n\nExamples: growth of bacteria, disease spread, species growth."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#curvilinear-relationship",
    "href": "lectures/L12/lecture-12.html#curvilinear-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Curvilinear relationship",
    "text": "Curvilinear relationship\nResponse variable changes in a variety of ways as the predictor variable changes.\n y = a + bx + cx^2 + dx^3 + ... \n\nExamples: food intake, drug dosage, exercise."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-exponential-decay",
    "href": "lectures/L12/lecture-12.html#transformations-exponential-decay",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Exponential decay",
    "text": "Transformations: Exponential decay\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-exponential-decay-1",
    "href": "lectures/L12/lecture-12.html#transformations-exponential-decay-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Exponential decay",
    "text": "Transformations: Exponential decay\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship",
    "href": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Asymptotic relationship",
    "text": "Transformations: Asymptotic relationship\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship-1",
    "href": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Asymptotic relationship",
    "text": "Transformations: Asymptotic relationship\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-logistic-relationship",
    "href": "lectures/L12/lecture-12.html#transformations-logistic-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Logistic relationship",
    "text": "Transformations: Logistic relationship\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-logistic-relationship-1",
    "href": "lectures/L12/lecture-12.html#transformations-logistic-relationship-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Logistic relationship",
    "text": "Transformations: Logistic relationship\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-curvilinear-relationship",
    "href": "lectures/L12/lecture-12.html#transformations-curvilinear-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Curvilinear relationship",
    "text": "Transformations: Curvilinear relationship\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-curvilinear-relationship-1",
    "href": "lectures/L12/lecture-12.html#transformations-curvilinear-relationship-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Transformations: Curvilinear relationship",
    "text": "Transformations: Curvilinear relationship\n\n\nBefore transformation\n\n\n\n\n\n\n\n\n\n\nAfter loge transform"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#did-the-transformations-work",
    "href": "lectures/L12/lecture-12.html#did-the-transformations-work",
    "title": "Topic 12 – Non-linear regression",
    "section": "Did the transformations work?",
    "text": "Did the transformations work?\n\nTo a certain extent…\nProblems:\n\nRelationships typically do not meet the linear assumption, but seem “ok” for other assumptions.\nPoor fit to the data (over or underfitting in some areas).\nDifficult to interpret the results."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-regression-2",
    "href": "lectures/L12/lecture-12.html#nonlinear-regression-2",
    "title": "Topic 12 – Non-linear regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nA way to model complex (nonlinear) relationships.\n\ni.e. phenomena that arise in the natural and physical sciences e.g. biology, chemistry, physics, engineering.\n\nAt least one predictor is not linearly related to the response variable."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#wait",
    "href": "lectures/L12/lecture-12.html#wait",
    "title": "Topic 12 – Non-linear regression",
    "section": "Wait!",
    "text": "Wait!\n\n\nIt’s easier than you think in R.\nPolynomial regression: still linear in the parameters and a good place to start.\nNonlinear regression: use the nls() function to fit the following nonlinear models:\n\nExponential growth\nExponential decay\nLogistic"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#model",
    "href": "lectures/L12/lecture-12.html#model",
    "title": "Topic 12 – Non-linear regression",
    "section": "Model",
    "text": "Model\n Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i \nwhere k is the degree of the polynomial.\n\nThe model is still linear in the parameters \\beta and can be fitted using least squares.\nInstead of multiple predictors, we have multiple terms of the same predictor.\nCan still be fit using lm().\n\n\n\nAdding polynomial terms\n\nLinear: y = \\beta_0 + \\beta_1 x\nQuadratic: y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\nCubic: y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\nEach level increases the power of the predictor by 1."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#the-data",
    "href": "lectures/L12/lecture-12.html#the-data",
    "title": "Topic 12 – Non-linear regression",
    "section": "The data",
    "text": "The data\nSee Slide 11 for the relationship and mathematical expression."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-linear",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-linear",
    "title": "Topic 12 – Non-linear regression",
    "section": "Fitting the model (linear)",
    "text": "Fitting the model (linear)\n Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-poly-order-2",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-poly-order-2",
    "title": "Topic 12 – Non-linear regression",
    "section": "Fitting the model (poly order 2)",
    "text": "Fitting the model (poly order 2)\n Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-poly-order-3",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-poly-order-3",
    "title": "Topic 12 – Non-linear regression",
    "section": "Fitting the model (poly order 3)",
    "text": "Fitting the model (poly order 3)\n Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-poly-order-10",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-poly-order-10",
    "title": "Topic 12 – Non-linear regression",
    "section": "Fitting the model (poly order 10)",
    "text": "Fitting the model (poly order 10)\n Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_10 x_i^{10} + \\epsilon_i"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#limitations",
    "href": "lectures/L12/lecture-12.html#limitations",
    "title": "Topic 12 – Non-linear regression",
    "section": "Limitations",
    "text": "Limitations\n\nMeaning of the coefficients is not always clear.\nExtrapolation can be dangerous.\nExtra terms can lead to overfitting and are difficult to interpret:\n\n\n\n\nCall:\nlm(formula = response ~ poly(predictor, 10), data = asymptotic)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1659  -8.6908  -0.0494   8.8003  16.4012 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             79.818      1.552  51.426  &lt; 2e-16 ***\npoly(predictor, 10)1   159.368     11.084  14.378  &lt; 2e-16 ***\npoly(predictor, 10)2  -106.939     11.084  -9.648 5.37e-12 ***\npoly(predictor, 10)3    48.570     11.084   4.382 8.28e-05 ***\npoly(predictor, 10)4   -19.411     11.084  -1.751   0.0876 .  \npoly(predictor, 10)5     1.193     11.084   0.108   0.9148    \npoly(predictor, 10)6    -2.769     11.084  -0.250   0.8040    \npoly(predictor, 10)7    -1.343     11.084  -0.121   0.9042    \npoly(predictor, 10)8    -4.009     11.084  -0.362   0.7195    \npoly(predictor, 10)9    -2.851     11.084  -0.257   0.7984    \npoly(predictor, 10)10    5.769     11.084   0.520   0.6056    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.08 on 40 degrees of freedom\nMultiple R-squared:  0.8897,    Adjusted R-squared:  0.8621 \nF-statistic: 32.26 on 10 and 40 DF,  p-value: 4.846e-16\n\n\nBut:\n\nEasy to fit: just add polynomial terms to the model.\nSimple to perform: use lm()."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-a-nonlinear-model-1",
    "href": "lectures/L12/lecture-12.html#fitting-a-nonlinear-model-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Fitting a nonlinear model",
    "text": "Fitting a nonlinear model\nIf you have some understanding of the underlying relationship (e.g. mechanistic process) between the variables, you can fit a nonlinear model. \n\n\nMathematical expression\n Y_i = f(x_i, \\beta) + \\epsilon_i \nwhere f(x_i, \\beta) is a nonlinear function of the parameters \\beta.\n\nY_i is the continuous response variable.\nx_i is the vector of predictor variables.\n\\beta is the vector of unknown parameters.\n\\epsilon_i is the random error term (residual error)."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#assumptions",
    "href": "lectures/L12/lecture-12.html#assumptions",
    "title": "Topic 12 – Non-linear regression",
    "section": "Assumptions",
    "text": "Assumptions\nLike the linear model, the nonlinear model assumes:\n\nError terms are normally distributed (Normality).\nError terms are independent (Independence).\nError terms have constant variance (Homoscedasticity).\n\nBasically:\n \\epsilon_i \\sim N(0, \\sigma^2) \n\nLike all other models we have seen, we focus on the residuals to assess the model fit, since the residuals are the only part of the model that is random."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#estimating-the-model-parameters",
    "href": "lectures/L12/lecture-12.html#estimating-the-model-parameters",
    "title": "Topic 12 – Non-linear regression",
    "section": "Estimating the model parameters",
    "text": "Estimating the model parameters\n\nThe parameters are estimated using the method of least squares.\nFor nonlinear models, a nonlinear optimization algorithm is used to find the best fit, rather than ordinary least squares, e.g.:\n\nGauss-Newton algorithm\nLevenberg-Marquardt algorithm\n\nThis can only be performed iteratively and depends on a “best guess” of the parameters as a start.\n\ni.e. we need to provide a starting point for a nonlinear least squares algorithm to begin."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#implementation",
    "href": "lectures/L12/lecture-12.html#implementation",
    "title": "Topic 12 – Non-linear regression",
    "section": "Implementation",
    "text": "Implementation\nuse nls() function in R.\n\nformula: a formula object, with the response variable on the left of a ~ operator, and the predictor variable(s) on the right.\ndata: a data frame containing the variables in the model.\nstart: a named list of starting values for the parameters in the model."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#finding-starting-values",
    "href": "lectures/L12/lecture-12.html#finding-starting-values",
    "title": "Topic 12 – Non-linear regression",
    "section": "Finding starting values",
    "text": "Finding starting values\n y = a + b(1 - e^{-cx}) \n\na is value of y when x = 0.\nb is the upper limit: the maximum value of y.\nc is the rate of change: the rate at which y approaches the upper limit."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#first-guess",
    "href": "lectures/L12/lecture-12.html#first-guess",
    "title": "Topic 12 – Non-linear regression",
    "section": "First guess",
    "text": "First guess\n y = a + b(1 - e^{-cx})"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#check-the-fit",
    "href": "lectures/L12/lecture-12.html#check-the-fit",
    "title": "Topic 12 – Non-linear regression",
    "section": "Check the fit",
    "text": "Check the fit"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#interpretation",
    "href": "lectures/L12/lecture-12.html#interpretation",
    "title": "Topic 12 – Non-linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\nFormula: response ~ a + b * (1 - exp(-c * predictor))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na -14.51755    6.64162  -2.186   0.0337 *  \nb 113.03797    6.44716  17.533  &lt; 2e-16 ***\nc   0.62962    0.07142   8.816 1.32e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 48 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.178e-06\n\n\n\nThe model is significant since the p-value is less than 0.05 for all parameters.\nThe parameterised model is:\n\n y = -14.5 + 113.04(1 - e^{-0.63x})  The R-square value is not reported for nonlinear models as the sum of squares is not partitioned into explained and unexplained components."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#recap-on-logistic-relationship",
    "href": "lectures/L12/lecture-12.html#recap-on-logistic-relationship",
    "title": "Topic 12 – Non-linear regression",
    "section": "Recap on logistic relationship",
    "text": "Recap on logistic relationship\n y = c + \\frac{d-c}{1+e^{-b(x-a)}}"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#recap-on-logistic-relationship-1",
    "href": "lectures/L12/lecture-12.html#recap-on-logistic-relationship-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Recap on logistic relationship",
    "text": "Recap on logistic relationship\n y = c + \\frac{d-c}{1+e^{-b(x-a)}}"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#finding-the-starting-values",
    "href": "lectures/L12/lecture-12.html#finding-the-starting-values",
    "title": "Topic 12 – Non-linear regression",
    "section": "Finding the starting values",
    "text": "Finding the starting values\n y = c + \\frac{d-c}{1+e^{-b(x-a)}} \n\nc is the lower limit: the minimum value of y.\nd is the upper limit: the maximum value of y.\na is the value of x when y is halfway between the lower and upper limits.\nb is the rate of change: the rate at which y approaches the upper limit."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#finding-the-starting-values-1",
    "href": "lectures/L12/lecture-12.html#finding-the-starting-values-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Finding the starting values",
    "text": "Finding the starting values\n y = c + \\frac{d-c}{1+e^{-b(x-a)}} \n\nc is the lower limit: the minimum value of y.\nd is the upper limit: the maximum value of y.\na is the value of x when y is halfway between the lower and upper limits.\nb is the rate of change: the rate at which y approaches the upper limit.\n\n\n\nNOPE"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#self-starting-functions",
    "href": "lectures/L12/lecture-12.html#self-starting-functions",
    "title": "Topic 12 – Non-linear regression",
    "section": "Self-starting functions",
    "text": "Self-starting functions\n\nThe nls() function requires a formula and starting point(s) for the parameters.\n\nHow about starting to nope out…\n\n\n\n\nWait!\n\nSeveral self-starting functions are available in R that can be used to estimate the starting values.\nThese functions are named after the model they fit, e.g. SSasymp(), SSlogis(), SSmicmen(), SSweibull(), etc.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nWe still need to have some understanding of the underlying relationship between the variables to pick the right function."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#revisiting-the-logistic-model",
    "href": "lectures/L12/lecture-12.html#revisiting-the-logistic-model",
    "title": "Topic 12 – Non-linear regression",
    "section": "Revisiting the logistic model",
    "text": "Revisiting the logistic model\n y = c + \\frac{d-c}{1+e^{-b(x-a)}} \n\ninput: the predictor variable.\nAsym: the upper limit.\nxmid: the value of x when y is halfway between the lower and upper limits.\nscal: the rate of change.\n\nThe equation ia different: see ?SSlogis:\n y = \\frac{Asym}{1+exp \\frac{xmid-input}{scal}} \n\nOther than input, the other parameters can be left to the function to estimate."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#what-does-the-fit-look-like",
    "href": "lectures/L12/lecture-12.html#what-does-the-fit-look-like",
    "title": "Topic 12 – Non-linear regression",
    "section": "What does the fit look like?",
    "text": "What does the fit look like?"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#check-the-fit-1",
    "href": "lectures/L12/lecture-12.html#check-the-fit-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Check the fit",
    "text": "Check the fit"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#interpretation-1",
    "href": "lectures/L12/lecture-12.html#interpretation-1",
    "title": "Topic 12 – Non-linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\nFormula: response ~ SSlogis(predictor, Asym, xmid, scal)\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nAsym 310.64727    4.62579   67.16   &lt;2e-16 ***\nxmid   4.92715    0.07142   68.99   &lt;2e-16 ***\nscal   1.34877    0.05418   24.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.22 on 48 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 6.626e-07"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#comparing-manual-fit-to-self-starting-function",
    "href": "lectures/L12/lecture-12.html#comparing-manual-fit-to-self-starting-function",
    "title": "Topic 12 – Non-linear regression",
    "section": "Comparing manual fit to self-starting function",
    "text": "Comparing manual fit to self-starting function\nThe self-starting function for the asymptotic model is SSasymp().\nComparing outputs:\n\nIn some cases, the fits are identical, but in others, they are not."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#summary",
    "href": "lectures/L12/lecture-12.html#summary",
    "title": "Topic 12 – Non-linear regression",
    "section": "Summary",
    "text": "Summary\n\nWhen fitting a nonlinear model, there are three possible approaches:\n\nLinearize the model by transforming the response variable or predictor variable(s):\n\nFit: easy/difficult\nInterpret: difficult\n\nApproximate the model by adding polynomial terms:\n\nFit: easy\nInterpret: difficult\n\nFit the model using a nonlinear least squares algorithm:\n\nFit: difficult\nInterpret: easy\n\n\n\n\n\nNonlinear models:\n\nUseful for modelling complex relationships.\nRequire some understanding of the underlying relationship between the variables, especially asympotic and logistic models.\nMost useful when prediction is the goal, since we do not necessarily need to interpret the parameters to assess the model fit."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#example-polynomial-regression",
    "href": "lectures/L12/lecture-12.html#example-polynomial-regression",
    "title": "Topic 12 – Non-linear regression",
    "section": "Example: polynomial regression",
    "text": "Example: polynomial regression"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#prediction-quality",
    "href": "lectures/L12/lecture-12.html#prediction-quality",
    "title": "Topic 12 – Non-linear regression",
    "section": "Prediction quality",
    "text": "Prediction quality\nWe can use prediction quality metrics to compare the fits.\n\nAkaike information criterion (AIC) and Bayesian information criterion (BIC).\n\nUseful for comparing model fits.\n\nroot mean squared error (RMSE) and mean absolute error (MAE).\n\nUseful for assessing the quality of the fit."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#aic-and-bic",
    "href": "lectures/L12/lecture-12.html#aic-and-bic",
    "title": "Topic 12 – Non-linear regression",
    "section": "AIC and BIC",
    "text": "AIC and BIC\nUse the broom package to extract the AIC and BIC values from the model fits.\n\n\n# A tibble: 4 × 3\n  model    AIC   BIC\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 linear  453.  459.\n2 poly2   409.  416.\n3 poly3   392.  402.\n4 poly10  402.  425.\n\n\n\nThe smaller the AIC or BIC, the better the fit compared to other models.\nHowever, for better performance, cross-validation is recommended as it explains how well the model will perform on new data, rather than just assessing the fit to the data."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#cross-validation",
    "href": "lectures/L12/lecture-12.html#cross-validation",
    "title": "Topic 12 – Non-linear regression",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nSplit the data into training and testing sets.\nFit the model to the training set.\nPredict the response variable using the testing set.\nCalculate the RMSE or MAE between the predicted and observed values.\nRepeat for each fold of the data.\nAverage the RMSE or MAE across all folds.\nThe model with the lowest RMSE or MAE is the best model.\n\nLooks like a lot of work, but it’s easy in R using the caret package."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#performing-cross-validation-on-the-polynomial-fits",
    "href": "lectures/L12/lecture-12.html#performing-cross-validation-on-the-polynomial-fits",
    "title": "Topic 12 – Non-linear regression",
    "section": "Performing cross-validation on the polynomial fits",
    "text": "Performing cross-validation on the polynomial fits\n\n\n\n\n\n\nMAE\nRMSE\n\n\n\n\nlinear\n15.751679\n19.76680\n\n\nquadratic\n9.881451\n11.93538\n\n\ncubic\n9.741090\n11.27664\n\n\npoly10\n11.774449\n13.70916\n\n\n\n\n\nFrom the results, the cubic model has the lowest RMSE and MAE, so is the best model."
  },
  {
    "objectID": "lectures/L02/lecture-02.html#categorical-data---bar-chart-2",
    "href": "lectures/L02/lecture-02.html#categorical-data---bar-chart-2",
    "title": "Topic 2 – Exploratory Data Analysis (EDA)",
    "section": "Categorical data - Bar chart",
    "text": "Categorical data - Bar chart\n\nUsing tidyverse"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#last-week",
    "href": "lectures/L10/lecture-10.html#last-week",
    "title": "Topic 10 – Linear functions",
    "section": "Last week…",
    "text": "Last week…\n\nCorrelation r: a measure of the strength and direction of the linear relationship between two variables\nIs there a causal relationship between two variables?\n\nNo: use correlation analysis\nYes: use regression analysis\n\n\n\nSimple linear regression modelling\n Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \n\nBasically, a deterministic straight line equation y=c+mx, with added random variation that is normally distributed\n\n Y = c + mx + \\epsilon"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#fitting-the-line",
    "href": "lectures/L10/lecture-10.html#fitting-the-line",
    "title": "Topic 10 – Linear functions",
    "section": "Fitting the line",
    "text": "Fitting the line\n Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \n Y = c + mx + \\epsilon \nHow do we fit a line to data if data are “noisy”?\n\n\nCode\nx &lt;- 1:10\ny &lt;- 2 * x + rnorm(10, 0, 2)\n# generate y with predicted values\ny_pred &lt;- 2 * x\ndf &lt;- data.frame(x, y)\n\np1 &lt;- ggplot(df, aes(x, y_pred)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  labs(x = \"x\", y = \"y\", title = \"A\")\n\np2 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"royalblue\") +\n  labs(x = \"x\", y = \"y\", title = \"B (How do we fit this?)\")\n\nlibrary(patchwork)\np1 + p2 + plot_layout(ncol = 2)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#usage",
    "href": "lectures/L10/lecture-10.html#usage",
    "title": "Topic 10 – Linear functions",
    "section": "Usage",
    "text": "Usage\n\nStudent’s t-test\nlinear regression\n\n\n\nANOVA\nlogistic regression\nnonlinear regression\nridge regression\nlasso regression\nprinciple component analysis\ngeneralised linear model\netc…"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#galtons-data-revisited",
    "href": "lectures/L10/lecture-10.html#galtons-data-revisited",
    "title": "Topic 10 – Linear functions",
    "section": "Galton’s data revisited",
    "text": "Galton’s data revisited\n\nGalton’s data on the heights of parents and their children.\nIs there a relationship between the heights of parents and their children?\n\n\n\nCode\nlibrary(HistData)\ndata(Galton)\nfit &lt;- lm(child ~ parent, data = Galton)\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  labs(x = \"Parent height (inches)\", y = \"Child height (inches)\")\n\n\n\nHow did we end up with the line in the plot above?"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#how-do-we-fit-a-line",
    "href": "lectures/L10/lecture-10.html#how-do-we-fit-a-line",
    "title": "Topic 10 – Linear functions",
    "section": "How do we fit a line?",
    "text": "How do we fit a line?\n\nMinimise the sum of the squared residuals:\n\n\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2\n\nSource"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#residuals-hat-epsilon",
    "href": "lectures/L10/lecture-10.html#residuals-hat-epsilon",
    "title": "Topic 10 – Linear functions",
    "section": "Residuals, \\hat \\epsilon",
    "text": "Residuals, \\hat \\epsilon\n \\color{firebrick}{\\hat{\\epsilon_i}} = \\color{royalblue}{y_i} - \\color{forestgreen}{\\hat{y_i}} \n\n\n\nCode\n# simulate example data\nset.seed(340)\nx &lt;- runif(8, 0, 30)\ny &lt;- 5 * x + rnorm(8, 0, 40)\ndf &lt;- data.frame(x, y)\n\n# fit linear model, add residual vertical lines as arrows\nmod &lt;- lm(y ~ x, data = df)\np1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_segment(aes(xend = x, yend = fitted(mod)),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    color = \"royalblue\"\n  ) +\n  labs(x = \"x\", y = \"y\")\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  annotate(\"text\",\n    x = 6.3, y = -6, size = 7,\n    label = expression(hat(epsilon[i])), colour = \"royalblue\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = 25, size = 7,\n    label = expression(hat(y[i])), colour = \"forestgreen\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = -36, size = 7,\n    label = expression(y[i]), colour = \"firebrick\"\n  )"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#slope-beta_1",
    "href": "lectures/L10/lecture-10.html#slope-beta_1",
    "title": "Topic 10 – Linear functions",
    "section": "Slope, \\beta_1",
    "text": "Slope, \\beta_1\n\n \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} \n\n\nCode\n# Calculate slope from df\nbeta1 &lt;- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) /\n  sum((df$x - mean(df$x))^2)\n# beta0 &lt;- mean(df$y) - beta1 * mean(df$x)\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linetype = 2) +\n  # label the line\n  annotate(\"text\",\n    x = 15, y = 65, size = 7,\n    label = expression(beta[1]), colour = \"firebrick\"\n  )"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#intercept",
    "href": "lectures/L10/lecture-10.html#intercept",
    "title": "Topic 10 – Linear functions",
    "section": "Intercept",
    "text": "Intercept\n \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \n\n\nCode\n# calculate mean y from df\nybar &lt;- mean(df$y)\nxbar &lt;- mean(df$x)\nbeta0 &lt;- ybar - beta1 * xbar\n\np1 + geom_vline(xintercept = xbar, linetype = \"dashed\", color = \"slateblue\") +\n  geom_hline(yintercept = ybar, linetype = \"dashed\", color = \"slateblue\") +\n  # label the lines\n  annotate(\"text\",\n    x = 25, y = ybar * 0.8, size = 7,\n    label = expression(bar(y)), colour = \"slateblue\"\n  ) +\n  annotate(\"text\",\n    x = xbar * 1.05, y = 150, size = 7,\n    label = expression(bar(x)), colour = \"slateblue\"\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linetype = 2) +\n  # extend the geom_smooth line to intercept x=0\n  geom_segment(aes(x = xbar, y = ybar, xend = 0, yend = beta0),\n    color = \"firebrick\", linetype = 2\n  ) +\n  # label the slope line\n  annotate(\"text\",\n    x = 15, y = 65, size = 7,\n    label = expression(beta[1]), colour = \"firebrick\"\n  ) +\n  # add a dot at the intercept\n  geom_point(aes(x = 0, y = beta0), color = \"seagreen\", size = 3) +\n  # label the intercept\n  annotate(\"text\",\n    x = 0, y = beta0 * 1.4, size = 7,\n    label = expression(beta[0]), colour = \"seagreen\"\n  )"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#fitting-a-linear-model-in-r",
    "href": "lectures/L10/lecture-10.html#fitting-a-linear-model-in-r",
    "title": "Topic 10 – Linear functions",
    "section": "Fitting a linear model in R",
    "text": "Fitting a linear model in R\nIs there a relationship between the heights of parents and their children?\n\nfit &lt;- lm(child ~ parent, data = Galton)\nfit\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n\n\n \\widehat{child} = 23.9 + 0.646 \\cdot parent\n\nBut is the model any good?"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumptions",
    "href": "lectures/L10/lecture-10.html#assumptions",
    "title": "Topic 10 – Linear functions",
    "section": "Assumptions",
    "text": "Assumptions\nThe data must meet certain criteria, which we often call assumptions. They can be remembered using LINE:\n\nLinearity. The relationship between y and x is linear.\nIndependence. The errors \\epsilon are independent.\nNormal. The errors \\epsilon are normally distributed.\nEqual Variance. At each value of x, the variance of y is the same i.e. homoskedasticity, or constant variance.\n\n\n\n\n\n\n\n\nTip\n\n\nAll but the independence assumption can be assessed using diagnostic plots."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumptions-why-do-we-care",
    "href": "lectures/L10/lecture-10.html#assumptions-why-do-we-care",
    "title": "Topic 10 – Linear functions",
    "section": "Assumptions: Why do we care?",
    "text": "Assumptions: Why do we care?\n\nIf the assumptions are met, then we can be confident that the model is a good representation of the data.\nIf they are not met, the results are still presented, but our interpretation of the model is likely to be flawed.\n\n\n\n\n\n\n\n\nWarning\n\n\nR will not warn you if the assumptions are not met. It is up to you to check them!"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#how-do-we-check-the-assumptions",
    "href": "lectures/L10/lecture-10.html#how-do-we-check-the-assumptions",
    "title": "Topic 10 – Linear functions",
    "section": "How do we check the assumptions?",
    "text": "How do we check the assumptions?\nRecall that the linear model is a deterministic straight line equation y = c + mx plus some random noise \\epsilon:\n Y_i = \\beta_0 + \\beta_1 x + \\epsilon \n\nIf the only source of variation in y is \\epsilon, then we can check our assumptions by just looking at the residuals \\hat{\\epsilon}.\n\nHow do we get the residuals?\n\nFit the model…\nResiduals need to be calculated from the model, not from the raw data.\nIn R, these values are stored automatically."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#another-way-to-look-at-residuals",
    "href": "lectures/L10/lecture-10.html#another-way-to-look-at-residuals",
    "title": "Topic 10 – Linear functions",
    "section": "Another way to look at residuals",
    "text": "Another way to look at residuals\n\nOnce you have fitted the line, it does not change. The residuals are the vertical distances between the points (not shown) and the line."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#step",
    "href": "lectures/L10/lecture-10.html#step",
    "title": "Topic 10 – Linear functions",
    "section": "1-step",
    "text": "1-step\n\npar(mfrow = c(2, 2)) # need to do this to get 4 plots on one page\nplot(fit)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumption-linearity",
    "href": "lectures/L10/lecture-10.html#assumption-linearity",
    "title": "Topic 10 – Linear functions",
    "section": "Assumption: Linearity",
    "text": "Assumption: Linearity\n\nResiduals vs. fitted plot looks at the relationship between the residuals and the fitted values.\nIf the relationship is linear:\n\nResiduals should be randomly scattered around the horizontal axis.\nThe red line should be reasonably straight.\n\n\n\nplot(fit, which = 1)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumption-normality",
    "href": "lectures/L10/lecture-10.html#assumption-normality",
    "title": "Topic 10 – Linear functions",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\nQ-Q plot looks at the distribution of the residuals against a normal distribution function (the dotted line).\nSometimes, a histogram is still useful to see the shape of the distribution.\n\n\npar(mfrow = c(1, 2))\nplot(fit, which = 2)\nhist(rstandard(fit))"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumption-normality-1",
    "href": "lectures/L10/lecture-10.html#assumption-normality-1",
    "title": "Topic 10 – Linear functions",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\nIf normally distributed, the points should follow the red line.\nDeviation from the red line is common in the tails (i.e. the ends), but not in the middle.\n\n\nTips\n\nLight-tailed: small variance in residuals, resulting in a narrow distribution.\nHeavy-tailed: many extreme positive and negative residuals, resulting in a wide distribution.\nLeft-skewed (n shape): more data falls to the left of the mean.\nRight-skewed (u shape): more data falls to the right of the mean."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#section",
    "href": "lectures/L10/lecture-10.html#section",
    "title": "Topic 10 – Linear functions",
    "section": "",
    "text": "Code\nset.seed(915)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x + rchisq(100, df = 2)\ndf &lt;- data.frame(x, y)\nfit_eg &lt;- lm(y ~ x, data = df)\npar(mfrow = c(1, 2))\nplot(fit_eg, which = 2)\nhist(rstandard(fit_eg))"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#equal-variances",
    "href": "lectures/L10/lecture-10.html#equal-variances",
    "title": "Topic 10 – Linear functions",
    "section": "Equal variances",
    "text": "Equal variances\n\nLook at the scale-location plot.\nIf variances are equal, the points should be randomly scattered around the horizontal axis.\nThe red line should be more or less horizontal.\n\n\nplot(fit, which = 3)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#equal-variances-1",
    "href": "lectures/L10/lecture-10.html#equal-variances-1",
    "title": "Topic 10 – Linear functions",
    "section": "Equal variances",
    "text": "Equal variances\n\nIf variances are not equal we may see:\n\nA funnel shape, where the points are more spread out at the ends than in the middle. Sometimes also called “fanning”.\nPatterns in the scale-location plot, such as a curve or a wave, indicating that the variance is changing.\n\nLook at the red line for a general trend, but don’t depend on it too much.\n\n\n\nCode\nset.seed(915)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x^2 + rchisq(100, df = 2)\ndf &lt;- data.frame(x, y)\nfit_eg &lt;- lm(y ~ x, data = df)\nplot(fit_eg, which = 3)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#outliers",
    "href": "lectures/L10/lecture-10.html#outliers",
    "title": "Topic 10 – Linear functions",
    "section": "Outliers",
    "text": "Outliers\n\nLeverage is a measure of how far away the predictor variable is from the mean of the predictor variable.\nThe Residuals vs Leverage plot shows the relationship between the residuals and the leverage of each point.\nCook’s distance is a measure of how much the model would change if a point was removed."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#it-depends",
    "href": "lectures/L10/lecture-10.html#it-depends",
    "title": "Topic 10 – Linear functions",
    "section": "It depends…",
    "text": "It depends…\n\nDepends which assumption is not met and the type of data i.e. circumstances.\n\nIf data is non-linear, try a transformation of the response variable y, from light to extreme:\n\nroot: \\sqrt{y} or \\sqrt{y+1} if y contains zeros\nlog: \\log(y) or \\log(y+1) if y contains zeros\ninverse: \\frac{1}{y} or \\frac{1}{y+1} if y contains zeros\n\n\n\n\n\nIf data is not normally distributed, try a transformation of the response variable y first, otherwise transform the predictor variable x. Both can be done at the same time.\n\n\n\n\nIf equal variances assumption is not met, same as above.\nIf outliers are present, try removing them, or transforming the response variable y."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#what-if-that-doesnt-work",
    "href": "lectures/L10/lecture-10.html#what-if-that-doesnt-work",
    "title": "Topic 10 – Linear functions",
    "section": "What if that doesn’t work?",
    "text": "What if that doesn’t work?\n\nIf the assumptions are still not met after trying the above, you can try:\n\nUsing a different model e.g. generalized linear model.\nUsing a different type of regression e.g. logistic regression.\nUsing a non-parametric test."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#model-assumptions-are-met.-now-what",
    "href": "lectures/L10/lecture-10.html#model-assumptions-are-met.-now-what",
    "title": "Topic 10 – Linear functions",
    "section": "Model assumptions are met. Now what?",
    "text": "Model assumptions are met. Now what?\n\nsummary(fit)\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#the-model-so-far",
    "href": "lectures/L10/lecture-10.html#the-model-so-far",
    "title": "Topic 10 – Linear functions",
    "section": "The model so far",
    "text": "The model so far\n\nlibrary(HistData)\ndata(Galton)\nfit &lt;- lm(child ~ parent, data = Galton)\nsummary(fit)\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#hypothesis-testing",
    "href": "lectures/L10/lecture-10.html#hypothesis-testing",
    "title": "Topic 10 – Linear functions",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nHow does our null (H_0: \\beta_1=0) model compare to the linear (H_0: \\beta_1 \\neq 0) model?\n\n\nCode\nnull_model &lt;- Galton %&gt;%\n  lm(child ~ 1, data = .) %&gt;%\n  augment(Galton)\nlin_model &lt;- Galton %&gt;%\n  lm(child ~ parent, data = .) %&gt;%\n  augment(Galton)\nmodels &lt;- bind_rows(null_model, lin_model) %&gt;%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#what-are-we-testing",
    "href": "lectures/L10/lecture-10.html#what-are-we-testing",
    "title": "Topic 10 – Linear functions",
    "section": "What are we testing?",
    "text": "What are we testing?\n\nThe null model is a model with no predictors, i.e. y = \\beta_0 + \\epsilon\nThe linear model is a model with one predictor, i.e. y = \\beta_0 + \\beta_1 x + \\epsilon\nWe use the t-test to compare the two models:\n\n t = \\frac{estimate - 0}{Standard\\ error} = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}  where SE(\\hat{\\beta}_1) is the standard error of the slope estimate:\n SE(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nCall: the model formula"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-1",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-1",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nResiduals: distribution of the residuals"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-2",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-2",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nCoefficients: a summary table of the coefficients, their standard errors, t-values, and p-values addressing the hypothesis that the coefficient is 0"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-3",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-3",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\n(Intercept) term is the mean of the response when all predictors are 0, which is not meaningful in most cases. In this case, it is the mean child height when the parent height is 0.\nparent: the slope coefficient that we are interested in, which represents the change in the mean of the response for a one-unit increase in the predictor.\n\nThe p-value (Pr) tells us whether the slope is significantly different from 0.\nIf it is, then we can conclude that there is a significant linear relationship between the predictor and the response."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-4",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-4",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also use the Estimate values to write the equation of the regression line:  \\widehat{child} = 23.94153 + 0.64629 \\cdot parent\nFor every one-inch increase in the parent height, the child height is predicted to increase by 0.64629 inches."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-5",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-5",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nResidual standard error: the standard deviation of the residuals.\n\nInterpretation: the average amount that the response will deviate from the true regression line.\n\ndegrees of freedom: the number of observations minus the number of parameters being estimated. Used in hypothesis testing and calculating the standard error of the regression coefficients.\n\nCan estimate sample size from this number."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-6",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-6",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nMultiple R-squared: the proportion of variance explained by the model.\nAdjusted R-squared: the proportion of variance explained by the model, adjusted for the number of predictors.\nInterpretation:\n\nRanges from 0 to 1.\nSince this is SLR, we can interpret this as the proportion of variance in the response that is explained by parent: 21.05% (from Multiple R-squared)."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-7",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-7",
    "title": "Topic 10 – Linear functions",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nF-statistic: the ratio of the variance of the regression model to the variance of the residuals.\n\nAlso known as the partial F-test between the full model and the intercept-only (null) model.\n\np-value: for the linear model, the p-value is the probability that the F-statistic is greater than the observed value under the null hypothesis.\n\nA significant p-value indicates that the linear model is a better fit than the intercept-only model."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#two-methods",
    "href": "lectures/L10/lecture-10.html#two-methods",
    "title": "Topic 10 – Linear functions",
    "section": "Two methods",
    "text": "Two methods\n\n\nUsing ANOVA\nanova(fit)\n\nfit &lt;- lm(formula = child ~ parent, data = Galton)\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: child\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nparent      1 1236.9 1236.93  246.84 &lt; 2.2e-16 ***\nResiduals 926 4640.3    5.01                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nUsing Regression\nsummary(fit)\n\nsummary(fit)\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#two-methods-1",
    "href": "lectures/L10/lecture-10.html#two-methods-1",
    "title": "Topic 10 – Linear functions",
    "section": "Two methods",
    "text": "Two methods\n\n\nUsing ANOVA\n\nThe ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p &lt; .001)\n\n\nUsing Regression\n\nWe fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p &lt; .001, adj. R2 = 0.21). Within this model, the effect of parent is statistically significant and positive (\\beta = 0.65, t(926) = 15.71, p &lt; .001).\n\n\n\n\n\n\n\n\n\nTip\n\n\nFor simple linear models, summary() provides more information than anova(), but the results are the same."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#explore",
    "href": "lectures/L10/lecture-10.html#explore",
    "title": "Topic 10 – Linear functions",
    "section": "Explore",
    "text": "Explore\nRead the data:\n\nlibrary(readxl) # load the readxl package\n\nalligator &lt;- read_excel(path = \"data/ENVX1002_Lecture_wk10_data.xlsx\", \n  sheet = \"Alligator\") # read in the data\n\nWhat does the data look like?\n\nstr(alligator)\n\ntibble [25 × 2] (S3: tbl_df/tbl/data.frame)\n $ Length: num [1:25] 58 61 63 68 69 72 72 74 74 76 ...\n $ Weight: num [1:25] 28 44 33 39 36 38 61 54 51 42 ..."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#plot",
    "href": "lectures/L10/lecture-10.html#plot",
    "title": "Topic 10 – Linear functions",
    "section": "Plot",
    "text": "Plot\n\nUsing base RUsing ggplot2\n\n\n\nplot(x = alligator$Length, y = alligator$Weight, \n  xlab = \"Length (cm)\", ylab = \"Weight (kg)\")\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2) # load the ggplot2 package\nggplot(data = alligator, aes(x = Length, y = Weight)) +\n  geom_point() +\n  labs(x = \"Length (cm)\", y = \"Weight (kg)\")"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#plot-residual-diagnostics",
    "href": "lectures/L10/lecture-10.html#plot-residual-diagnostics",
    "title": "Topic 10 – Linear functions",
    "section": "Plot residual diagnostics",
    "text": "Plot residual diagnostics\nTo check assumptions, we need to fit the model first, then plot the model.\n\nfit &lt;- lm(formula = Weight ~ Length, data = alligator)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#check-assumptions",
    "href": "lectures/L10/lecture-10.html#check-assumptions",
    "title": "Topic 10 – Linear functions",
    "section": "Check assumptions",
    "text": "Check assumptions\nIs the relationship linear?\n\nplot(fit, which = 1)\n\n\nIf the linearity assumption is not met, there is no reason to validate the model since it is no longer suitable for the data."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#dealing-with-non-linearity-transform-the-data",
    "href": "lectures/L10/lecture-10.html#dealing-with-non-linearity-transform-the-data",
    "title": "Topic 10 – Linear functions",
    "section": "Dealing with non-linearity: transform the data",
    "text": "Dealing with non-linearity: transform the data\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- ggplot(data = alligator, aes(x = Length, y = Weight)) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"Weight (kg)\", title = \"Original\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np2 &lt;- ggplot(data = alligator, aes(x = Length, y = sqrt(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"sqrt[Weight (kg)]\", title = \"Square root\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np3 &lt;- ggplot(data = alligator, aes(x = Length, y = log(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"log[Weight (kg)]\", title = \"Natural log\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np4 &lt;- ggplot(data = alligator, aes(x = Length, y = log10(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"log10[Weight (kg)]\", title = \"Log base 10\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#natural-log-transformation",
    "href": "lectures/L10/lecture-10.html#natural-log-transformation",
    "title": "Topic 10 – Linear functions",
    "section": "Natural log transformation",
    "text": "Natural log transformation\n\nfit &lt;- lm(formula = log(Weight) ~ Length, data = alligator)\nplot(fit, which = 1)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#natural-log-transformation-check-assumptions-again",
    "href": "lectures/L10/lecture-10.html#natural-log-transformation-check-assumptions-again",
    "title": "Topic 10 – Linear functions",
    "section": "Natural log transformation – Check assumptions again",
    "text": "Natural log transformation – Check assumptions again\n\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpretation",
    "href": "lectures/L10/lecture-10.html#interpretation",
    "title": "Topic 10 – Linear functions",
    "section": "Interpretation",
    "text": "Interpretation\n\nsummary(fit)\n\n\nCall:\nlm(formula = log(Weight) ~ Length, data = alligator)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.289266 -0.079989  0.000933  0.102216  0.288491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.335335   0.131394   10.16 5.63e-10 ***\nLength      0.035416   0.001506   23.52  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1493 on 23 degrees of freedom\nMultiple R-squared:  0.9601,    Adjusted R-squared:  0.9583 \nF-statistic:   553 on 1 and 23 DF,  p-value: &lt; 2.2e-16\n\n\n\nLength is a statistically significant predictor of log(Weight) (p &lt; .001).\nThe model explains a statistically significant and large proportion (96%) of variance (R2 = 0.96, F(1, 23) = 553, p &lt; .001)\nFor every 1 cm increase in Length, log(Weight) increases by 0.0354.\n\nOr, for every 1 cm increase in Length, percent increase in Weight is 3.54% (only works when transforming using natural log)."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#summary",
    "href": "lectures/L10/lecture-10.html#summary",
    "title": "Topic 10 – Linear functions",
    "section": "Summary",
    "text": "Summary\nYou should know the workflow by now\n\nExplore\nPlot\nFit model and plot residual diagnostics\nCheck assumptions, transform data if necessary. Go back to step 3.\nInterpret"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#module-overview",
    "href": "lectures/L11/lecture-11.html#module-overview",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Module overview",
    "text": "Module overview\n\nWeek 9. Describing relationships\n\nCorrelation → calculation, interpretation, things to watch out for\nRegression → Why do we care? model structure, model fitting\n\nWeek 10. Linear functions\n\nIs the model worth fitting? → Assumptions, hypothesis testing\nHow good is the model? → Measures of model fit\n\nWeek 11. Linear functions - multiple predictors\n\nParsimonious models\nIntroduction to Multiple Linear Regression (MLR) modelling\nAssumptions and interpretation\n\nWeek 12. Nonlinear functions\n\nCommon nonlinear functions\nTransformations\nPerforming nonlinear regression"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#module-overview-1",
    "href": "lectures/L11/lecture-11.html#module-overview-1",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Module overview",
    "text": "Module overview\n\nWeek 11. Linear functions - multiple predictors\n\nParsimonious models\nIntroduction to Multiple Linear Regression (MLR) modelling\nAssumptions and interpretation"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#simple-linear-regression",
    "href": "lectures/L11/lecture-11.html#simple-linear-regression",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \nIdeal for predicting a continuous response variable from a single predictor variable: “How does y change as x changes?”\n\nWhat if we have more than one predictor?\nWhat is the model and how do we interpret the results?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#history",
    "href": "lectures/L11/lecture-11.html#history",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "History",
    "text": "History\n\nFirst raised by Francis Galton in 1886, after studying genetic variations in sweet peas over several generations.\nKarl Pearson developed the mathematical formalism for the multiple linear regression model in the early 1900s.\n\n\n“The somewhat complicated mathematics of multiple correlation, with its repeated appeals to the geometrical notions of hyperspace, remained a closed chamber to him.”\n\n– Pearson (1930), on Galton’s work with MLR"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#air-quality",
    "href": "lectures/L11/lecture-11.html#air-quality",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Air quality",
    "text": "Air quality\n\n\nRows: 153\nColumns: 6\n$ Ozone   &lt;int&gt; 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R &lt;int&gt; 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    &lt;dbl&gt; 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    &lt;int&gt; 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#section",
    "href": "lectures/L11/lecture-11.html#section",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "",
    "text": "Rows: 153\nColumns: 6\n$ Ozone   &lt;int&gt; 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R &lt;int&gt; 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    &lt;dbl&gt; 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    &lt;int&gt; 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n\n\n\nOzone: harmful air pollutant when present at ground level; main component of smog:\n\nOzone: ozone concentration (ppb)\nSolar.R: solar radiation (lang)\nWind: wind speed (mph)\nTemp: ambient temperature (degrees F)\nMonth: month (1-12)\nDay: day of the month (1-31)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#correlations",
    "href": "lectures/L11/lecture-11.html#correlations",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Correlations",
    "text": "Correlations"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#corrplot",
    "href": "lectures/L11/lecture-11.html#corrplot",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "corrplot",
    "text": "corrplot"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#psych",
    "href": "lectures/L11/lecture-11.html#psych",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "psych",
    "text": "psych"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-simplest-model",
    "href": "lectures/L11/lecture-11.html#the-simplest-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The simplest model",
    "text": "The simplest model\nPick the predictor that has the highest correlation coefficient with the response variable.\n\n\n\n               Ozone     Solar.R        Wind       Temp        Month\nOzone    1.000000000  0.34834169 -0.61249658  0.6985414  0.142885168\nSolar.R  0.348341693  1.00000000 -0.12718345  0.2940876 -0.074066683\nWind    -0.612496576 -0.12718345  1.00000000 -0.4971897 -0.194495804\nTemp     0.698541410  0.29408764 -0.49718972  1.0000000  0.403971709\nMonth    0.142885168 -0.07406668 -0.19449580  0.4039717  1.000000000\nDay     -0.005189769 -0.05775380  0.04987102 -0.0965458 -0.009001079\n                 Day\nOzone   -0.005189769\nSolar.R -0.057753801\nWind     0.049871017\nTemp    -0.096545800\nMonth   -0.009001079\nDay      1.000000000\n\n\n\n\nWhat can we understand about the relationship between Ozone and Temp (r = 0.7)?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#relationship",
    "href": "lectures/L11/lecture-11.html#relationship",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Relationship",
    "text": "Relationship\nWhat can we understand about the relationship between Ozone and Temp (r = 0.7)?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#relationship-1",
    "href": "lectures/L11/lecture-11.html#relationship-1",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Relationship",
    "text": "Relationship\nWhat can we understand about the relationship between Ozone and Temp (r = 0.7)?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#fitting-the-model",
    "href": "lectures/L11/lecture-11.html#fitting-the-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Fitting the model",
    "text": "Fitting the model"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions",
    "href": "lectures/L11/lecture-11.html#assumptions",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions",
    "text": "Assumptions"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#ggfortify",
    "href": "lectures/L11/lecture-11.html#ggfortify",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "ggfortify",
    "text": "ggfortify"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#interpretation",
    "href": "lectures/L11/lecture-11.html#interpretation",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nTemp is a statistically significant predictor of Ozone (p &lt; .001).\nThe (simple linear) model explains 49% of variance (r2 = 0.49).\n\n\nCan we improve the model in other ways?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#important-concepts",
    "href": "lectures/L11/lecture-11.html#important-concepts",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Important concepts",
    "text": "Important concepts\n\nThe “best” model is the one that best describes the relationship between the response and the predictors.\n\nNOT the model that includes all possible predictors (data dredging).\n\n\n\nPrinciple of parsimony\nA good model:\n\nHas only useful predictors.\nHas no redundant predictors (principle of orthogonality).\nIs interpretable (principle of transparency) or predicts well (principle of accuracy)."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-mlr-model",
    "href": "lectures/L11/lecture-11.html#the-mlr-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The MLR model",
    "text": "The MLR model\nAn extension of simple linear regression to include more than one predictor variable: “How does y change as x_1, x_2, …, x_k change?”\n Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i \n\nTherefore, estimating the model involves estimating the values of \\beta_0, \\beta_1, \\beta_2, …, \\beta_k.\n\n\\beta_0 is the intercept\n\\beta_1 to \\beta_k are the partial regression coefficients\n\\epsilon is the error term"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#explore",
    "href": "lectures/L11/lecture-11.html#explore",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Explore",
    "text": "Explore\n\n\nRows: 153\nColumns: 6\n$ Ozone   &lt;int&gt; 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R &lt;int&gt; 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    &lt;dbl&gt; 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    &lt;int&gt; 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n\n\n\n\n\nThe “best” model\nThe variables Month and Day are not useful predictors, so we will exclude them from the model."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#visualisation-not-easy",
    "href": "lectures/L11/lecture-11.html#visualisation-not-easy",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Visualisation: not easy",
    "text": "Visualisation: not easy\nAre the plots useful?\n3D plot"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#visualisation-not-easy-1",
    "href": "lectures/L11/lecture-11.html#visualisation-not-easy-1",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Visualisation: not easy",
    "text": "Visualisation: not easy\nAre the plots useful?\n4D plot"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#partial-regression-coefficients",
    "href": "lectures/L11/lecture-11.html#partial-regression-coefficients",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Partial regression coefficients",
    "text": "Partial regression coefficients\nGiven the multiple linear model:  Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i \nThe partial regression coefficient for a predictor x_i is the amount by which the response variable Y changes when x_k is increased by one unit, while all other predictors are held constant.\n \\beta_k = \\frac{\\Delta Y}{\\Delta x_k} \n\n\n\n\n\\operatorname{Ozone} = \\alpha + \\beta_{1}(\\operatorname{Solar.R}) + \\beta_{2}(\\operatorname{Wind}) + \\beta_{3}(\\operatorname{Temp}) + \\epsilon\n\n\n\n\nWith Wind and Solar.R held constant, how does Temp affect Ozone?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#partial-regression-coefficients-visualisation",
    "href": "lectures/L11/lecture-11.html#partial-regression-coefficients-visualisation",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Partial regression coefficients: visualisation",
    "text": "Partial regression coefficients: visualisation\n\n\nWith Wind and Solar.R held constant, how does Temp affect Ozone?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#interpreting-the-partial-regression-coefficients",
    "href": "lectures/L11/lecture-11.html#interpreting-the-partial-regression-coefficients",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Interpreting the partial regression coefficients",
    "text": "Interpreting the partial regression coefficients\n\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality)\n\nCoefficients:\n(Intercept)      Solar.R         Wind         Temp  \n  -64.34208      0.05982     -3.33359      1.65209  \n\n\n\nHolding all other variables constant:\n\nFor every 1 unit increase in Solar.R, Ozone increases by a mean value of 0.06 ppb.\nFor every 1 degree increase in Temp, Ozone increases by a mean value of 1.65 ppb.\nFor every 1 unit increase in Wind, Ozone decreases by a mean value of 3.33 ppb.\n\n\n\n\n\n\n\n\n\nCaution\n\n\nIf the model is not “valid”, then the partial regression coefficients are not meaningful."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#line",
    "href": "lectures/L11/lecture-11.html#line",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "LINE",
    "text": "LINE\nAs with Simple Linear Regression, we need to check the assumptions of the model (LINE):\n\nLinearity: the relationships between the response and the predictors are all linear.\nIndependence: the observations are independent of each other.\nNormality: the residuals are normally distributed.\nEqual variance: the variance of the residuals is constant."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#recall",
    "href": "lectures/L11/lecture-11.html#recall",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Recall",
    "text": "Recall\nIn SLR, the model is made up of the deterministic component (the line) and the random component (the error term).\n Y_i = \\color{seagreen}{\\beta_0 + \\beta_1 x_i} + \\color{firebrick}\\epsilon_i \n\nThis is the same for MLR:  Y_i = \\color{seagreen}{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k} + \\color{firebrick}{\\epsilon_i} \nSince only the error term is random, the assumptions are still about the error term, \\hat\\epsilon, which is simple to assess!"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions-of-mlr",
    "href": "lectures/L11/lecture-11.html#assumptions-of-mlr",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions of MLR",
    "text": "Assumptions of MLR"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#transformation-using-log",
    "href": "lectures/L11/lecture-11.html#transformation-using-log",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Transformation using log()",
    "text": "Transformation using log()\nSome evidence of nonlinearity in the diagnostic plots. Transform and re-check assumptions."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#results",
    "href": "lectures/L11/lecture-11.html#results",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Results",
    "text": "Results\n\n\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nAll three predictors are statistically significant (p &lt; .001).\nThe model explains 66% of variance (r2 = 0.66)."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#results-compared-to-slr",
    "href": "lectures/L11/lecture-11.html#results-compared-to-slr",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Results compared to SLR",
    "text": "Results compared to SLR\n\n\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAll three predictors are statistically significant (p &lt; .001).\nThe model explains 66% of variance (r2 = 0.66 vs. 0.48 in SLR)."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#coefficients",
    "href": "lectures/L11/lecture-11.html#coefficients",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Coefficients",
    "text": "Coefficients\nAll three predictors are statistically significant (p &lt; .001).\n\nFor every 1 unit increase in Solar.R, log(Ozone) increases by a mean value of 0.0025 ppb, holding all other variables constant.\nFor every 1 unit increase in Wind, log(Ozone) decreases by a mean value of 0.062 ppb, holding all other variables constant.\nFor every 1 degree increase in Temp, log(Ozone) increases by a mean value of 0.049 ppb, holding all other variables constant."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#residual-standard-error",
    "href": "lectures/L11/lecture-11.html#residual-standard-error",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Residual standard error",
    "text": "Residual standard error\n\nOn average, the model predicts log(Ozone) within 0.51 ppb of the true value. Not bad?\n\n\n\n\n[1] 1.665291\n\n\n\nOn average, the model predicts Ozone within 1.6652912 ppb of the true value.\nNumber of observations = degrees of freedom (107) + number of parameters in the model (4) = 111."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#r-squared",
    "href": "lectures/L11/lecture-11.html#r-squared",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "R-squared",
    "text": "R-squared\nIf there are &gt;1 predictors, use the Adjusted R-Squared as it penalises the model for having more predictors that are not useful."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#f-stat",
    "href": "lectures/L11/lecture-11.html#f-stat",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "F-stat",
    "text": "F-stat\n\nThe F-statistic tests the null hypothesis that all the regression coefficients are equal to zero, i.e. H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0.\nAs a ratio, it tells us how much better the model is than the null model (i.e. a model with no predictors).\nIf the p-value is less than our specified critical value (e.g. 0.05), we reject the null hypothesis and conclude that the current model is better than the null model."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#reporting",
    "href": "lectures/L11/lecture-11.html#reporting",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Reporting",
    "text": "Reporting\nSolar radiation, wind speed and temperature are significant predictors of Ozone concentration (p &lt; 0.001) with the model accounting for 66% of the variation in weight."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#data",
    "href": "lectures/L11/lecture-11.html#data",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Data",
    "text": "Data\nData from the UCI Machine Learning Repository.\n\n\nRows: 4,177\nColumns: 9\n$ sex      &lt;chr&gt; \"M\", \"M\", \"F\", \"M\", \"I\", \"I\", \"F\", \"F\", \"M\", \"F\", \"F\", \"M\", \"…\n$ length   &lt;dbl&gt; 0.455, 0.350, 0.530, 0.440, 0.330, 0.425, 0.530, 0.545, 0.475…\n$ diameter &lt;dbl&gt; 0.365, 0.265, 0.420, 0.365, 0.255, 0.300, 0.415, 0.425, 0.370…\n$ height   &lt;dbl&gt; 0.095, 0.090, 0.135, 0.125, 0.080, 0.095, 0.150, 0.125, 0.125…\n$ whole    &lt;dbl&gt; 0.5140, 0.2255, 0.6770, 0.5160, 0.2050, 0.3515, 0.7775, 0.768…\n$ shucked  &lt;dbl&gt; 0.2245, 0.0995, 0.2565, 0.2155, 0.0895, 0.1410, 0.2370, 0.294…\n$ viscera  &lt;dbl&gt; 0.1010, 0.0485, 0.1415, 0.1140, 0.0395, 0.0775, 0.1415, 0.149…\n$ shell    &lt;dbl&gt; 0.150, 0.070, 0.210, 0.155, 0.055, 0.120, 0.330, 0.260, 0.165…\n$ rings    &lt;dbl&gt; 15, 7, 9, 10, 7, 8, 20, 16, 9, 19, 14, 10, 11, 10, 10, 12, 7,…"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#preview",
    "href": "lectures/L11/lecture-11.html#preview",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Preview",
    "text": "Preview"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#what-we-did",
    "href": "lectures/L11/lecture-11.html#what-we-did",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "What we did",
    "text": "What we did\n\nFitted a model to predict the whole weight of abalone from other measured variables.\nPerformed a transformation of the response variable to improve model fit.\nChecked the assumptions of the model.\nInterpreted the model coefficients.\nInterpreted the model fit."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-problem-with-using-too-many-predictors",
    "href": "lectures/L11/lecture-11.html#the-problem-with-using-too-many-predictors",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The problem with using too many predictors",
    "text": "The problem with using too many predictors\n\nThe more predictors you add, the better the model fits the data.\nHowever, the model may not be able to generalise to new data: overfitting.\n\n\n\n\n\n\n\n\n\n\n\nModel\nr.squared\nadj.r.squared\n\n\n\n\nsqrt(whole) ~ shucked\n0.892\n0.891\n\n\nsqrt(whole) ~ shucked + shell\n0.952\n0.951\n\n\nsqrt(whole) ~ height + shucked + shell\n0.963\n0.962\n\n\nsqrt(whole) ~ length + height + shucked + shell\n0.982\n0.981\n\n\nsqrt(whole) ~ length + height + shucked + shell + rings\n0.982\n0.981\n\n\nsqrt(whole) ~ length + height + shucked + viscera + shell + rings\n0.982\n0.981\n\n\nsqrt(whole) ~ .\n0.982\n0.981"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-r2-value",
    "href": "lectures/L11/lecture-11.html#the-r2-value",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The r2 value",
    "text": "The r2 value\nThe R-squared value is the proportion of variance explained by the model.\n r^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \nThe adjusted R-squared value is the proportion of variance explained by the model, adjusted for the number of predictors.\n r^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} \nwhere n is the number of observations and p is the number of predictors."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#full-model-vs-reduced-model",
    "href": "lectures/L11/lecture-11.html#full-model-vs-reduced-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Full model vs reduced model",
    "text": "Full model vs reduced model\n\n\n\n\n\nCall:\nlm(formula = sqrt(whole) ~ ., data = abalone)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.218383 -0.016249  0.000771  0.020543  0.105263 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.027849   0.036151  -0.770 0.443065    \nlength       0.959033   0.296239   3.237 0.001678 ** \ndiameter    -0.024686   0.377611  -0.065 0.948019    \nheight       0.969022   0.265067   3.656 0.000427 ***\nshucked      0.317776   0.055354   5.741 1.20e-07 ***\nviscera      0.107616   0.104461   1.030 0.305614    \nshell        0.433048   0.095434   4.538 1.72e-05 ***\nrings        0.001984   0.001800   1.103 0.273097    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03903 on 92 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9811 \nF-statistic: 735.2 on 7 and 92 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCall:\nlm(formula = sqrt(whole) ~ shell + height + diameter, data = abalone)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.149252 -0.030922 -0.004514  0.023821  0.160182 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.13051    0.03854  -3.386 0.001028 ** \nshell        0.56407    0.09945   5.672 1.49e-07 ***\nheight       1.33325    0.34613   3.852 0.000212 ***\ndiameter     1.62282    0.14862  10.919  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05209 on 96 degrees of freedom\nMultiple R-squared:  0.9674,    Adjusted R-squared:  0.9663 \nF-statistic: 948.5 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nIs the 0.015 improvement in the adjusted R-squared – an extra 1.5% of the variance explained – worth the extra predictors?\nRecall: principle of parsimony - the simplest model that explains the data is the best.\nBut how do we know which predictors to keep?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#model-selection",
    "href": "lectures/L11/lecture-11.html#model-selection",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Model selection",
    "text": "Model selection\n\nCovered in second year (ENVX2001).\nUsing techniques of stepwise regression, we can select the best model from a set of “candidate” models.\nIf we have non-significant predictors, we can consider the effect of removing them from the model (partial F-test).\nAim is to achieve the best balance between model fit and model complexity."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#summary",
    "href": "lectures/L11/lecture-11.html#summary",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Summary",
    "text": "Summary\n\nMLR is an extension of SLR to include more than one predictor.\n\nInstead of a line, we are fitting a “hyperplane” i.e. multiple dimensions.\nHowever, the principles are the same: we are still trying to minimise the sum of squared residuals.\nAssumptions of MLR are the same as SLR.\nInstead of the multiple R-squared value, we use the adjusted R-squared value to assess model fit.\n\n\n\n\nFollow the rules of parsimony: the simplest model that explains the data is the best, given similar model fit.\n\nConsider the effect of removing non-significant predictors from the model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX1002 – Introduction to statistical methods",
    "section": "",
    "text": "Module 1 – describing data\n\nLecture 01 – Introduction & the scientific method\nLecture 02 – Exploring data\nLecture 03 – Probability distributions\nLecture 04 – Sampling distributions\n\nModule 2 – inference\n\nLecture 05 – Hypothesis testing & one-sample t-tests\nLecture 06 – Two-sample t-tests\nLecture 07 – Non-parametric tests\nLecture 08 – Bootstrapping | Revision\n\nModule 3 – modelling\n\nLecture 09 – Describing relationships\nLecture 10 – Linear functions\nLecture 11 – Linear functions - multiple predictors\nLecture 12 – Non-linear functions",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Home**"
    ]
  },
  {
    "objectID": "lectures/L11/index.html",
    "href": "lectures/L11/index.html",
    "title": "Lecture 11",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L11 -- Multiple predictors"
    ]
  },
  {
    "objectID": "lectures/L10/index.html",
    "href": "lectures/L10/index.html",
    "title": "Lecture 10",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L10 -- Linear functions"
    ]
  },
  {
    "objectID": "lectures/L03/index.html",
    "href": "lectures/L03/index.html",
    "title": "Lecture 03",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L03 -- Probability distributions"
    ]
  },
  {
    "objectID": "lectures/L04/index.html",
    "href": "lectures/L04/index.html",
    "title": "Lecture 04",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "lectures/L05/index.html",
    "href": "lectures/L05/index.html",
    "title": "Lecture 05",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "lectures/L02/index.html",
    "href": "lectures/L02/index.html",
    "title": "Lecture 02",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L02 -- Exlploring data"
    ]
  },
  {
    "objectID": "lectures/L12/index.html",
    "href": "lectures/L12/index.html",
    "title": "Lecture 12",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L12 -- Non-linear functions"
    ]
  },
  {
    "objectID": "lectures/L09/lecture-09.html#about-me",
    "href": "lectures/L09/lecture-09.html#about-me",
    "title": "Topic 9 – Describing relationships",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\n\nLecturer in Agricultural Data Science\nSpatial modelling and mapping, Soil science, Precision Agriculture\nThis year back to being a student again, GradCert (Higher Education)\n\n\n\n\n\n\nNarrabri cotton field"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#learning-outcomes",
    "href": "lectures/L09/lecture-09.html#learning-outcomes",
    "title": "Topic 9 – Describing relationships",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLO1. Demonstrate proficiency in utilizing R and Excel to effectively explore and describe data sets in the life sciences.\nLO2. Evaluate and interpret different types of data in the natural sciences by visualising probability distributions and calculating probabilities using RStudio and Excel.\nLO3. Apply parametric and non-parametric statistical inference methods to experimental data using RStudio and effectively interpret and communicate the results in the context of the data.\nLO4. Apply both linear and non-linear models to describe relationships between variables using RStudio and Excel, demonstrating creativity in developing models that effectively represent complex data patterns.\nLO5. Articulate statistical and modelling results clearly and convincingly in both written reports and oral presentations, working effectively as an individual and collaboratively in a team, showcasing the ability to convey complex information to varied audiences."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#learning-outcomes-1",
    "href": "lectures/L09/lecture-09.html#learning-outcomes-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLO1. Demonstrate proficiency in utilizing R and Excel to effectively explore and describe data sets in the life sciences.\nLO2. Evaluate and interpret different types of data in the natural sciences by visualising probability distributions and calculating probabilities using RStudio and Excel.\nLO3. Apply parametric and non-parametric statistical inference methods to experimental data using RStudio and effectively interpret and communicate the results in the context of the data.\nLO4. Apply both linear and non-linear models to describe relationships between variables using RStudio and Excel, demonstrating creativity in developing models that effectively represent complex data patterns.\nLO5. Articulate statistical and modelling results clearly and convincingly in both written reports and oral presentations, working effectively as an individual and collaboratively in a team, showcasing the ability to convey complex information to varied audiences."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#module-overview",
    "href": "lectures/L09/lecture-09.html#module-overview",
    "title": "Topic 9 – Describing relationships",
    "section": "Module overview",
    "text": "Module overview\n\nWeek 9. Describing relationships\n\nCorrelation → calculation, interpretation, things to watch out for\nRegression → Why do we care? model structure, model fitting\n\nWeek 10. Linear functions\n\nIs the model worth fitting? → Assumptions, hypothesis testing\nHow good is the model? → Measures of model fit\n\nWeek 11. Linear functions - multiple predictors\n\nParsimonious models\nIntroduction to Multiple Linear Regression (MLR) modelling\nAssumptions and interpretation\n\nWeek 12. Nonlinear functions\n\nCommon nonlinear functions\nTransformations\nPerforming nonlinear regression"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#module-overview-1",
    "href": "lectures/L09/lecture-09.html#module-overview-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Module overview",
    "text": "Module overview\n\nWeek 9. Describing relationships\n\nCorrelation → calculation, interpretation, things to watch out for\nRegression → Why do we care? model structure, model fitting"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#least-squares-correlation-and-astronomy",
    "href": "lectures/L09/lecture-09.html#least-squares-correlation-and-astronomy",
    "title": "Topic 9 – Describing relationships",
    "section": "Least squares, correlation and astronomy",
    "text": "Least squares, correlation and astronomy\n\nMethod of least squares first published paper by Adrien-Marie Legendre in 1805\nTechnique of least squares used by Carl Friedrich Gauss in 1809 to fit a parabola to the orbit of the asteroid Ceres\nModel fitting first published by Francis Galton in 1886 to the problem of predicting the height of a child from the height of the parents\nKarl Pearson developed the correlation coefficient in 1800s based on the work by Francis Galton\n\n\n\n\n\n\n\nNote\n\n\nMany other people contributed to the development of regression analysis, but these three are the “most” well-known."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#galtons-data",
    "href": "lectures/L09/lecture-09.html#galtons-data",
    "title": "Topic 9 – Describing relationships",
    "section": "Galton’s data",
    "text": "Galton’s data\n\n\n# A tibble: 928 × 2\n   parent child\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   70.5  61.7\n 2   68.5  61.7\n 3   65.5  61.7\n 4   64.5  61.7\n 5   64    61.7\n 6   67.5  62.2\n 7   67.5  62.2\n 8   67.5  62.2\n 9   66.5  62.2\n10   66.5  62.2\n# ℹ 918 more rows\n\n\n\n928 children of 205 pairs of parents\nHeight of parents and children measured in inches\nSize classes were binned (hence data looks discrete)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#why-do-we-care",
    "href": "lectures/L09/lecture-09.html#why-do-we-care",
    "title": "Topic 9 – Describing relationships",
    "section": "Why do we care?",
    "text": "Why do we care?\n\nCorrelations are useful for describing relationships between two continuous variables\n\nDirection:\n\npositive - both variables increase together\nnegative - one variable increases as the other decreases\n\nStrength: weak, moderate, strong – subjective, but useful for describing the relationship"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#what-can-correlation-analysis-be-used-for",
    "href": "lectures/L09/lecture-09.html#what-can-correlation-analysis-be-used-for",
    "title": "Topic 9 – Describing relationships",
    "section": "What can correlation analysis be used for?",
    "text": "What can correlation analysis be used for?\n\nDescribing the possible linear relationship between two variables\nUsed extensively in exploratory data analysis: because it is fast and easy to calculate."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#different-types-of-correlation-coeefficients",
    "href": "lectures/L09/lecture-09.html#different-types-of-correlation-coeefficients",
    "title": "Topic 9 – Describing relationships",
    "section": "Different types of correlation coeefficients",
    "text": "Different types of correlation coeefficients\n\nParametric (normally distributed data):\n\nPearson correlation coefficient\n\nmost commonly used\n\n\nNon-parametric (non-normally distributed data):\n\nSpearman rank correlation coefficient\nKendall’s tau\nmore conservative i.e. values are often smaller, but more robust to outliers"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#pearson-correlation-coefficient",
    "href": "lectures/L09/lecture-09.html#pearson-correlation-coefficient",
    "title": "Topic 9 – Describing relationships",
    "section": "Pearson correlation coefficient",
    "text": "Pearson correlation coefficient\nFormula:\n r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \n\n\n\n\n\n\n\n\n\ncovariance divided by the product of the standard deviations"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#luckily-we-have-excel-and-r",
    "href": "lectures/L09/lecture-09.html#luckily-we-have-excel-and-r",
    "title": "Topic 9 – Describing relationships",
    "section": "Luckily we have Excel and R",
    "text": "Luckily we have Excel and R\n\nExcel: =CORREL() formula, or use the analysis toolpak\nR: cor() function\n\n\n\n[1] 0.4587624\n\n\nWe can also visually inspect the relationship between the two variables using a scatterplot:\n\n… but this is not a very good way to assess the strength of the relationship between the two variables."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#interpretation-strong",
    "href": "lectures/L09/lecture-09.html#interpretation-strong",
    "title": "Topic 9 – Describing relationships",
    "section": "Interpretation: strong",
    "text": "Interpretation: strong\n\nA strong relationship is one where the correlation coefficient is close to 1 or -1."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#interpretation-weak",
    "href": "lectures/L09/lecture-09.html#interpretation-weak",
    "title": "Topic 9 – Describing relationships",
    "section": "Interpretation: weak",
    "text": "Interpretation: weak\n\nA weak or nonexistent relationship is one where the correlation coefficient is close to 0."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#interpretation-numbers",
    "href": "lectures/L09/lecture-09.html#interpretation-numbers",
    "title": "Topic 9 – Describing relationships",
    "section": "Interpretation: numbers",
    "text": "Interpretation: numbers\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\n\nIt is enough to deduce the strength of the relationship from the correlation coefficient value(s) alone.\nNot reliable for inference about the relationship between variables. For this you must visualise."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#anscombes-quartet",
    "href": "lectures/L09/lecture-09.html#anscombes-quartet",
    "title": "Topic 9 – Describing relationships",
    "section": "Anscombe’s quartet",
    "text": "Anscombe’s quartet\n\nAll of these data have a correlation coefficient of about 0.8."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#datasaurus-dozen",
    "href": "lectures/L09/lecture-09.html#datasaurus-dozen",
    "title": "Topic 9 – Describing relationships",
    "section": "Datasaurus Dozen",
    "text": "Datasaurus Dozen\n\nAll of these data have a correlation coefficient close to zero!"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#what-comes-after-correlation",
    "href": "lectures/L09/lecture-09.html#what-comes-after-correlation",
    "title": "Topic 9 – Describing relationships",
    "section": "What comes after correlation?",
    "text": "What comes after correlation?\n\nFirst, a summary:\n\nCorrelation analysis is a fast and easy way to describe the possible linear relationship between two variables\nBut, we can’t infer causation: domain knowledge is required to interpret the results\n\nAre we expecting a relationship between the two variables?\nDo you have a hypothesis about the relationship between the two variables?\n\n\n\nIf we have a hypothesis about the relationship between two variables, we can use regression analysis to test it."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#why-regression",
    "href": "lectures/L09/lecture-09.html#why-regression",
    "title": "Topic 9 – Describing relationships",
    "section": "Why regression?",
    "text": "Why regression?\n\nDescribe the relationship between two variables\nWhat is the relationship between a response variable Y and a predictor variable x?\nExplain the relationship between two variables\nHow much variation in Y can be explained by a relationship with x?\nPredict the value of a response variable\nWhat is the value of Y for a given value of x?"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#a-gateway-to-the-world-of-modelling",
    "href": "lectures/L09/lecture-09.html#a-gateway-to-the-world-of-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "A gateway to the world of modelling",
    "text": "A gateway to the world of modelling\nMany types of regression models exist:\n\nSimple linear regression\nMultiple linear regression\nNon-linear regression, using functions such as polynomials, exponentials, logarithms, etc.\n\n\nAsking ChatGPT for help with the next slide:\n\nUsing R code, can you generate some data that is useful to demonstrate simple linear regression, multiple linear regression, polynomial, exponential and logarithmic regressions in ggplot2?\n\n\nSure! Here’s an example code that generates a sample dataset and visualizes it using ggplot2 library in R."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#visualising-regression-models",
    "href": "lectures/L09/lecture-09.html#visualising-regression-models",
    "title": "Topic 9 – Describing relationships",
    "section": "Visualising regression models",
    "text": "Visualising regression models"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#visualising-regression-models-1",
    "href": "lectures/L09/lecture-09.html#visualising-regression-models-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Visualising regression models",
    "text": "Visualising regression models"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example-climate-change-modelling",
    "href": "lectures/L09/lecture-09.html#example-climate-change-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "Example: climate change modelling",
    "text": "Example: climate change modelling\n\nSource: https://science2017.globalchange.gov/chapter/4/"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example-covid-19-transmission-modelling",
    "href": "lectures/L09/lecture-09.html#example-covid-19-transmission-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "Example: COVID-19 transmission modelling",
    "text": "Example: COVID-19 transmission modelling\n\nSource: https://www.nature.com/articles/s41598-021-84893-4/figures/1"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#defining-a-linear-relationship",
    "href": "lectures/L09/lecture-09.html#defining-a-linear-relationship",
    "title": "Topic 9 – Describing relationships",
    "section": "Defining a linear relationship",
    "text": "Defining a linear relationship\n\nPearson correlation coefficient measures the linear correlation between two variables\nDoes not distinguish different patterns of association, only the strength of the association\n\n\n\nNot quite usable for predictive modelling, or for inference about the relationship between variables"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#simple-linear-regression-modelling",
    "href": "lectures/L09/lecture-09.html#simple-linear-regression-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "Simple linear regression modelling",
    "text": "Simple linear regression modelling\nWe want to predict an outcome Y based on a predictor x for i number of observations:\nY_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}\nwhere\n\\epsilon_i \\sim N(0, \\sigma^2)\n\nY_i, the response, is an observed value of the dependent variable.\n\\beta_0, the constant, is the population intercept and is fixed.\n\\beta_1 is the population slope parameter, and like \\beta_0, is also fixed.\n\\epsilon_i is the error associated with predictions of y_i, and unlike \\beta_0 or \\beta_1, it is not fixed.\n\n\n\n\n\n\n\n\nNote\n\n\nWe tend to associate \\epsilon_i with the residual, which is a positive or negative difference from the “predicted” response, rather than error itself which is a difference from the true response"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#in-pictures",
    "href": "lectures/L09/lecture-09.html#in-pictures",
    "title": "Topic 9 – Describing relationships",
    "section": "In pictures…",
    "text": "In pictures…"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#interpreting-the-relationship",
    "href": "lectures/L09/lecture-09.html#interpreting-the-relationship",
    "title": "Topic 9 – Describing relationships",
    "section": "Interpreting the relationship",
    "text": "Interpreting the relationship\nY_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}\nBasically, a deterministic straight line equation y = c + mx, with added random variation that is normally distributed\n\n\nResponse = Prediction + Error\nResponse = Signal + Noise\nResponse = Model + Unexplained\nResponse = Deterministic + Random\nResponse = Explainable + Everything else\n\n\n\n\nY = f(x)\nDependent variable = f(Independent variable)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#the-variation-in-the-response",
    "href": "lectures/L09/lecture-09.html#the-variation-in-the-response",
    "title": "Topic 9 – Describing relationships",
    "section": "The variation in the response",
    "text": "The variation in the response"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#model-fitting",
    "href": "lectures/L09/lecture-09.html#model-fitting",
    "title": "Topic 9 – Describing relationships",
    "section": "Model fitting",
    "text": "Model fitting\nTwo approaches; analytical and numerical:\nAnalytical: equation(s) used directly to find solution, e.g. estimate parameters that minimise residual sum of squares\nNumerical: computer uses “random guesses” to find set of parameters to that minimises objective function, in this case residual sum of squares\nIn this week’s practical we will use Solver in Excel to numerically fit linear models."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#hypothesis-testing",
    "href": "lectures/L09/lecture-09.html#hypothesis-testing",
    "title": "Topic 9 – Describing relationships",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nHow does our null (H_0: \\beta_1=0) model compare to the linear (H_0: \\beta_1 \\neq 0) model?\nNull model thinks the data can be summarised by the mean \\bar{y}, and the linear model thinks the data can be summarised by the estimate \\hat{y}."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#simple-linear-regression-in-one-step",
    "href": "lectures/L09/lecture-09.html#simple-linear-regression-in-one-step",
    "title": "Topic 9 – Describing relationships",
    "section": "Simple linear regression in one step",
    "text": "Simple linear regression in one step\nDone!\n\nAnd then we can use summary() to get a summary of the model:\n\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#summary",
    "href": "lectures/L09/lecture-09.html#summary",
    "title": "Topic 9 – Describing relationships",
    "section": "Summary",
    "text": "Summary\n\nCorrelation is a measure of the strength of the linear relationship between two variables.\nCorrelation coefficient provides information on strength and direction of the linear relationship.\nCorrelation \\neq causation.\nRegression models the relationship between a dependent variable and independent variable(s) - fits a line or function to the data.\n\nUsing the method of least squares, we can find the line that minimises the sum of the squared residuals.\n\nBoth Excel and R can be used to fit regression models."
  },
  {
    "objectID": "lectures/L07/index.html",
    "href": "lectures/L07/index.html",
    "title": "Lecture 07",
    "section": "",
    "text": "Lecture 07a\nFull Screen | PDF\n\n\n\nLecture 07b\nFull Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L07 -- Non-parametric tests"
    ]
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#parametric-and-non-parametric-alternatives",
    "href": "lectures/L07/lecture-07b-chisq.html#parametric-and-non-parametric-alternatives",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Parametric and non-parametric alternatives",
    "text": "Parametric and non-parametric alternatives\n\n\nSo far, all of our techniques have been aimed at comparing means/medians of continuous variables.\nThe assumption of normality underpins these techniques – if the data is not normally distributed, we have alternatives like transforming the data or using non-parametric tests.\nDoes this apply to all data?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#a-rational-assumption",
    "href": "lectures/L07/lecture-07b-chisq.html#a-rational-assumption",
    "title": "Topic 7 – Chi-squared tests",
    "section": "A rational assumption?",
    "text": "A rational assumption?\n\n\nAre all randomly sampled data normally distributed?\nRecall probability distributions (Week 3) – normal distribution is just one of several possible distributions of data.\nIt turns out that there are non-parametric techniques that are not just alternatives of parametric tests, but better suited for certain types of data."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#what-are-categorical-variables",
    "href": "lectures/L07/lecture-07b-chisq.html#what-are-categorical-variables",
    "title": "Topic 7 – Chi-squared tests",
    "section": "What are categorical variables?",
    "text": "What are categorical variables?\nConsider the following questions:\n\n\nA biologist claims that when sampling the Australian Botanical Gardens for butterflies, the ratio of the most dominant colours (red, blue, green, and yellow) is equal. How would you determine if the biologist’s claim is true?\n\n\nA study was conducted on a population of deer to see if there is a relationship between their age group (young, adult, old) and their preferred type of vegetation (grass, leaves, bark). Is age group of the deer independent of their vegetation preference?\n\n\n\nHow would you measure these variables, and what sort of summary statistics can you use?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#visualising-categorical-variables",
    "href": "lectures/L07/lecture-07b-chisq.html#visualising-categorical-variables",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Visualising categorical variables",
    "text": "Visualising categorical variables\n\nWe can only count the number of times a particular category occurs, or the proportion of the total that each category represents."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#types-of-categorical-data",
    "href": "lectures/L07/lecture-07b-chisq.html#types-of-categorical-data",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Types of categorical data",
    "text": "Types of categorical data\n\nRather than measuring a continuous variable, we are interested in counting the number of times a particular category occurs, or the proportion of the total that each category represents.\nThese are known as categorical variables.\nGenerally 3 types of categorical data:\n\nNominal: Categories have no inherent order (e.g. colours, breeds of dogs).\nOrdinal: Categories have an inherent order (e.g. Likert scales, grades).\nBinary: Only two mutually exclusive categories (e.g rain or no rain)."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#the-chi-squared-test",
    "href": "lectures/L07/lecture-07b-chisq.html#the-chi-squared-test",
    "title": "Topic 7 – Chi-squared tests",
    "section": "The chi-squared test",
    "text": "The chi-squared test\n\nThe chi-squared test is perhaps one of the most prominent examples of non-parametric tests.\nDeveloped by Karl Pearson in 1900, pronounced “ki” as in “kite”, uses the Greek letter \\chi.\nActually derived from the normal distribution: a chi-squared distribution is the sum of squared standard normal deviates – essentially a folded-over and stretched out normal."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#chi-squared-distribution-vs-normal-distribution",
    "href": "lectures/L07/lecture-07b-chisq.html#chi-squared-distribution-vs-normal-distribution",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Chi-squared distribution vs normal distribution",
    "text": "Chi-squared distribution vs normal distribution\n\nHow is the chi-squared distribution used in hypothesis testing?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#butterflies-data",
    "href": "lectures/L07/lecture-07b-chisq.html#butterflies-data",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Butterflies data",
    "text": "Butterflies data\n\nA biologist claims that when sampling the Australian Botanical Gardens for butterflies, the ratio of the most dominant colours (red, blue, green, and yellow) is equal. How would you determine if the biologist’s claim is true?\n\nSuppose we have the following data on the colours of butterflies after randomly sampling 200 of them:\n\n\n\n\n\ncolor\ncount\n\n\n\n\nred\n48\n\n\nblue\n62\n\n\ngreen\n56\n\n\nyellow\n34"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#testing-the-claim",
    "href": "lectures/L07/lecture-07b-chisq.html#testing-the-claim",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Testing the claim",
    "text": "Testing the claim\n\nIf the biologist’s claim is true, we would expect the number of butterflies of each colour to be equal.\nIf 200 butterflies were sampled, we would expect 50 of each colour, as the expected frequency of each colour is 200 \\times 0.25 = 50.\n\nTherefore:\n\n\n\n\n\ncolor\ncount\nexpected\n\n\n\n\nred\n48\n50\n\n\nblue\n62\n50\n\n\ngreen\n56\n50\n\n\nyellow\n34\n50"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#test-statistic",
    "href": "lectures/L07/lecture-07b-chisq.html#test-statistic",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Test statistic",
    "text": "Test statistic\nThe test statistic for the chi-squared test is calculated as:\n \\chi^2 = \\sum \\frac{(O - E)^2}{E} \nwhere O is the observed frequency and E is the expected frequency.\n\nSo for the butterfly data:\n\n\n[1] 8.8\n\n\nThis is the test statistic for one sample. How do we interpret this value?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#simulate-the-null-distribution",
    "href": "lectures/L07/lecture-07b-chisq.html#simulate-the-null-distribution",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Simulate the null distribution",
    "text": "Simulate the null distribution\nUnder the null hypothesis, the observed frequencies are equal to the expected frequencies i.e. the biologist’s claim is true.\nSuppose we repeat the sampling process many times, assuming the null hypothesis is true, each time calculating the test statistic. What would the distribution of test statistics look like?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#what-does-our-test-statistic-tell-us",
    "href": "lectures/L07/lecture-07b-chisq.html#what-does-our-test-statistic-tell-us",
    "title": "Topic 7 – Chi-squared tests",
    "section": "What does our test statistic tell us?",
    "text": "What does our test statistic tell us?\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.034\n\n\nComparing our test statistic to the simulated distribution, we can see that the 0.03% of the simulated values are greater than our test statistic. What does this tell us?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#a-chi2-test",
    "href": "lectures/L07/lecture-07b-chisq.html#a-chi2-test",
    "title": "Topic 7 – Chi-squared tests",
    "section": "A \\chi^2 test",
    "text": "A \\chi^2 test\nA chi-squared distribution allows us to perform the same hypothesis test without the need for simulation."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#conclusion",
    "href": "lectures/L07/lecture-07b-chisq.html#conclusion",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Conclusion?",
    "text": "Conclusion?\nThe results of the simulation suggest that the observed frequencies of butterfly colours are significantly different from the expected frequencies, and we can reject the biologist’s claim."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#more-on-the-chi-squared-distribution",
    "href": "lectures/L07/lecture-07b-chisq.html#more-on-the-chi-squared-distribution",
    "title": "Topic 7 – Chi-squared tests",
    "section": "More on the chi-squared distribution",
    "text": "More on the chi-squared distribution\n\nThe chi-squared distribution is non-symmetric and right-skewed.\nThe shape of the distribution is determined by the degrees of freedom, calculated as the number of categories minus 1.\nAs the degrees of freedom increase, the chi-squared distribution approaches a normal distribution due to the central limit theorem."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#definitions",
    "href": "lectures/L07/lecture-07b-chisq.html#definitions",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Definitions",
    "text": "Definitions\n\nChi-squared distribution: a distribution derived from the normal distribution that allows us to determine whether the observed frequencies of a categorical variable differ from the expected frequencies.\nContingency table: a table that displays the frequency of observations for two or more categorical variables.\nExpected frequency: the frequency that we would expect to observe if the null hypothesis is true.\nObserved frequency: the frequency that we actually observe.\nTest statistic: a measure of how much the observed frequencies differ from the expected frequencies, standardised by the expected frequencies."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#types-of-chi-squared-tests",
    "href": "lectures/L07/lecture-07b-chisq.html#types-of-chi-squared-tests",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Types of chi-squared tests",
    "text": "Types of chi-squared tests\n\nGoodness-of-fit test: used to determine whether the observed frequencies of a categorical variable differ from the expected frequencies.\nTest of independence: used to determine whether there is a relationship between two or more categorical variables.\nTest of homogeneity: used to determine whether the distribution of a categorical variable is the same across different groups."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#assumptions",
    "href": "lectures/L07/lecture-07b-chisq.html#assumptions",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe chi-squared test is a non-parametric test, so it does not rely on the assumption of normality. However, it does have some assumptions:\n\nIndependence: the observations are independent.\nSample size: the expected frequency of each category is at least 1, and no more than 20% of the expected frequencies are less than 5.\n\n\nThe sample size assumption ensures that the chi-squared distribution is a good approximation of the normal distribution."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#hypothesis",
    "href": "lectures/L07/lecture-07b-chisq.html#hypothesis",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nNull hypothesis: the observed proportion of butterfly colours are equal to the expected proportions of 0.25 each.\nAlternative hypothesis: the observed proportions are not equal.\n\n H_0: p_1 = p_2 = p_3 = p_4 = 0.25   H_1: \\text{at least one } p_i \\neq 0.25"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#test-statistic-and-check-assumptions-in-r",
    "href": "lectures/L07/lecture-07b-chisq.html#test-statistic-and-check-assumptions-in-r",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Test statistic and check assumptions (in R)",
    "text": "Test statistic and check assumptions (in R)\nAssumptions\nBy performing the chi-squared test, we can check the assumptions of the test by looking at the calculated frequences in the output:\n\n\n[1] 48 62 56 34\n\n\nTest statistic\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  df$count\nX-squared = 8.8, df = 3, p-value = 0.03207"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#conclusion-1",
    "href": "lectures/L07/lecture-07b-chisq.html#conclusion-1",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Conclusion",
    "text": "Conclusion\nThe results of the chi-squared test suggest that the observed frequencies of butterfly colours are significantly different from the expected frequencies (\\chi^2 = 8.8, df = 3, p &lt; 0.001). We can reject the null hypothesis and conclude that the biologist’s claim is not true.\n\n\n\n\n\n\nNote\n\n\nIf you’re interested, compare this result to the simulation we performed earlier."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#hypothesis-1",
    "href": "lectures/L07/lecture-07b-chisq.html#hypothesis-1",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nNull hypothesis: the age group of the deer is independent of their vegetation preference.\nAlternative hypothesis: the age group of the deer is not independent of their vegetation preference.\n\n H_0: \\text{Age group is independent of vegetation preference} \n\nNo relationship between the two variables\n\n H_1: \\text{Age group is not independent of vegetation preference} \n\nThere is a relationship between the two variables"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#data",
    "href": "lectures/L07/lecture-07b-chisq.html#data",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Data",
    "text": "Data\nSuppose we have the following data on the age group and vegetation preference of 100 deer:\n\n\n      grass leaves bark\nyoung    20     30   10\nadult    10     10   20\nold      10     10   10"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#test-statistic-and-check-assumptions-in-r-1",
    "href": "lectures/L07/lecture-07b-chisq.html#test-statistic-and-check-assumptions-in-r-1",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Test statistic and check assumptions (in R)",
    "text": "Test statistic and check assumptions (in R)\nAssumptions are met as we can see the contingency table in the previous slide.\nTest statistic\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  deer_data\nX-squared = 13.542, df = 4, p-value = 0.008911\n\n\nWe reject the null hypothesis since the p-value is less than 0.05."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#conclusion-2",
    "href": "lectures/L07/lecture-07b-chisq.html#conclusion-2",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Conclusion",
    "text": "Conclusion\nThe results of the chi-squared test suggest that the age group of the deer is not independent of their vegetation preference (\\chi^2 = 12.4, df = 4, p &lt; 0.001). We can reject the null hypothesis and conclude that there is a relationship between the age group of the deer and their vegetation preference."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#mosaic-plots",
    "href": "lectures/L07/lecture-07b-chisq.html#mosaic-plots",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Mosaic plots",
    "text": "Mosaic plots"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#interpretation",
    "href": "lectures/L07/lecture-07b-chisq.html#interpretation",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe area of each rectangle is proportional to the number of observations in that category.\nThe shading of each rectangle indicates the expected frequency of observations in that category.\nThe darker the shading, the greater the difference between the observed and expected frequencies.\nDotted lines indicate independence between the two variables.\nSolid lines indicate dependence between the two variables."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#association-plots",
    "href": "lectures/L07/lecture-07b-chisq.html#association-plots",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Association plots",
    "text": "Association plots"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#interpretation-1",
    "href": "lectures/L07/lecture-07b-chisq.html#interpretation-1",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Interpretation",
    "text": "Interpretation\n\nSize of cells indicate the number of observations in that category.\nThe shadings are made based on the residuals of the chi-squared test (see legend), highlighting which cells contribute most to the chi-squared statistic.\nColour of the shadings indicate whether they are more or less frequent than expected (again, see legend)."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#test-of-homogeneity-vs.-test-of-independence",
    "href": "lectures/L07/lecture-07b-chisq.html#test-of-homogeneity-vs.-test-of-independence",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Test of homogeneity vs. test of independence",
    "text": "Test of homogeneity vs. test of independence\n\nThe test of homogeneity is similar to the test of independence, but is used when we have two or more groups and we want to determine whether the distribution of a categorical variable is the same across different groups.\nIn general, this means that the null hypothesis is stated differently, and the test statistic is calculated in a slightly different way with different degrees of freedom.\nHomogeneity\n\nH_0: The distribution of the categorical variable is the same across different groups.\nH_1: The distribution of the categorical variable is not the same across different groups.\n\nIndependence\n\nH_0: The variables of interest are independent.\nH_1: The variables of interest are not independent."
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#differences-are-subtle",
    "href": "lectures/L07/lecture-07b-chisq.html#differences-are-subtle",
    "title": "Topic 7 – Chi-squared tests",
    "section": "Differences are subtle",
    "text": "Differences are subtle\n\nIn the test of independence, observational units are collected at random from a single population and two (or more) categorical variables are observed for each unit.\nFor the deer example, the experimental design would involve randomly sampling deer and recording their age group and vegetation preference.\n\n\nIs age group independent of vegetation preference?\n\n\nIn the test of homogeneity, the data are collected by randomly sampling from two or more subgroups, and the same categorical variable is observed for each unit.\nFor the deer example, the experimental design would have to be modified to sample the vegetation preference of deer from young, adult, and old populations.\n\n\nIs the distribution of vegetation preference the same if we compare young, adult, and old deer?"
  },
  {
    "objectID": "lectures/L07/lecture-07b-chisq.html#when-to-use-a-chi-squared-test",
    "href": "lectures/L07/lecture-07b-chisq.html#when-to-use-a-chi-squared-test",
    "title": "Topic 7 – Chi-squared tests",
    "section": "When to use a chi-squared test?",
    "text": "When to use a chi-squared test?\n\nThe chi-squared test is not an “alternative” to a parametric test, but is better suited for certain types of data and requires delibarate experimental design that collects data in a certain way.\nIf we have categorical data and we want to determine whether the observed frequencies differ from the expected frequencies, then we can use a chi-squared test."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#recap-assumptions-of-the-two-sample-t-test",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#recap-assumptions-of-the-two-sample-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Recap: Assumptions of the two-sample t-test",
    "text": "Recap: Assumptions of the two-sample t-test\nWith independent samples:\n\nNormality: the data are normally distributed\nHomogeneity of variance (equal variances): the variances of the two groups are equal\n\nWith paired samples:\n\nNormality: the differences between the paired samples are normally distributed\nEqual variances is implied"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#if-we-analyse-the-data-anyway",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#if-we-analyse-the-data-anyway",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "If we analyse the data anyway…",
    "text": "If we analyse the data anyway…\nThe t-test:\n\nmay provide incorrect results as mean and variance calculations depend on normally distributed data.\nmay be less powerful (i.e., less likely to detect a true difference).\nmay be biased (i.e., systematically over- or under-estimating the true difference)."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#what-can-we-do",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#what-can-we-do",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "What can we do?",
    "text": "What can we do?\nThe t-test is quite robust to violations of normality, especially when the sample size is large. However, the assumption of equal variances is more critical – we cannot simply depend on large sample sizes to “fix” the problem.\nOptions include:\n\nTransform the data to normalise the data and/or scale the variance\nUse a Welch’s t-test or a Welch’s ANOVA (limited cases)\nUse a non-parametric test, such as the Mann-Whitney U test or Wilcoxon signed-rank test (paired samples) – however, these tests have less power than the t-test i.e. less likely to detect a true difference."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#is-the-food-collected-by-ants-different-between-two-sites",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#is-the-food-collected-by-ants-different-between-two-sites",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Is the food collected by ants different between two sites?",
    "text": "Is the food collected by ants different between two sites?\nData structure\n\n\nRows: 54\nColumns: 2\n$ Food &lt;dbl&gt; 11.9, 33.3, 4.6, 5.5, 6.2, 11.0, 24.3, 20.7, 5.7, 12.6, 10.2, 4.7…\n$ Tree &lt;fct&gt; Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Ro…\n\n\nWe want to compare the mean biomass of food, collected by ants between the two sites in dry weight (mg) of prey, divided by the total number of ants leaving the tree in 30 minutes."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#visualising-the-data",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#visualising-the-data",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Visualising the data",
    "text": "Visualising the data\n\nDoes this data meet the assumptions of the two-sample t-test?"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#checking-assumptions",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#checking-assumptions",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nWe have some idea that the data may not be normally distributed, but are not quite sure. So let’s check using the Q-Q plot.\n\n\n\nCurvature of the data points away from the line indicates non-normality.\nBoxplots (previous slide) suggest equal variances.\nLet’s transform the data."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#picking-a-transformation",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#picking-a-transformation",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Picking a transformation",
    "text": "Picking a transformation\nWe need to consider the type of data and the shape of its distribution when choosing a transformation. These can be assessed using:\n\n\nHistograms and Q-Q plots to assess normality - DONE\nBox plots to assess homogeneity of variance - DONE\nSkewness and kurtosis to assess the shape of the distribution - NEXT"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#skewness",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#skewness",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Skewness",
    "text": "Skewness\nThe degree of asymmetry in the data distribution when compared to a normal distribution.\n\n\nRepresented by the skewness coefficient (\\gamma_1) and can be positive, negative, or zero.\nSkewness values between -0.5 and 0.5 are considered acceptable (fairly symmetrical).\nNegative skewness indicates a left-skewed distribution, while positive skewness indicates a right-skewed distribution.\nAbove 1 or below -1, the distribution is considered highly skewed."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#example-skewness",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#example-skewness",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Example: skewness",
    "text": "Example: skewness"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#kurtosis",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#kurtosis",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Kurtosis",
    "text": "Kurtosis\nUsed to describe the extreme values (outliers) in the distribution versus the tails.\n\n\nHigh kurtosis (&gt;3) indicates a distribution with heavy tails and a peaked centre. When this happens, we should investigate the data for outliers.\nLow kurtosis (&lt;3) indicates a distribution with light tails and a flat centre. There are fewer to no outliers in the data."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#example-kurtosis",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#example-kurtosis",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Example: kurtosis",
    "text": "Example: kurtosis"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#skewness-and-kurtosis-in-the-ants-data",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#skewness-and-kurtosis-in-the-ants-data",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Skewness and kurtosis in the ants data",
    "text": "Skewness and kurtosis in the ants data\nWith experience we can “eyeball” the data, but we can also calculate the skewness and kurtosis.\n\n\n# A tibble: 2 × 3\n  Tree     skewness kurtosis\n  &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 Rowan       1.04      3.15\n2 Sycamore    0.807     2.63\n\n\n\n\nFrom the results we can see that both sites have a positive skewness. Site Rowan has high kurtosis."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#workflow",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#workflow",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Workflow",
    "text": "Workflow\n\n\nCheck the data for normality and homogeneity of variance (i.e. test assumptions).\nIf the assumptions are violated, consider transforming the data.\nRepeat checks on assumptions. If assumptions are met, proceed with the t-test on the transformed scale. Otherwise, use a different transformation or consider using a non-parametric test.\nInterpret the statistical results and back-transform the results to the original scale (optional but recommended) to aid interpretation."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#picking-a-transformation-1",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#picking-a-transformation-1",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Picking a transformation",
    "text": "Picking a transformation\n\n\nFor positive skewness\n\nSquare root transformation: \\sqrt{x} for skewness between 0.5 and 1 and kurtosis &lt; 3.\nLogarithmic transformation: \\log(x) for skewness &gt; 1 and kurtosis &lt; 3.\nReciprocal transformation: \\frac{1}{x} for skewness &gt; 1 and kurtosis &gt; 3 (quite extreme).\n\n\n\n\nFor negative skewness\n\nThis is rare as most biological data are positively skewed. However, you can try the square x^2 or cube x^3 transformation.\nIf negatively skewed data contains zeros, consider using the log transform and adding a constant to the data before transformation e.g. \\log(x + 1).\n\n\n\n\n\n\n\n\n\nNote\n\n\nThere is also the Box-Cox transformation which informs us of the best transformation to apply to the data without the need to check skewness and kurtosis. This method is not covered in this unit, but you can read more about it here (the simple R version) or here (more detailed mathematical explanation)."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#how-do-we-check-if-the-transformation-worked",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#how-do-we-check-if-the-transformation-worked",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "How do we check if the transformation worked?",
    "text": "How do we check if the transformation worked?\nWe need to apply the transformation to the entire dataset and check the Q-Q plot again."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#checking-skewness-and-kurtosis-after-transformation",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#checking-skewness-and-kurtosis-after-transformation",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Checking skewness and kurtosis after transformation",
    "text": "Checking skewness and kurtosis after transformation\n\n\n# A tibble: 2 × 3\n  Tree      skewness kurtosis\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Rowan    0.271         1.77\n2 Sycamore 0.0000457     1.86"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#performing-the-t-test",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#performing-the-t-test",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Performing the t-test",
    "text": "Performing the t-test\n\n\n\n    Two Sample t-test\n\ndata:  Food_log by Tree\nt = -2.0521, df = 52, p-value = 0.04521\nalternative hypothesis: true difference in means between group Rowan and group Sycamore is not equal to 0\n95 percent confidence interval:\n -0.732858080 -0.008203447\nsample estimates:\n   mean in group Rowan mean in group Sycamore \n              2.287756               2.658287 \n\n\n\n\nHow do we interpret the results?\nEvidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045)."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#back-transforming-the-results",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#back-transforming-the-results",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Back-transforming the results",
    "text": "Back-transforming the results\n\nFor power transformations, we can back-transform the results to the original scale using the inverse function.\nLog transformations are a bit tricky as the inverse function is the exponential function.\n\nFor the natural log transformation which is log() in R, the inverse function is the exponential function: e^x.\nFor the base 10 log transformation which is log10() in R, the inverse function is 10^x."
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#interpretation",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#interpretation",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Interpretation",
    "text": "Interpretation\nBack-transforming mean values\n\n\n[1] 1.448503\n\n\n\nEvidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045).\n\nThe mean biomass of food collected by ants from the Sycamore site (14.3 mg) is 1.4 times greater than the mean biomass of food collected by ants from the Rowan site (9.9 mg).\nBack-transforming confidence intervals\n\n\n[1] 0.4805336 0.9918301\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "lectures/L06/lecture-06b-2s-ttest-2.html#comparing-to-a-test-without-transformation",
    "href": "lectures/L06/lecture-06b-2s-ttest-2.html#comparing-to-a-test-without-transformation",
    "title": "Topic 6 – Two-sample t-tests – Part II",
    "section": "Comparing to a test without transformation",
    "text": "Comparing to a test without transformation\n\n\n\n    Two Sample t-test\n\ndata:  Food by Tree\nt = -1.9217, df = 52, p-value = 0.06013\nalternative hypothesis: true difference in means between group Rowan and group Sycamore is not equal to 0\n95 percent confidence interval:\n -10.4916030   0.2267678\nsample estimates:\n   mean in group Rowan mean in group Sycamore \n              12.27143               17.40385 \n\n\n\n\n\nOriginal mean values:\n\nRowan = 12.3 mg\nSycamore = 17.4 mg\n\nLog-transformed mean values:\n\nRowan = 2.3 lg(mg)\nSycamore = 2.7 lg(mg)\n\nBack-transformed mean values:\n\nRowan = 9.9 mg\nSycamore = 14.3 mg\n\n\nThe original mean values are based on the arithmetic mean, while the log-transformed mean values are based on the geometric mean. The geometric mean is more appropriate for skewed data.\n\n\nOriginal 95% confidence interval:\n\n-10.5 to 0.2 mg\n\nLog-transformed 95% confidence interval:\n\n0.5 to 1 lg(mg)\n\nBack-transformed 95% confidence interval:\n\n1.6 to 2.7 mg\n\n\nThe influence of kurtosis on the 95% confidence interval is evident when comparing the original and back-transformed confidence intervals, as the log transform reduces the effect of outliers on the data."
  },
  {
    "objectID": "lectures/L08/index.html",
    "href": "lectures/L08/index.html",
    "title": "Lecture 08",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L08 -- Bootstrapping"
    ]
  }
]