[
  {
    "objectID": "lectures/L12/lecture-12.html#module-overview",
    "href": "lectures/L12/lecture-12.html#module-overview",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Module overview",
    "text": "Module overview\n\n\n\n\nWeek 9. Describing Relationships\n\nCorrelation (calculation, interpretation)\nRegression (model structure, model fitting\nWhat/when/why/how\n\nWeek 10. Simple Linear Regression\n\nCan we use the model?(assumptions, hypothesis testing)\nHow good is the model?(interpretation, model fit)\n\nWeek 11. Multiple Linear Regression\n\nMultiple Linear Regression (MLR) modelling\nAssumptions, interpretation and the principle of parsimony\n\nWeek 12. Nonlinear Regression\n\nCommon nonlinear functions\nTransformations"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#regressions",
    "href": "lectures/L12/lecture-12.html#regressions",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Regressions",
    "text": "Regressions\nSimple linear regression\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\nIdeal for predicting a continuous response variable from a single predictor variable: “How does \\(y\\) change as \\(x\\) changes, when the relationship is linear?”\nMultiple linear regression\n\\[ Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + \\epsilon_i \\]\n“How does \\(y\\) change as \\(x_1\\), \\(x_2\\), …, \\(x_k\\) change?”\n\nNonlinear regression\n\\[ Y_i = f(x_i, \\beta) + \\epsilon_i \\]\n\n\nwhere \\(f(x_i, \\beta)\\) is a nonlinear function of the parameters \\(\\beta\\): “How do we model a change in \\(y\\) with \\(x\\) when the relationship is nonlinear?”"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-relationships",
    "href": "lectures/L12/lecture-12.html#nonlinear-relationships",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Nonlinear relationships",
    "text": "Nonlinear relationships\nLinear relationships are simple to interpret since the rate of change is constant.\n“As one changes, the other changes at a constant rate.”\nNonlinear relationships often involve exponential, logarithmic, or power functions.\n“As one changes, the other changes at a rate that is not proportional to the change in the other."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#dealing-with-nonlinearity",
    "href": "lectures/L12/lecture-12.html#dealing-with-nonlinearity",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Dealing with nonlinearity",
    "text": "Dealing with nonlinearity\nTransformations\nOften, a nonlinear relationship may be transformed into a linear relationship by applying a transformation to the response variable or the predictor variable(s).\n\nLogarithmic: \\(y = \\log(x)\\)\nExponential: \\(y = e^x\\)\nSquare-root: \\(y = \\sqrt{x}\\)\nInverse: \\(y = \\frac{1}{x}\\)\n\n\n\nUsually works when \\(y\\) changes monotically with \\(x\\).\nMore interpretable and easier to fit."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-relationships-exponents",
    "href": "lectures/L12/lecture-12.html#nonlinear-relationships-exponents",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Nonlinear relationships: exponents",
    "text": "Nonlinear relationships: exponents\n\n\\(x^2\\) is the square of \\(x\\).\n\\(x^3\\) is the cube of \\(x\\).\n\\(x^a\\) is x raised to the power of \\(a\\).\n\n\nIn a relationship where \\(y\\) is a function of \\(x^a\\), as \\(x\\) increases, \\(y\\) increases nonlinearly at a rate that depends on the value of \\(x\\) and \\(a\\) (\\(\\frac{dy}{dx} = ax^{a-1}\\)).\n\n\n\nCode\n# Plot a simulation of above in ggplot2\nset.seed(123)\ntibble(x = seq(0, 10, by = 0.2), y = x^2) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(x = \"x\", y = \"y\") +\n  ggtitle(expression(y == x^2)) +\n  theme(plot.title = element_text(size = 40, face = \"bold\"))"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-relationships-logarithms",
    "href": "lectures/L12/lecture-12.html#nonlinear-relationships-logarithms",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Nonlinear relationships: logarithms",
    "text": "Nonlinear relationships: logarithms\n\n\\(log_e(x)\\) is the natural logarithm of \\(x\\).\n\\(log_{10}(x)\\) is the common logarithm of \\(x\\).\n\\(log_a(x)\\) is the logarithm of \\(x\\) to the base \\(a\\).\n\nInterpretation:\n\nIf \\(\\log_a(y) = x\\): as \\(x\\) increases, the value of \\(y\\) increases by \\(y = a^x\\).\nIf \\(y = \\log_a(x)\\): as \\(x\\) increases, the value of \\(y\\) increases by \\(y = \\log_a(x)\\). As \\(y\\) increases, the value of \\(x\\) increases by \\(x = a^y\\)."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#exponents-and-logarithms",
    "href": "lectures/L12/lecture-12.html#exponents-and-logarithms",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Exponents and logarithms",
    "text": "Exponents and logarithms\n\n\n\n\n\n\n\n\n\nExponents\nLogarithms\n\n\n\n\nDefinition\nIf \\(a^n = b\\), \\(a\\) is the base, \\(n\\) is the exponent, and \\(b\\) is the result.\nIf \\(\\log_a b = n\\), \\(a\\) is the base, \\(b\\) is the result, and \\(n\\) is the logarithm (or the exponent in the equivalent exponential form).\n\n\nExample\n\\(2^3 = 8\\)\n\\(\\log_2 8 = 3\\)\n\n\nInterpretation\n\\(2\\) raised to the power of \\(3\\) equals \\(8\\).\nThe power to which you must raise \\(2\\) to get \\(8\\) is \\(3\\).\n\n\nInverse\nThe logarithm is the inverse operation of exponentiation.\nThe exponentiation is the inverse operation of logarithm.\n\n\nProperties\n\\((a^n)^m = a^{n \\cdot m}\\), \\(a^n \\cdot a^m = a^{n+m}\\), \\(\\frac{a^n}{a^m} = a^{n-m}\\)\n\\(\\log_a(b \\cdot c) = \\log_a b + \\log_a c\\), \\(\\log_a\\left(\\frac{b}{c}\\right) = \\log_a b - \\log_a c\\), \\(\\log_a(b^n) = n \\cdot \\log_a b\\)\n\n\n\n\n\n\n\n\n\nNote\n\n\nFor your understanding, not examinable."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#exponential-decay-relationship",
    "href": "lectures/L12/lecture-12.html#exponential-decay-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Exponential decay relationship",
    "text": "Exponential decay relationship\nResponse variable decreases and approaches limit as predictor variable increases.\n\\[ y = a \\cdot e^{-bx} \\]\n\n\nCode\nset.seed(429) # set seed\n# Simulate data:\ndecay &lt;- tibble(\n  predictor = seq(0,10, by = 0.2),\n  response = abs(exp(-0.5*predictor) + rnorm(length(predictor), mean = 1, sd = 0.1)))\n\nggplot(data = decay, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\nExamples: radioactive decay, population decline, chemical reactions."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#asymptotic-relationship",
    "href": "lectures/L12/lecture-12.html#asymptotic-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Asymptotic relationship",
    "text": "Asymptotic relationship\nResponse variable increases and approaches a limit as the predictor variable increases.\n\\[ y = a + b(1 - e^{-cx}) \\]\n\n\nCode\nset.seed(442) # set seed\n# Simulate data:\nasymptotic = tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100*(1-exp(-0.5*predictor)) + rnorm(length(predictor), mean = 0, sd = 10))\n\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\nExamples: population growth, enzyme kinetics."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#logistic-relationship",
    "href": "lectures/L12/lecture-12.html#logistic-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Logistic relationship",
    "text": "Logistic relationship\nAn S-shaped relationship, where the response variable is at first exponential, then asymptotic.\n\\[ y = c + \\frac{d-c}{1+e^{-b(x-a)}} \\]\n\n\nCode\nset.seed(450)\n# Simulate data:\nlogistic &lt;- tibble(predictor = seq(0, 10, by = 0.2), \n  response = 10 + abs(300 * (1 / (1 + exp(-0.8 * (predictor - 5)))) + rnorm(length(predictor), mean = 0, sd = 10)))\n\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\nExamples: growth of bacteria, disease spread, species growth."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#polynomial-relationship",
    "href": "lectures/L12/lecture-12.html#polynomial-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Polynomial relationship",
    "text": "Polynomial relationship\nResponse variable changes in a variety of ways as the predictor variable changes. Also known as ‘curvilinear’.\n\\[ y = a + bx + cx^2 + dx^3 + ... \\]\n\n\nCode\n# Set seed for reproducibility\nset.seed(529)\n# Simulate data:\ncurvilinear &lt;- tibble(predictor = seq(0, 30, length.out = 50), \n  response = 50 * (1 - (predictor - 15)^2 / 225) + rnorm(length(predictor), mean = 0, sd = 5))\n\nggplot(data = curvilinear, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\nExamples: food intake, drug dosage, exercise."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-exponential-decay",
    "href": "lectures/L12/lecture-12.html#transformations-exponential-decay",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: exponential decay",
    "text": "Transformations: exponential decay\n\n\nBefore transformation\n\n\nCode\nggplot(data = decay,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nggplot(data = decay, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-exponential-decay-1",
    "href": "lectures/L12/lecture-12.html#transformations-exponential-decay-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: exponential decay",
    "text": "Transformations: exponential decay\n\n\nBefore transformation\n\n\nCode\nautoplot(lm(response ~ predictor, data = decay)) +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nautoplot(lm(log(response) ~ predictor, data = decay)) +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship",
    "href": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: asymptotic relationship",
    "text": "Transformations: asymptotic relationship\n\n\nBefore transformation\n\n\nCode\nggplot(data = asymptotic,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nggplot(data = asymptotic, \n       aes(x = log(predictor), y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship-1",
    "href": "lectures/L12/lecture-12.html#transformations-asymptotic-relationship-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: asymptotic relationship",
    "text": "Transformations: asymptotic relationship\n\n\nBefore transformation\n\n\nCode\nautoplot(lm(response ~ predictor, data = asymptotic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nautoplot(lm(log(response) ~ predictor, data = asymptotic)) +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-logistic-relationship",
    "href": "lectures/L12/lecture-12.html#transformations-logistic-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: logistic relationship",
    "text": "Transformations: logistic relationship\n\n\nBefore transformation\n\n\nCode\nggplot(data = logistic,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nggplot(data = logistic, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-logistic-relationship-1",
    "href": "lectures/L12/lecture-12.html#transformations-logistic-relationship-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: logistic relationship",
    "text": "Transformations: logistic relationship\n\n\nBefore transformation\n\n\nCode\nautoplot(lm(response ~ predictor, data = logistic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nautoplot(lm(log(response) ~ predictor, data = logistic)) +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-polynomial-relationship",
    "href": "lectures/L12/lecture-12.html#transformations-polynomial-relationship",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: polynomial relationship",
    "text": "Transformations: polynomial relationship\n\n\nBefore transformation\n\n\nCode\nggplot(data = curvilinear,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nggplot(data = curvilinear, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#transformations-polynomial-relationship-1",
    "href": "lectures/L12/lecture-12.html#transformations-polynomial-relationship-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Transformations: polynomial relationship",
    "text": "Transformations: polynomial relationship\n\n\nBefore transformation\n\n\nCode\nautoplot(lm(response ~ predictor, data = curvilinear)) +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\nAfter loge transform\n\n\nCode\nautoplot(lm(log(response) ~ predictor, data = curvilinear)) +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#did-the-transformations-work",
    "href": "lectures/L12/lecture-12.html#did-the-transformations-work",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Did the transformations work?",
    "text": "Did the transformations work?\n\nTo a certain extent…\nProblems:\n\nRelationships typically do not meet the linear assumption, but seem “ok” for other assumptions.\nPoor fit to the data (over or underfitting in some areas).\nDifficult to interpret the results."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#nonlinear-regression-2",
    "href": "lectures/L12/lecture-12.html#nonlinear-regression-2",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nA way to model complex (nonlinear) relationships.\n\ni.e. phenomena that arise in the natural and physical sciences e.g. biology, chemistry, physics, engineering.\n\nAt least one predictor is not linearly related to the response variable.\nUnique/specific shape - apply only if you are sure of the relationship, e.g. asymptotic, quadratic."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#performing-nonlinear-regression",
    "href": "lectures/L12/lecture-12.html#performing-nonlinear-regression",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Performing nonlinear regression",
    "text": "Performing nonlinear regression\n\nPolynomial regression: still linear in the parameters and a good place to start.\nNonlinear regression: use the nls() function to fit the following nonlinear models:\n\nExponential growth\nExponential decay\nLogistic"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#model",
    "href": "lectures/L12/lecture-12.html#model",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Model",
    "text": "Model\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i \\]\nwhere \\(k\\) is the degree of the polynomial.\n\nThe model is still linear in the parameters \\(\\beta\\) and can be fitted using least squares.\nInstead of multiple predictors, we have multiple terms of the same predictor (same \\(x\\)).\nOnly the highest-order term is tested for significance.\nCan still be fit using lm().\nThe more complex, the less likely it follows a true biological relationship…\n\n\n\nAdding polynomial terms\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nEach level increases the power of the predictor by 1."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#the-data",
    "href": "lectures/L12/lecture-12.html#the-data",
    "title": "Topic 12 – Nonlinear regression",
    "section": "The data",
    "text": "The data\nSee Slide 11 for the relationship and mathematical expression.\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-linear",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-linear",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fitting the model (linear)",
    "text": "Fitting the model (linear)\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\n\n\nCode\nlin_fit &lt;- lm(response ~ predictor, asymptotic)\n\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\", size = 2)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-polydegree-2",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-polydegree-2",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fitting the model (poly(degree = 2))",
    "text": "Fitting the model (poly(degree = 2))\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\]\n\n\nCode\npoly2_fit &lt;- lm(response ~ poly(predictor, 2), asymptotic)\n\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\", size = 2)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-polydegree-3",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-polydegree-3",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fitting the model (poly(degree = 3))",
    "text": "Fitting the model (poly(degree = 3))\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i \\]\n\n\nCode\npoly3_fit &lt;- lm(response ~ poly(predictor, 3), asymptotic)\n\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\") +\n  geom_line(aes(y = predict(poly3_fit)), color = \"seagreen\", size = 2)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-polydegree-10",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-polydegree-10",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fitting the model (poly(degree = 10))",
    "text": "Fitting the model (poly(degree = 10))\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_10 x_i^{10} + \\epsilon_i \\]\n\n\nCode\npoly10_fit &lt;- lm(response ~ poly(predictor, 10), asymptotic)\n\n\n\n\n\n\nComparison of R2 of Polynomial Models\n\n\nModel\nR2\n\n\n\n\nLinear\n0.570\n\n\nPoly2\n0.820\n\n\nPoly3\n0.872\n\n\nPoly10\n0.862\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe use adjusted R2 for polynomials - extra terms, extra complexity, so extra penalty."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#limitations",
    "href": "lectures/L12/lecture-12.html#limitations",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Limitations",
    "text": "Limitations\n\nMeaning of the coefficients is not always clear.\nExtrapolation can be dangerous.\nExtra terms can lead to overfitting and are difficult to interpret:\nParsimony: is the most complex term (highest power) significant? If not, use a lower power.\n\n\n\nCode\nsummary(poly10_fit)\n\n\n\nCall:\nlm(formula = response ~ poly(predictor, 10), data = asymptotic)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1659  -8.6908  -0.0494   8.8003  16.4012 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             79.818      1.552  51.426  &lt; 2e-16 ***\npoly(predictor, 10)1   159.368     11.084  14.378  &lt; 2e-16 ***\npoly(predictor, 10)2  -106.939     11.084  -9.648 5.37e-12 ***\npoly(predictor, 10)3    48.570     11.084   4.382 8.28e-05 ***\npoly(predictor, 10)4   -19.411     11.084  -1.751   0.0876 .  \npoly(predictor, 10)5     1.193     11.084   0.108   0.9148    \npoly(predictor, 10)6    -2.769     11.084  -0.250   0.8040    \npoly(predictor, 10)7    -1.343     11.084  -0.121   0.9042    \npoly(predictor, 10)8    -4.009     11.084  -0.362   0.7195    \npoly(predictor, 10)9    -2.851     11.084  -0.257   0.7984    \npoly(predictor, 10)10    5.769     11.084   0.520   0.6056    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.08 on 40 degrees of freedom\nMultiple R-squared:  0.8897,    Adjusted R-squared:  0.8621 \nF-statistic: 32.26 on 10 and 40 DF,  p-value: 4.846e-16\n\n\nStill:\n\nEasy to fit: just add polynomial terms to the model.\nSimple to perform: use lm()."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-a-nonlinear-model",
    "href": "lectures/L12/lecture-12.html#fitting-a-nonlinear-model",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fitting a nonlinear model",
    "text": "Fitting a nonlinear model\nIf you have some understanding of the underlying relationship (e.g. mechanistic process) between the variables, you can fit a nonlinear model. \n\nMathematical expression\n\\[ Y_i = f(x_i, \\beta) + \\epsilon_i \\]\nwhere \\(f(x_i, \\beta)\\) is a nonlinear function of the parameters \\(\\beta\\).\n\n\\(Y_i\\) is the continuous response variable.\n\\(x_i\\) is the vector of predictor variables.\n\\(\\beta\\) is the vector of unknown parameters.\n\\(\\epsilon_i\\) is the random error term (residual error)."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#assumptions",
    "href": "lectures/L12/lecture-12.html#assumptions",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Assumptions",
    "text": "Assumptions\nLike the linear model, the nonlinear model assumes INE:\n\nError terms are independent (Independence).\nError terms are normally distributed (Normality).\nError terms have equal/constant variance (Homoscedasticity).\n\nBasically:\n\\[ \\epsilon_i \\sim N(0, \\sigma^2) \\]\n\nLike all other models we have seen, we focus on the residuals to assess the model fit, since the residuals are the only part of the model that is random."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#estimating-the-model-parameters",
    "href": "lectures/L12/lecture-12.html#estimating-the-model-parameters",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Estimating the model parameters",
    "text": "Estimating the model parameters\n\n\n\nThe parameters are estimated using the method of least squares.\nFor nonlinear models, a nonlinear optimization algorithm is used to find the best fit, rather than ordinary least squares:\n\nGauss-Newton algorithm\nLevenberg-Marquardt algorithm\n\nThis can only be performed iteratively and depends on a “best guess” of the parameters as a start.\n\ni.e. we need to provide a starting point for a nonlinear least squares algorithm to begin.\n\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#two-methods-in-r",
    "href": "lectures/L12/lecture-12.html#two-methods-in-r",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Two methods in R",
    "text": "Two methods in R\n\n\nUse nls() function in R.\n\n\nCode\nnls(formula, data, start)\n\n\n\nformula: a formula object, response variable ~ predictor variable(s).\ndata: a data frame containing the variables in the model (response, predictor).\nstart: a named list of starting values for the parameters in the model.\n\n\nSelf-starting functions: SSexpf(), SSasymp(), SSlogis(), etc.\n\nSelf-starting functions estimate the starting values for you.\nNamed after the models they fit.\nExisting functions have pre-set formulas.\nCan define own functions but more complex than nls()."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#with-nls",
    "href": "lectures/L12/lecture-12.html#with-nls",
    "title": "Topic 12 – Nonlinear regression",
    "section": "With nls()",
    "text": "With nls()\n\\(y=y_0e^{kx}\\)\nwhere\n\n\\(y\\) is the response and \\(x\\) is the predictor\n\\(y_0\\) is the value of \\(y\\) when \\(x = 0\\)\n\\(k\\) is the rate of change\n\n\\(k\\) can be estimated with the equation \\(slope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}\\), but usually a value of 1 is a good starting point.\n\n\nCode\nset.seed(123)\ngrowth &lt;- tibble(\n  predictor = seq(0,10, by = 0.2),\n  response = abs(exp(0.5*predictor) + rnorm(length(predictor), mean = 1, sd = 5)))\n\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predictor\", y = \"Response\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#first-guess",
    "href": "lectures/L12/lecture-12.html#first-guess",
    "title": "Topic 12 – Nonlinear regression",
    "section": "First guess",
    "text": "First guess\nBased on the plot, we can estimate \\(y_0 ~ 0\\) and \\(k = 1\\). Because of the equation, \\(y=y_0e^{kx}\\), \\(y_0\\) cannot be 0!\n\n\nCode\nfit_exponential &lt;- nls(response ~ y0*exp(k*predictor), data = growth, \n  start = list(y0 = 0.1, k = 1))\n\n\n\n\nCode\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() + \n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_exponential)), color = \"red\", size = 2)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#check-assumptions",
    "href": "lectures/L12/lecture-12.html#check-assumptions",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Check assumptions",
    "text": "Check assumptions\n\n\nCode\nlibrary(nlstools)\nresids &lt;- nlsResiduals(fit_exponential)\nplot(resids)\n\n\n\n\nThese plots determine if the residuals are normally distributed and have equal variance\nNormal QQ looks good\nResiduals vs fitted and Standardized Residuals even spread but slight fanning.\nWith Autocorrelation we want random scatter around 0 – this indicates independence. Harder to meet with time-series data.\nNonlinear models typically should meet assumptions because they are fitted specifically to the data."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#interpretation",
    "href": "lectures/L12/lecture-12.html#interpretation",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nCode\nsummary(fit_exponential)\n\n\n\nFormula: response ~ y0 * exp(k * predictor)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \ny0   1.1694     0.1291   9.059 4.82e-12 ***\nk    0.4847     0.0121  40.057  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.409 on 49 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 1.192e-06\n\n\n\nThe model is significant since the p-value is less than 0.05 for all parameters.\nIf this were real data (e.g. population growth), the parameters themselves e.g. rate of change, are useful\nThe parameterised model is:\n\n\\[ y = 1.17 \\cdot e^{-0.484x} \\] The R-squared value is not reported for nonlinear models as the sum of squares is not partitioned into explained and unexplained components. You can use the residual standard error and plots instead to compare between models."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#a-really-bad-guess",
    "href": "lectures/L12/lecture-12.html#a-really-bad-guess",
    "title": "Topic 12 – Nonlinear regression",
    "section": "A really bad guess",
    "text": "A really bad guess\nWhat if we don’t estimate our parameters very well? R will either give an error or get there eventually.\nNote the parameters and residual standard error are the same as the previous slide - but the Number of iterations to convergence is higher.\n\n\nCode\nfit_exponential &lt;- nls(response ~ y0*exp(k*predictor), data = growth, \n  start = list(y0 = 50, k = 1.5)) # totally bogus numbers\n\nsummary(fit_exponential)\n\n\n\nFormula: response ~ y0 * exp(k * predictor)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \ny0   1.1694     0.1291   9.059 4.82e-12 ***\nk    0.4847     0.0121  40.057  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.409 on 49 degrees of freedom\n\nNumber of iterations to convergence: 28 \nAchieved convergence tolerance: 2.003e-06\n\n\n\n\n\n\n\n\nTip\n\n\nIf an error pops up, try different starting values - the rate of change is most likely the problem."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fitting-the-model-with-ssexpf",
    "href": "lectures/L12/lecture-12.html#fitting-the-model-with-ssexpf",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fitting the model with SSexpf()",
    "text": "Fitting the model with SSexpf()\n\nSSexpf() is from the nlraa package.\nIt has the same formula as above – different names for parameters (\\(y_0\\) = \\(a\\), \\(k\\) = \\(c\\)) but we can re-define them to anything we want\nReaches the same result but with less effort.\n\n\n\nCode\nlibrary(nlraa)\nfit_exponential_ss &lt;- nls(response ~ SSexpf(predictor, y0, k), data = decay) \nsummary(fit_exponential_ss)\n\n\n\nFormula: response ~ SSexpf(predictor, y0, k)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \ny0  1.65486    0.04699   35.22  &lt; 2e-16 ***\nk  -0.06527    0.00590  -11.06 6.33e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1471 on 49 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 1.048e-06"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#the-equation",
    "href": "lectures/L12/lecture-12.html#the-equation",
    "title": "Topic 12 – Nonlinear regression",
    "section": "The equation",
    "text": "The equation\n\nThere are multiple equations for asymptotic models, this is the equation that SSasymp() (base R) uses:\n\n\\[ y = Asym + (R_0-Asym) \\cdot e^{-e^{lrc} \\cdot x} \\]\n\n\\(R_0\\) is value of \\(y\\) when \\(x = 0\\).\n\\(Asym\\) is the upper limit: the maximum value of \\(y\\).\n\\(lrc\\) is the rate of change: the rate at which \\(y\\) approaches the upper limit.\n\n\n\n\nCode\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  labs(x = \"Predictor\", y = \"Response\")\n\n\n\n\n\n\n\n\n\nSome plausible estimates – \\(R_0 = 0\\), \\(Asym = 100\\), \\(lrc = 0.8\\)."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fit-model",
    "href": "lectures/L12/lecture-12.html#fit-model",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fit model",
    "text": "Fit model\n\n\nCode\n# fit_asymptotic &lt;- nls(response ~ Asym + (R0-Asym)*(exp(-exp(lrc)*predictor)), data = asymptotic, \n#   start = list(R0 = 0, Asym = 100, lrc = 0.8))\n\nfit_asymptotic &lt;- nls(response ~ SSasymp(predictor, Asym, R0, lrc), data = asymptotic)\n\n\n\n\nCode\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_asymptotic)), color = \"red\", size = 2)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#check-assumptions-1",
    "href": "lectures/L12/lecture-12.html#check-assumptions-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Check assumptions",
    "text": "Check assumptions\n\n\nCode\nlibrary(nlstools)\nresids &lt;- nlsResiduals(fit_asymptotic)\nplot(resids)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#interpretation-1",
    "href": "lectures/L12/lecture-12.html#interpretation-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nCode\nsummary(fit_asymptotic)\n\n\n\nFormula: response ~ SSasymp(predictor, Asym, R0, lrc)\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nAsym  98.5204     2.2852  43.113  &lt; 2e-16 ***\nR0   -14.5176     6.6416  -2.186  0.03374 *  \nlrc   -0.4626     0.1134  -4.079  0.00017 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 48 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 3.341e-07\n\n\n\nThe model is significant since the p-value is less than 0.05 for all parameters.\nIf this were real data (e.g. population growth), the parameters themselves e.g. rate of change, are useful\nThe parameterised model is:\n\n\\[ y = 98.5 + (-14.5-98.5) \\cdot e^{-e^{-0.463} \\cdot x}  \\] # Example: fitting a logistic model"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#the-equation-1",
    "href": "lectures/L12/lecture-12.html#the-equation-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "The equation",
    "text": "The equation\nThere are multiple equations for logistic models, but they all have an ‘S’ or sigmoid shape. The equation that SSlogis() (base R) assumes \\(y\\) is positive and uses:\n\\[ y = \\frac{Asym}{1+e^{\\frac{xmid-x}{scal}}} \\] where\n\n\\(Asym\\) is the upper limit: the maximum value of \\(y\\).\n\\(xmid\\) is the value of \\(x\\) when \\(y\\) is halfway between the lower and upper limits.\n\\(scal\\) is the rate of change: the rate at which \\(y\\) approaches the upper limit.\n\n\n\nCode\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_hline(yintercept = 300, linetype = \"dashed\") +\n  geom_vline(xintercept = 5, linetype = \"dashed\") +\n  # label the lines above\n  annotate(\"text\", x = 0, y = 300, label = \"Asym\", size = 8, vjust = 1.5) +\n  annotate(\"text\", x = 5, y = 100, label = \"xmid\", size = 8, hjust = -1) +\n  ## plot the rate\n  geom_segment(aes(x = 2.5, y = 60, xend = 6, yend = 250), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  # label the rate\n  annotate(\"text\", x = 4, y = 180, label = \"scal\", size = 8, colour = \"red\", hjust = -1)\n\n\n\nSome starting values would be \\(Asym = 300\\), \\(xmid = 5\\), \\(scal = 1\\)."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#fit-model-1",
    "href": "lectures/L12/lecture-12.html#fit-model-1",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Fit model",
    "text": "Fit model\nEstimating the parameters or using the self-starting function SSlogis() gives a near-identical result.\n\n\nCode\n# fit_logistic &lt;- nls(response ~ Asym/(1+exp((xmid-predictor)/scal)), data = logistic, \n#    start = list(Asym = 300, xmid = 5, scal = 1))\n\nfit_logistic &lt;- nls(response ~ SSlogis(predictor, Asym, xmid, scal), data = logistic)\n\nresids &lt;- nlsResiduals(fit_logistic)\nplot(resids)"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#interpretation-2",
    "href": "lectures/L12/lecture-12.html#interpretation-2",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Interpretation",
    "text": "Interpretation\nSSlogis() guessed the parameters on the first try.\n\n\nCode\nsummary(fit_logistic)\n\n\n\nFormula: response ~ SSlogis(predictor, Asym, xmid, scal)\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nAsym 310.64727    4.62579   67.16   &lt;2e-16 ***\nxmid   4.92715    0.07142   68.99   &lt;2e-16 ***\nscal   1.34877    0.05418   24.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.22 on 48 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 6.632e-07\n\n\n\nThe model is significant since the p-value is less than 0.05 for all parameters.\nIf the model visually fits well and relationship has reasoning (parameter significance not always important).\nThe parameterised model is:\n\n\\[ y = \\frac{310}{1+e^{\\frac{4.93-x}{1.35}}} \\]"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#example-polynomial-regression",
    "href": "lectures/L12/lecture-12.html#example-polynomial-regression",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Example: polynomial regression",
    "text": "Example: polynomial regression\n\n\nCode\nlibrary(tidyr)\n\n# Create a new data frame with predictor values and model predictions\npredictions &lt;- data.frame(\n  predictor = asymptotic$predictor,\n  Linear = predict(lin_fit),\n  Poly_2 = predict(poly2_fit),\n  Poly_3 = predict(poly3_fit),\n  Poly_10 = predict(poly10_fit)\n)\n\n# Reshape the data to long format\npredictions_long &lt;- predictions %&gt;%\n  pivot_longer(cols = -predictor, names_to = \"Model\", values_to = \"response\")\n\n# Plot the data\nggplot(predictions_long, aes(x = predictor, y = response, color = Model)) +\n  geom_point(data = asymptotic, aes(x = predictor, y = response), inherit.aes = FALSE) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  scale_color_brewer(palette = \"Spectral\")"
  },
  {
    "objectID": "lectures/L12/lecture-12.html#prediction-quality",
    "href": "lectures/L12/lecture-12.html#prediction-quality",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Prediction quality",
    "text": "Prediction quality\nWe can use prediction quality metrics to compare the fits.\n\nAkaike information criterion (AIC) and Bayesian information criterion (BIC).\n\nUseful for comparing model fits.\nHas a penalty for more predictors\n\nResidual standard error, residual sum of squares (deviance(mod)), root mean squared error (RMSE) and mean absolute error (MAE).\n\nEssentially the difference between observed and predicted (residuals).\nRMSE penalises larger residuals."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#aic-and-bic",
    "href": "lectures/L12/lecture-12.html#aic-and-bic",
    "title": "Topic 12 – Nonlinear regression",
    "section": "AIC and BIC",
    "text": "AIC and BIC\nUse the broom package to extract the AIC and BIC values from the model fits.\n\n\nCode\nlibrary(broom)\n# collect all polynomial fits into a single tibble using glance\npoly_fits &lt;- tibble(\n  model = c(\"linear\", \"poly2\", \"poly3\", \"poly10\"),\n  fit = list(lin_fit, poly2_fit, poly3_fit, poly10_fit)) %&gt;%\n  mutate(glance = map(fit, glance)) %&gt;%\n  unnest(glance) %&gt;%\n  select(model, AIC, BIC)\npoly_fits\n\n\n# A tibble: 4 × 3\n  model    AIC   BIC\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 linear  453.  459.\n2 poly2   409.  416.\n3 poly3   392.  402.\n4 poly10  402.  425.\n\n\n\nThe smaller the AIC or BIC, the better the fit compared to other models."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#calculate-rmse-and-mae",
    "href": "lectures/L12/lecture-12.html#calculate-rmse-and-mae",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Calculate RMSE and MAE",
    "text": "Calculate RMSE and MAE\n\n\nCode\npredictions &lt;- data.frame(\n  observed = asymptotic$response,\n  Linear = predict(lin_fit),\n  Poly_2 = predict(poly2_fit),\n  Poly_3 = predict(poly3_fit),\n  Poly_10 = predict(poly10_fit)\n)\n\nerrors &lt;- predictions %&gt;%\n  pivot_longer(cols = -observed, names_to = \"Model\", values_to = \"Predicted\") %&gt;%\n  group_by(Model) %&gt;%\n  summarise(\n    RMSE = sqrt(mean((observed - Predicted)^2)),\n    MAE = mean(abs(observed - Predicted))\n)\n\nknitr::kable(errors, digits=2, caption = \"Comparison of RMSE and MAE for different models\")\n\n\n\nComparison of RMSE and MAE for different models\n\n\nModel\nRMSE\nMAE\n\n\n\n\nLinear\n19.38\n15.17\n\n\nPoly_10\n9.82\n8.57\n\n\nPoly_2\n12.30\n9.88\n\n\nPoly_3\n10.25\n8.83\n\n\n\n\n\n\nFrom the results, the polynomial to the degree of 10 has the lowest error - but visually we know it is overfitting, and the cubic polynomial is more parsimonius.\nWe can say the model has a prediction error of 10.25 units (RMSE) and 8.83 units (MAE).\n\n\n\n\n\n\n\nNote\n\n\nBoth the RMSE and MAE measure error on the same scale as the response variable. e.g. if the response variable is in kg, the error will be in kg."
  },
  {
    "objectID": "lectures/L12/lecture-12.html#summary",
    "href": "lectures/L12/lecture-12.html#summary",
    "title": "Topic 12 – Nonlinear regression",
    "section": "Summary",
    "text": "Summary\n\n\n\nWith nonlinear relationships, there are three possible approaches:\n\nLinearise the relationship by transforming:\n\nFit: easy\nInterpret: difficult\n\nAdd polynomial terms:\n\nFit: easy\nInterpret: difficult\n\nFit the model using a nonlinear algorithm:\n\nFit: difficult\nInterpret: easy\n\n\n\n\n\n\n\n\nNonlinear models:\n\nUseful for modelling more complex relationships. Require some understanding of the underlying relationship and equations.\nMainly for prediction rather than interpreting relationships.\nSelf-starting functions have limited pre-defined formulas.\nAssumptions INE."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#module-overview",
    "href": "lectures/L11/lecture-11.html#module-overview",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Module overview",
    "text": "Module overview\n\n\n\n\nWeek 9. Describing Relationships\n\nCorrelation (calculation, interpretation)\nRegression (model structure, model fitting\nWhat/when/why/how\n\nWeek 10. Simple Linear Regression\n\nCan we use the model?(assumptions, hypothesis testing)\nHow good is the model?(interpretation, model fit)\n\nWeek 11. Multiple Linear Regression\n\nMultiple Linear Regression (MLR) modelling\nAssumptions, interpretation and the principle of parsimony\n\nWeek 12. Nonlinear Regression\n\nCommon nonlinear functions\nTransformations"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#last-week-simple-linear-regression",
    "href": "lectures/L11/lecture-11.html#last-week-simple-linear-regression",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Last week: simple linear regression",
    "text": "Last week: simple linear regression\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\nIdeal for predicting a continuous response variable from a single predictor variable: “How does \\(y\\) change as \\(x\\) changes?”\n\nIdentify/quantify relationships between variables\nPredict future values\n\n\nWhat if we have more than one predictor?\nWhat is the model and how do we interpret the results?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#history",
    "href": "lectures/L11/lecture-11.html#history",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "History",
    "text": "History\n\n\n \nFrancis Galton and Karl Pearson\n\n\nFirst suggested by Francis Galton in 1886 while studying genetic variations in sweet peas over several generations\nKarl Pearson developed the mathematical formula for multiple linear regression model later (early 1900s)\n\n\n\n“The somewhat complicated mathematics of multiple correlation, with its repeated appeals to the geometrical notions of hyperspace, remained a closed chamber to him.”\n\n– Pearson (1930), on Galton’s work with MLR"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#steps-for-regression",
    "href": "lectures/L11/lecture-11.html#steps-for-regression",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Steps for Regression",
    "text": "Steps for Regression\n\nUnderstand the variables\nExplore data\nFit model\nCheck assumptions\nAssess fit of model/s (parsimony)\nInterpret output"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#example-air-quality-in-new-york-1973",
    "href": "lectures/L11/lecture-11.html#example-air-quality-in-new-york-1973",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Example: Air Quality in New York (1973)",
    "text": "Example: Air Quality in New York (1973)\n\n\nCode\ndata(airquality)\ndplyr::glimpse(airquality)\n\n\nRows: 153\nColumns: 6\n$ Ozone   &lt;int&gt; 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R &lt;int&gt; 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    &lt;dbl&gt; 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    &lt;int&gt; 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n\n\n\nOzone (\\(O_3\\)) is a harmful air pollutant at ground level - the main component of smog:\n\nOzone: ozone concentration (ppb)\nSolar.R: solar radiation (lang, Langleys)\nWind: wind speed (mph)\nTemp: ambient temperature (degrees F)\nMonth: month (1-12)\nDay: day of the month (1-31)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#scatterplots",
    "href": "lectures/L11/lecture-11.html#scatterplots",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nCode\npairs(airquality)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#correlations-via-base-r",
    "href": "lectures/L11/lecture-11.html#correlations-via-base-r",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Correlations via base R",
    "text": "Correlations via base R\n\n\nCode\ncor(airquality, use = \"complete.obs\") |&gt; round(2)\n\n\n        Ozone Solar.R  Wind  Temp Month   Day\nOzone    1.00    0.35 -0.61  0.70  0.14 -0.01\nSolar.R  0.35    1.00 -0.13  0.29 -0.07 -0.06\nWind    -0.61   -0.13  1.00 -0.50 -0.19  0.05\nTemp     0.70    0.29 -0.50  1.00  0.40 -0.10\nMonth    0.14   -0.07 -0.19  0.40  1.00 -0.01\nDay     -0.01   -0.06  0.05 -0.10 -0.01  1.00"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#correlations-via-corrplot",
    "href": "lectures/L11/lecture-11.html#correlations-via-corrplot",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Correlations via corrplot",
    "text": "Correlations via corrplot\n\n\n\nCode\ncorrplot::corrplot(cor(airquality, use = \"complete.obs\"), method = \"circle\")"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#correlations-via-psych",
    "href": "lectures/L11/lecture-11.html#correlations-via-psych",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Correlations via psych",
    "text": "Correlations via psych\n\n\nCode\npsych::pairs.panels(airquality)\n\n\n\n\nWhat predictors could be useful to predict Ozone?\n\n\nTemp (\\(r\\) = 0.70), Wind (\\(r\\) = -0.60) and Solar.R (\\(r\\) = 0.35) are the most correlated with Ozone.\nWhat can we understand about the relationship between Ozone and Temp (\\(r\\) = 0.70)?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#relationship",
    "href": "lectures/L11/lecture-11.html#relationship",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Relationship",
    "text": "Relationship\nWhat can we understand about the relationship between Ozone and Temp (\\(r\\) = 0.70)?\n\n\n\nCode\nlibrary(ggplot2)\nggplot(data = airquality, aes(x = Temp, y = Ozone)) +\n  geom_point() + \n  labs(x = expression(\"Temperature \" (degree~F)), y = \"Ozone (ppb)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThe higher the temperature, the higher the ozone concentration. The relationship is almost linear."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#fitting-a-simple-model",
    "href": "lectures/L11/lecture-11.html#fitting-a-simple-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Fitting a simple model",
    "text": "Fitting a simple model\n\n\nCode\nfit &lt;- lm(formula = Ozone ~ Temp, data = airquality)\n\n\n\nSimple linear regression between Ozone and Temp\nThis is our baseline or control model"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions-via-base-r",
    "href": "lectures/L11/lecture-11.html#assumptions-via-base-r",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions via base R",
    "text": "Assumptions via base R\n\n\nCode\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions-via-ggfortify-package",
    "href": "lectures/L11/lecture-11.html#assumptions-via-ggfortify-package",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions via ggfortify package",
    "text": "Assumptions via ggfortify package\n\n\nCode\nlibrary(ggfortify)\nggplot2::autoplot(fit) # requires ggfortify but is a ggplot2 function"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions-via-performance-package",
    "href": "lectures/L11/lecture-11.html#assumptions-via-performance-package",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions via performance package",
    "text": "Assumptions via performance package\n\n\nCode\nlibrary(performance)\nperformance::check_model(fit, check = c(\"linearity\", \"qq\", \"homogeneity\", \"outliers\")) # check specific assumptions"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#interpretation",
    "href": "lectures/L11/lecture-11.html#interpretation",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nTemp is a statistically significant predictor of Ozone (p &lt; .001).\nThe (simple linear) model explains 49% of variance (R2 = 0.49).\n\n\nCan we improve the model in other ways? Maybe - by transforming or adding more variables."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#principle-of-parsimony",
    "href": "lectures/L11/lecture-11.html#principle-of-parsimony",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Principle of parsimony",
    "text": "Principle of parsimony\n\nAlso known as Occam’s razor;\n\n\nEntia non sunt multiplicanda praeter necessitatem. “Entities should not be multiplied without necessity.”\n\n\nOxford definition;\n\n\nThe most acceptable explanation of an occurrence, phenomenon, or event is the simplest, involving the fewest entities, assumptions, or changes.\n\n\nSimple is best; i.e. if a simple (one variable) model and a complex (many variables) model predict similarly well, the simple model is preferred.\n\n\nA parsimonius model:\n\nHas only useful predictors\nNo redundant predictors"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-problem-with-using-too-many-predictors",
    "href": "lectures/L11/lecture-11.html#the-problem-with-using-too-many-predictors",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The problem with using too many predictors",
    "text": "The problem with using too many predictors\n\nGenerally, the more predictors we add, the better the model fits data\nHowever, adding too many may cause overfitting, i.e. the model becomes too complex\nAn overfitted model won’t be able to generalise to new data"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-multiple-linear-regression-model",
    "href": "lectures/L11/lecture-11.html#the-multiple-linear-regression-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The multiple linear regression model",
    "text": "The multiple linear regression model\nAn extension of simple linear regression to include more than one predictor variable: “How does \\(y\\) change as \\(x_1\\), \\(x_2\\), …, \\(x_k\\) change?”\n\\[ Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i \\]\n\nTherefore, estimating the model involves estimating the values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), …, \\(\\beta_k\\).\n\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) to \\(\\beta_k\\) are the partial regression coefficients\n\\(\\epsilon\\) is the error (residual) term\n\n\n\nFit MLR model to Air Quality data\nThe variables Month and Day are not useful predictors, so we will exclude them from the model.\n\n\nCode\nfit_multi &lt;- lm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality)\n# fit_multi &lt;- lm(formula = Ozone ~ .-Month -Day, data = airquality) # all variables excluding Month and Day"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#visualisation-not-easy",
    "href": "lectures/L11/lecture-11.html#visualisation-not-easy",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Visualisation: not easy",
    "text": "Visualisation: not easy\nAre the plots useful?\n3D plot\n\n\nCode\nlibrary(plotly)\nplot_ly(data = airquality, \n  x = ~Temp, y = ~Ozone, z = ~Solar.R,\n  type = \"scatter3d\", mode = \"markers\", opacity = 0.5)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#visualisation-not-easy-1",
    "href": "lectures/L11/lecture-11.html#visualisation-not-easy-1",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Visualisation: not easy",
    "text": "Visualisation: not easy\nAre the plots useful?\n4D plot\n\n\nCode\nlibrary(plotly)\nplot_ly(data = airquality, \n  x = ~Temp, y = ~Ozone, z = ~Solar.R, color = ~Wind,\n  type = \"scatter3d\", mode = \"markers\", opacity = 0.5)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#partial-regression-coefficients",
    "href": "lectures/L11/lecture-11.html#partial-regression-coefficients",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Partial regression coefficients",
    "text": "Partial regression coefficients\nGiven the multiple linear model: \\[ Y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i \\]\nThe partial regression coefficient for a predictor \\(x_i\\) is the amount by which the response variable \\(Y\\) changes when \\(x_k\\) is increased by one unit, while all other predictors are held constant.\n\\[ \\beta_k = \\frac{\\Delta Y}{\\Delta x_k} \\]\n\n\n\nCode\nequatiomatic::extract_eq(fit_multi)\n\n\n\\[\n\\operatorname{Ozone} = \\alpha + \\beta_{1}(\\operatorname{Solar.R}) + \\beta_{2}(\\operatorname{Wind}) + \\beta_{3}(\\operatorname{Temp}) + \\epsilon\n\\]\n\n\n\nWith Wind and Solar.R held constant, how does Temp affect Ozone?"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#partial-regression-coefficients-visualisation",
    "href": "lectures/L11/lecture-11.html#partial-regression-coefficients-visualisation",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Partial regression coefficients: visualisation",
    "text": "Partial regression coefficients: visualisation\n\n\nCode\nsjPlot::plot_model(fit_multi,\n  type = \"pred\", \n  terms = c(\"Temp\", \"Solar.R\", \"Wind\"), \n  ci.lvl = NA)\n\n\n\n\nWith Wind and Solar.R held constant, how does Temp affect Ozone?\n\n\n\n\n\n\n\nNote\n\n\nNot necessary to do this - lecture content only."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#interpreting-the-partial-regression-coefficients",
    "href": "lectures/L11/lecture-11.html#interpreting-the-partial-regression-coefficients",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Interpreting the partial regression coefficients",
    "text": "Interpreting the partial regression coefficients\n\n\nCode\nfit_multi\n\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality)\n\nCoefficients:\n(Intercept)      Solar.R         Wind         Temp  \n  -64.34208      0.05982     -3.33359      1.65209  \n\n\n\nHolding all other variables constant:\n\nFor every 1 unit increase in Solar.R, Ozone increases by a mean value of 0.06 ppb.\nFor every 1 degree increase in Temp, Ozone increases by a mean value of 1.65 ppb.\nFor every 1 unit increase in Wind, Ozone decreases by a mean value of 3.33 ppb.\n\n\n\n\n\n\n\n\n\nCaution\n\n\nIf the model is not “valid” (via assumptions or hypothesis), then the partial regression coefficients are not meaningful."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions",
    "href": "lectures/L11/lecture-11.html#assumptions",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions",
    "text": "Assumptions\nIn SLR, the model is made up of the deterministic component (the line) and the random component (the error term).\n\\[ Y_i = \\color{seagreen}{\\beta_0 + \\beta_1 x_i} + \\color{firebrick}\\epsilon_i \\]\n\nThis is the same for MLR: \\[ Y_i = \\color{seagreen}{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k} + \\color{firebrick}{\\epsilon_i} \\]\nSince only the error term is random, the assumptions are still about the error term (residuals), \\(\\hat\\epsilon\\), which is simple to assess!"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions---cline",
    "href": "lectures/L11/lecture-11.html#assumptions---cline",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions - CLINE",
    "text": "Assumptions - CLINE\nAs with Simple Linear Regression, we need to check the assumptions of the model (LINE):\n\nLinearity: the relationships between the response and the predictors are all linear.\nIndependence: the observations are independent of each other.\nNormality: the residuals are normally distributed.\nEqual variance: the variance of the residuals is constant.\n\nWith one extra assumption!\n\nCollinearity: there is no perfect linearity between predictors\n\nTwo predictors that have a perfect linear relationship (i.e. \\(r\\) = 1 or -1) breaks the assumption of collinearity. High (but not perfect) collinearity (e.g. strong/very strong \\(r\\)) does not break the assumption but can lead to unstable estimates and large standard errors.\n\nThe largest correlation between the predictors is between Temp and Wind (\\(r\\) = -0.5). This is not a problem."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#assumptions-of-mlr",
    "href": "lectures/L11/lecture-11.html#assumptions-of-mlr",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Assumptions of MLR",
    "text": "Assumptions of MLR\n\n\nCode\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit_multi)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#transformation-using-log",
    "href": "lectures/L11/lecture-11.html#transformation-using-log",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Transformation using log()",
    "text": "Transformation using log()\nSome evidence of non-linearity in the diagnostic plots. Transform and re-check assumptions.\n\n\n\nCode\nfit_multi_log &lt;- lm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit_multi_log)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#results-mlr-vs-slr",
    "href": "lectures/L11/lecture-11.html#results-mlr-vs-slr",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Results – MLR vs SLR",
    "text": "Results – MLR vs SLR\n\n\n\n\nCode\nsummary(fit_multi_log)\n\n\n\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAll three predictors are statistically significant (p &lt; .001).\nThe MLR model explains 66% of variance (adjusted R2 = 0.66), whereas the SLR explains 48% of variance (multiple R2 = 0.48).\nThus the MLR is the better model."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#hypothesis-testing",
    "href": "lectures/L11/lecture-11.html#hypothesis-testing",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nFor multiple linear regression, there are two hypothesis tests:\n\nIndividual predictors, where the significance of each predictor is tested via t-tests\n\n\\[H_0: \\beta_k = 0\\] \\[H_1: \\beta_k \\neq 0\\]\n\nThe overall model, which is tested with an F-test (to get F-stat). \\(H_0\\) is an intercept-only model (i.e. the mean), so if at least one predictor is useful, the model is better than the intercept-only model.\n\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\] \\[H_1: \\text{At least one } \\beta_k \\neq 0\\]"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#interpreting-coefficients",
    "href": "lectures/L11/lecture-11.html#interpreting-coefficients",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Interpreting coefficients",
    "text": "Interpreting coefficients\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAll three predictors are statistically significant (p &lt; .001). Holding all other variables constant:\n\nFor every 1 unit increase in Solar.R, log(Ozone) increases by a mean value of 0.0025,\nFor every 1 unit increase in Wind, log(Ozone) decreases by a mean value of 0.062,\nFor every 1 degree increase in Temp, log(Ozone) increases by a mean value of 0.049.\n\nOR\n\nFor every 1 unit increase in Solar.R, Ozone increases by approximately a mean value of 0.25%,\nFor every 1 unit increase in Wind, Ozone decreases by approximately a mean value of 6.2%,\nFor every 1 degree increase in Temp, Ozone increases by approximately a mean value of 4.9%."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#model-fit",
    "href": "lectures/L11/lecture-11.html#model-fit",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Model fit",
    "text": "Model fit\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\nOn average, the model predicts log(Ozone) within 0.51 ppb (residual standard error) of the true value. Not bad?\n\n\n\nCode\nexp(0.51) #backtransform\n\n\n[1] 1.665291\n\n\n\nOn average, the model predicts Ozone within 1.67 ppb of the true value.\nDegrees of freedom (107) = number of observations (111) - number of parameters in the model (3 predictors and 1 intercept)\n\n\n\nIf there are &gt;1 predictors, use the adjusted R-Squared as it penalises the model for having more predictors that are not useful.\n\nThe MLR model explains 66% of variance (adjusted R2 = 0.66)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#the-r2-value",
    "href": "lectures/L11/lecture-11.html#the-r2-value",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "The R2 value",
    "text": "The R2 value\nThe R-squared value is the proportion of variance explained by the model.\n\\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\nThe adjusted R-squared value is the proportion of variance explained by the model, adjusted for the number of predictors.\n\\[ R^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} \\]\nwhere \\(n\\) is the number of observations and \\(p\\) is the number of predictors."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#f-stat",
    "href": "lectures/L11/lecture-11.html#f-stat",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "F-stat",
    "text": "F-stat\nCall:\nlm(formula = log(Ozone) ~ Solar.R + Wind + Temp, data = airquality)\n\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\nThe F-statistic tests the null hypothesis that all the regression coefficients are equal to zero, i.e. \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\).\nAs a ratio, it tells us how much better the model is than the null model (i.e. a model with no predictors, the mean).\nIf the p-value is less than our specified critical value (e.g. 0.05), we reject the null hypothesis and conclude that the current model is better than the null model."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#reporting",
    "href": "lectures/L11/lecture-11.html#reporting",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Reporting",
    "text": "Reporting\nA quick (but not complete) summary:\n\nNew York air quality data was collected in 1973 by the New York State Department of Conservation and the National Weather Service (meteorological data). There were 111 observations of 6 variables.\nThere were non-linear relationships between Ozone (the response) and Temp, Wind and Solar.R (the predictors), hence a natural log transformation was applied to Ozone.\nMultiple linear regression was conducted on these variables, and model assumptions (CLINE) were met.\nSolar radiation, wind speed and temperature are significant predictors of Ozone concentration (p &lt; 0.001) with the model accounting for 66% of the variation in log(Ozone). The model explained more variance than a one-predictor model and was found to be significantly better than the null model."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#context",
    "href": "lectures/L11/lecture-11.html#context",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Context",
    "text": "Context\nAbalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. Researchers measured 9 attributes of the abalone: sex, length, diameter, height, whole, shucked, viscera, shell, and rings.\nNote: whole, shucked, viscera and shell are weight measurements.\n\n\nCode\nabalone &lt;- read.csv(\"data/abalone.csv\")\n\nset.seed(1113)          # reproducible randomness\nabalone &lt;- abalone %&gt;% \n  select(-sex) %&gt;%      # remove `sex` because it is categorical\n  sample_n(100)         # sample 100 observations for cleaner curve\n  \nstr(abalone)\n\n\n'data.frame':   100 obs. of  8 variables:\n $ length  : num  0.52 0.71 0.33 0.67 0.65 0.35 0.695 0.52 0.6 0.61 ...\n $ diameter: num  0.405 0.57 0.255 0.55 0.51 0.25 0.53 0.41 0.475 0.48 ...\n $ height  : num  0.14 0.195 0.095 0.17 0.19 0.1 0.15 0.14 0.15 0.17 ...\n $ whole   : num  0.692 1.348 0.188 1.247 1.542 ...\n $ shucked : num  0.276 0.8985 0.0735 0.472 0.7155 ...\n $ viscera : num  0.137 0.444 0.045 0.245 0.373 ...\n $ shell   : num  0.215 0.454 0.06 0.4 0.375 ...\n $ rings   : int  11 11 7 21 9 7 14 11 10 10 ..."
  },
  {
    "objectID": "lectures/L11/lecture-11.html#scatterplots-and-correlations",
    "href": "lectures/L11/lecture-11.html#scatterplots-and-correlations",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Scatterplots and correlations",
    "text": "Scatterplots and correlations\nWe remove sex from the dataset (not numerical), and subset 100 samples for a cleaner view.\n\n\nCode\npsych::pairs.panels(abalone)     # visualise relationships"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#full-model",
    "href": "lectures/L11/lecture-11.html#full-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Full model",
    "text": "Full model\nWe use natural log transformation on the response variable with log() to account for non-linear relationships.\n\n\nCode\nfit &lt;- lm(log(rings) ~ ., data = abalone)\nsummary(fit)\n\n\n\nCall:\nlm(formula = log(rings) ~ ., data = abalone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37297 -0.12727 -0.01584  0.08787  0.61636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34626    0.18219   7.389 6.57e-11 ***\nlength      -1.25389    1.50969  -0.831  0.40837    \ndiameter     3.24138    1.91481   1.693  0.09388 .  \nheight       2.26408    1.34813   1.679  0.09646 .  \nwhole        0.03089    0.29250   0.106  0.91612    \nshucked     -1.30902    0.38861  -3.368  0.00111 ** \nviscera     -0.24785    0.55098  -0.450  0.65389    \nshell        1.73328    0.60179   2.880  0.00494 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1996 on 92 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.5897 \nF-statistic: 21.32 on 7 and 92 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#all-models",
    "href": "lectures/L11/lecture-11.html#all-models",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "All models",
    "text": "All models\nHere, the model is fit with all predictors, then the least significant predictor is removed. This process is repeated until only one predictor remains.\n\n\nCode\nlibrary(broom)\n\nfull7 &lt;- lm(log(rings) ~ ., data = abalone)\npart6 &lt;- update(full7, . ~ . - whole)\npart5 &lt;- update(part6, . ~ . - viscera)\npart4 &lt;- update(part5, . ~ . - length)\npart3 &lt;- update(part4, . ~ . - height)\npart2 &lt;- update(part3, . ~ . - diameter)\npart1 &lt;- update(part2, . ~ . - shucked)\n\nformulas &lt;- c(part1$call$formula, \n              part2$call$formula, \n              part3$call$formula, \n              part4$call$formula, \n              part5$call$formula, \n              part6$call$formula, \n              full7$call$formula)\n\nrs &lt;- bind_rows(glance(part1),\n                glance(part2),\n                glance(part3),\n                glance(part4),\n                glance(part5),\n                glance(part6),\n                glance(full7)) %&gt;%\n  mutate(Model = formulas, n = 1:7) %&gt;%\n  select(Model, n, r.squared, adj.r.squared) %&gt;%\n  mutate_if(is.numeric, round, 3)\n\nknitr::kable(rs)\n\n\n\n\n\n\n\n\n\n\n\nModel\nn\nr.squared\nadj.r.squared\n\n\n\n\nlog(rings) ~ shell\n1\n0.445\n0.439\n\n\nlog(rings) ~ shucked + shell\n2\n0.557\n0.548\n\n\nlog(rings) ~ diameter + shucked + shell\n3\n0.604\n0.591\n\n\nlog(rings) ~ diameter + height + shucked + shell\n4\n0.614\n0.598\n\n\nlog(rings) ~ length + diameter + height + shucked + shell\n5\n0.618\n0.597\n\n\nlog(rings) ~ length + diameter + height + shucked + viscera + , shell\n6\n0.619\n0.594\n\n\nlog(rings) ~ .\n7\n0.619\n0.590"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#reduced-model",
    "href": "lectures/L11/lecture-11.html#reduced-model",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Reduced model",
    "text": "Reduced model\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.4122     0.1594   8.859 4.19e-14 ***\ndiameter      2.0346     0.6034   3.372  0.00108 ** \nshucked      -1.3339     0.2152  -6.200 1.42e-08 ***\nshell         2.0486     0.3672   5.579 2.23e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#model-and-variable-selection",
    "href": "lectures/L11/lecture-11.html#model-and-variable-selection",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Model and variable selection",
    "text": "Model and variable selection\n\nAim is to achieve the best balance between model fit and model complexity.\nFollow the rules of parsimony: the simplest model that explains the data is the best, given similar model fit.\n\nConsider the effect of removing non-significant predictors from the model.\nIf model fit (i.e. R2) reduces drastically, keep the predictor, else keep culling.\n\nCovered in more detail in second year (ENVX2001) (stepwise regression)"
  },
  {
    "objectID": "lectures/L11/lecture-11.html#summary",
    "href": "lectures/L11/lecture-11.html#summary",
    "title": "Topic 11 – Multiple Linear Regression",
    "section": "Summary",
    "text": "Summary\n\n\nMultiple Linear Regression\n\nMore than one predictor\nFit y to multiple x – multiple dimensions (hyperplane)\nPrinciple: minimise sum of squared residuals\nAssumptions: CLINE (collinearity)\nAdjusted R-squared\n\n\nSimple Linear Regression\n\nOne predictor, fit a straight line\nFit straight line between y and x\nPrinciple: minimise sum of squared residuals\nAssumptions: LINE\nMultiple R-squared"
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#introduction",
    "href": "lectures/L11/abalone-quiz.html#introduction",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Introduction",
    "text": "Introduction\nAbalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. Researchers measured 9 attributes of the abalone: sex, length, diameter, height, whole, shucked, viscera, shell, and rings.\nNote: whole, shucked, viscera and shell are weight measurements.\n\nWhat is the response variable?\n\nlength\nrings\nshell (weight)\nwhole (weight)\n\n\n\n\nReading comprehension :)"
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#research-question",
    "href": "lectures/L11/abalone-quiz.html#research-question",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Research question",
    "text": "Research question\nAbalone are marine snails that are a considered a delicacy and very expensive. The older the abalone, the higher the price. Age is determined by counting the number of rings in the shell. To do this, the shell needs to be cut, stained and viewed under a microscope - which is a lot of effort. Researchers measured 9 attributes of the abalone: sex, length, diameter, height, whole, shucked, viscera, shell, and rings.\nNote: whole, shucked, viscera and shell are weight measurements.\nWhat is the best research question, based on the context above?\n\nIs there a correlation between abalone age and weight?\nCan abalone weight be predicted from other measured variables?\nIs there a relationship between abalone size and age?\nCan age be measured by size?\n\n\n\nA is not a complete answer, there are many more predictors. B is incorrect, we care about age/rings. D is incorrect, we are trying to model or predict abalone age from size – terminology matters."
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#explore-data",
    "href": "lectures/L11/abalone-quiz.html#explore-data",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Explore data",
    "text": "Explore data\nWe sample the data to make it easier to visualise relationships. We also remove the sex variable because it is not numeric.\n\n\nCode\nabalone &lt;- read.csv(\"data/abalone.csv\")\n\nset.seed(1113)          # reproducible randomness\nabalone &lt;- abalone %&gt;% \n  select(-sex) %&gt;%      # remove `sex` because it is categorical\n  sample_n(100)         # sample 100 observations for cleaner curve\n  \nstr(abalone)\n\n\n'data.frame':   100 obs. of  8 variables:\n $ length  : num  0.52 0.71 0.33 0.67 0.65 0.35 0.695 0.52 0.6 0.61 ...\n $ diameter: num  0.405 0.57 0.255 0.55 0.51 0.25 0.53 0.41 0.475 0.48 ...\n $ height  : num  0.14 0.195 0.095 0.17 0.19 0.1 0.15 0.14 0.15 0.17 ...\n $ whole   : num  0.692 1.348 0.188 1.247 1.542 ...\n $ shucked : num  0.276 0.8985 0.0735 0.472 0.7155 ...\n $ viscera : num  0.137 0.444 0.045 0.245 0.373 ...\n $ shell   : num  0.215 0.454 0.06 0.4 0.375 ...\n $ rings   : int  11 11 7 21 9 7 14 11 10 10 ..."
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#fit-a-model",
    "href": "lectures/L11/abalone-quiz.html#fit-a-model",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Fit a model",
    "text": "Fit a model\n\n\nWe use natural log transformation on the response variable with log() to account for non-linear relationships.\n\n\nCode\nfit &lt;- lm(log(rings) ~ ., data = abalone)\nsummary(fit)\n\n\n\nCall:\nlm(formula = log(rings) ~ ., data = abalone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37297 -0.12727 -0.01584  0.08787  0.61636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.34626    0.18219   7.389 6.57e-11 ***\nlength      -1.25389    1.50969  -0.831  0.40837    \ndiameter     3.24138    1.91481   1.693  0.09388 .  \nheight       2.26408    1.34813   1.679  0.09646 .  \nwhole        0.03089    0.29250   0.106  0.91612    \nshucked     -1.30902    0.38861  -3.368  0.00111 ** \nviscera     -0.24785    0.55098  -0.450  0.65389    \nshell        1.73328    0.60179   2.880  0.00494 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1996 on 92 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.5897 \nF-statistic: 21.32 on 7 and 92 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhich predictor is NOT significant to the model?\n\nheight\nwhole (weight)\nshucked (weight)\nviscera (weight)\n\n\n\nWhole (weight) has a period (.) beside the p-value – this means the value is less than 0.10, but it needs to be &lt;0.05 to be considered significant."
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#fit-a-model-1",
    "href": "lectures/L11/abalone-quiz.html#fit-a-model-1",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Fit a model",
    "text": "Fit a model\nResidual standard error: 0.1996 on 92 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.5897 \nF-statistic: 21.32 on 7 and 92 DF,  p-value: &lt; 2.2e-16\nWe determine model fit with:\n\nMultiple R-squared and p-value\nAdjusted R-squared and p-value\nAdjusted R-squared and residual standard error\nMultiple R-squared and residual standard error\n\n\n\nWe have multiple variables, so we use the Adjusted R-squared. The p-value tests the hypothesis on whether the model should be used at all, in favour of the mean. The residual error is a measure of model fit."
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#the-problem-with-using-too-many-predictors",
    "href": "lectures/L11/abalone-quiz.html#the-problem-with-using-too-many-predictors",
    "title": "L11 MLR – Abalone Quiz",
    "section": "The problem with using too many predictors",
    "text": "The problem with using too many predictors\nHere, the model is fit with all predictors, then the least significant predictor is removed. This process is repeated until only one predictor remains.\n\n\nCode\nlibrary(broom)\n\nfull7 &lt;- lm(log(rings) ~ ., data = abalone)\npart6 &lt;- update(full7, . ~ . - whole)\npart5 &lt;- update(part6, . ~ . - viscera)\npart4 &lt;- update(part5, . ~ . - length)\npart3 &lt;- update(part4, . ~ . - height)\npart2 &lt;- update(part3, . ~ . - diameter)\npart1 &lt;- update(part2, . ~ . - shucked)\n\nformulas &lt;- c(part1$call$formula, \n              part2$call$formula, \n              part3$call$formula, \n              part4$call$formula, \n              part5$call$formula, \n              part6$call$formula, \n              full7$call$formula)\n\nrs &lt;- bind_rows(glance(part1),\n                glance(part2),\n                glance(part3),\n                glance(part4),\n                glance(part5),\n                glance(part6),\n                glance(full7)) %&gt;%\n  mutate(Model = formulas, n = 1:7) %&gt;%\n  select(Model, n, r.squared, adj.r.squared) %&gt;%\n  mutate_if(is.numeric, round, 3)\n\nknitr::kable(rs)\n\n\n\n\n\n\n\n\n\n\n\nModel\nn\nr.squared\nadj.r.squared\n\n\n\n\nlog(rings) ~ shell\n1\n0.445\n0.439\n\n\nlog(rings) ~ shucked + shell\n2\n0.557\n0.548\n\n\nlog(rings) ~ diameter + shucked + shell\n3\n0.604\n0.591\n\n\nlog(rings) ~ diameter + height + shucked + shell\n4\n0.614\n0.598\n\n\nlog(rings) ~ length + diameter + height + shucked + shell\n5\n0.618\n0.597\n\n\nlog(rings) ~ length + diameter + height + shucked + viscera + , shell\n6\n0.619\n0.594\n\n\nlog(rings) ~ .\n7\n0.619\n0.590\n\n\n\n\n\n\n\nConsidering only \\(R^2\\), which model would we choose?\n\nModel with 1 predictor\nModel with 3 predictors\nModel with 4 predictors\nModel with 7 predictors\n\n\n\n\nThe 1-predictor model sacrifices 14.5% of variation in the response (too much). The 7-predictor model is overfitted (worse than 4-predictor model). Between 3 and 4-predictor models - is a 0.7% improvement worth having to measure height? Realistically, the models with 2 or 3 predictors are justifiable."
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#interpretation",
    "href": "lectures/L11/abalone-quiz.html#interpretation",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Interpretation",
    "text": "Interpretation\n#| eval: false\nCall:\nlm(formula = log(rings) ~ diameter + shucked + shell, data = abalone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30290 -0.15469 -0.03485  0.11454  0.64573 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.4122     0.1594   8.859 4.19e-14 ***\ndiameter      2.0346     0.6034   3.372  0.00108 ** \nshucked      -1.3339     0.2152  -6.200 1.42e-08 ***\nshell         2.0486     0.3672   5.579 2.23e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nWhich is the correct equation?’\n\nlog(rings) = 1.41 + 2.04 x diameter - 1.33 x shucked + 2.04 x shell\nlog(rings) = 1.41 + 2.03 x diameter - 1.33 x shucked + 2.04 x shell\nlog(rings) = 1.41 + 2.04 x diameter + 1.33 x shucked + 2.05 x shell\nlog(rings) = 1.41 + 2.03 x diameter - 1.33 x shucked + 2.05 x shell\n\n\n\n\nAttention to detail :)"
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#interpretation-1",
    "href": "lectures/L11/abalone-quiz.html#interpretation-1",
    "title": "L11 MLR – Abalone Quiz",
    "section": "Interpretation",
    "text": "Interpretation\nThe equation of our model is:\nlog(rings) = 1.41 + 2.03 x diameter + -1.33 x shucked + 2.05 x shell\nBelow are three statements. Given all other predictors are held constant:\n\nrings changes by \\(e^{-1.33}\\) for every percent increase in shucked (weight)\nlog(rings) changes by 1.33 for every unit increase in shucked (weight)\nlog(rings) changes by approximately 1.33% for every percent increase in shucked (weight)\n\n\n\nHow many statements are correct?\n\nnone\n1 statement\n2 statements\nall of them\n\n\n\n\nThe first two are correct, the third is not. The natural log percent change appoximation only applies to small \\(\\beta\\) values below |0.25|."
  },
  {
    "objectID": "lectures/L11/abalone-quiz.html#the-most-important-question",
    "href": "lectures/L11/abalone-quiz.html#the-most-important-question",
    "title": "L11 MLR – Abalone Quiz",
    "section": "The most important question",
    "text": "The most important question\nHow do you feel about regression so far?\n\nEasy\nOK\nHard\nSOS"
  },
  {
    "objectID": "lectures/L10/index.html",
    "href": "lectures/L10/index.html",
    "title": "Lecture 10",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L10 -- Linear functions"
    ]
  },
  {
    "objectID": "lectures/L09/index.html",
    "href": "lectures/L09/index.html",
    "title": "Lecture 09",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L09 -- Describing relationships"
    ]
  },
  {
    "objectID": "lectures/L08/index.html",
    "href": "lectures/L08/index.html",
    "title": "Lecture 08",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L08 -- Bootstrapping"
    ]
  },
  {
    "objectID": "lectures/L07/index.html",
    "href": "lectures/L07/index.html",
    "title": "Lecture 07",
    "section": "",
    "text": "Lecture 07\nFull Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L07 -- Non-parametric tests"
    ]
  },
  {
    "objectID": "lectures/L06/index.html",
    "href": "lectures/L06/index.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Lecture 06a\nFull Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L06 -- Two-sample *t*-tests"
    ]
  },
  {
    "objectID": "lectures/L05/index.html",
    "href": "lectures/L05/index.html",
    "title": "Lecture 05",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 2 inference**",
      "L05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "lectures/L04/index.html",
    "href": "lectures/L04/index.html",
    "title": "Lecture 04",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L04 -- The central limit theorem"
    ]
  },
  {
    "objectID": "lectures/L03/index.html",
    "href": "lectures/L03/index.html",
    "title": "Lecture 03",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "lectures/L02/index.html",
    "href": "lectures/L02/index.html",
    "title": "Lecture 02",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L02 -- Introduction to statistical programming"
    ]
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#about-us",
    "href": "lectures/L01/lecture-01a.html#about-us",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "About us…",
    "text": "About us…\nLiana Pozza - Unit Coordinator\n\nRoom 303, Level 3, Biomedical Building C81, Australian Technology Park, Eveleigh\nPh: 02 8627 1012\nEmail: liana.pozza@sydney.edu.au\n\n\nYour Lecturers\n\n\n\nJanuar Harianto\nWeeks 1 – 4\n\n\nFloris van Ogtrop\nWeeks 5 – 8\n\n\nLiana Pozza\nWeeks 9 – 12"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#about-envx1002",
    "href": "lectures/L01/lecture-01a.html#about-envx1002",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "About ENVX1002",
    "text": "About ENVX1002\n\nLearning outcomes\n\nLO1. Implement basic reproducible research practices – including consistent data organisation, documented code, and version-controlled workflows so that statistical analyses and results can be readily replicated and validated by others.\nLO2. Demonstrate proficiency in utilising R and Excel to effectively explore and describe life science datasets.\nLO3. Apply parametric and non-parametric statistical inference methods to experimental and observational data using RStudio and effectively interpret and communicate the results in the context of the data.\nLO4. Be able to put into practice both linear and non-linear models to describe relationships between variables using RStudio and Excel, demonstrating creativity in developing models that effectively represent complex data patterns.\nLO5. Be able to articulate statistical and modelling results clearly and convincingly in both written reports and oral presentations, working effectively as an individual and collaboratively in a team, showcasing the ability to convey complex information to varied audiences."
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#delivery-format",
    "href": "lectures/L01/lecture-01a.html#delivery-format",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Delivery format",
    "text": "Delivery format\nAll lectures are held in ABS Lecture Theatre 1130.\nLab sessions are held in the Biomedical Building C81, 1 Central Avenue, Eveleigh.\n\nLectures (hybrid): deliver content, provide context, and introduce new concepts, applying concepts\nLabs: hands-on practice with R and data analysis, with demonstrators to help you\n\n\nThe following are optional (but highly recommended):\n\nDrop-in sessions: additional help and support, mostly on Zoom\nEd discussion: online forum for questions and discussions"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#timetable",
    "href": "lectures/L01/lecture-01a.html#timetable",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Timetable",
    "text": "Timetable\n\nLectures (hybrid)\n\nMonday 12pm–1pm, ABS Lecture Theatre 1130\nTuesday 9am–11am, ABS Lecture Theatre 1130\n\n\n\nComputer Labs\n\n2-hour in-person lab session with tutors and demonstrators\nBiomedical Building C81, Australian Technology Park, Eveleigh\nSee timetable for your allocated time"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#schedule-at-a-glance",
    "href": "lectures/L01/lecture-01a.html#schedule-at-a-glance",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Schedule at a glance…",
    "text": "Schedule at a glance…\n\n\nCode\nsequenceDiagram\n  participant M as Mon\n  participant T as Tue\n  participant W as Wed\n  participant Th as Thu\n  participant F as Fri\n  participant S as Sat\n  participant Su as Sun\n\n  Note over M,T: Lectures (hybrid) - ABS LT 1130\n  Note over T,Th: Lab Sessions - Biomedical Building\n  Th-&gt;&gt;+Su: Self-revision, pick ONE day (encouraged)\n\n\n\n\n\nsequenceDiagram\n  participant M as Mon\n  participant T as Tue\n  participant W as Wed\n  participant Th as Thu\n  participant F as Fri\n  participant S as Sat\n  participant Su as Sun\n\n  Note over M,T: Lectures (hybrid) - ABS LT 1130\n  Note over T,Th: Lab Sessions - Biomedical Building\n  Th-&gt;&gt;+Su: Self-revision, pick ONE day (encouraged)"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#resources",
    "href": "lectures/L01/lecture-01a.html#resources",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Resources",
    "text": "Resources\n\nCanvas (of course)\nENVX-resources – GitHub repository for our open-source materials\nEd Discussion – main platform for ANNOUNCEMENTS and Q&A"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#where-are-the-labs",
    "href": "lectures/L01/lecture-01a.html#where-are-the-labs",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Where are the Labs?",
    "text": "Where are the Labs?\n\nLab sessions include extra time (30 minutes) for travel – already programmed in the timetable (so clashes are avoided)\nWe are working on securing a free shuttle service between campus and the labs - stay tuned!\nTake advantage of the new community access gates at Redfern Station: saves 5 minutes"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#campus-bus",
    "href": "lectures/L01/lecture-01a.html#campus-bus",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Campus bus",
    "text": "Campus bus\nThere is currently a bus that goes to Redfern Station, but nothing out to C81 yet. You can find the timetable here: Campus Bus Timetables\nWe are looking to secure a bus service to C81, but need an idea of numbers first.\nIf you are interested in catching a shuttle bus to C81, please fill out this short survey:"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#topic-outline",
    "href": "lectures/L01/lecture-01a.html#topic-outline",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Topic outline",
    "text": "Topic outline\n\nWeek 01 - Data: Reproducible Science\nWeek 02 - Data: Statistical Programming Basics\nWeek 03 - Data: Exploring and Visualising Data\nWeek 04 - Data: The Central Limit Theorem\n\n\n\nWeek 05 - Inference: Introduction to Inference\nWeek 06 - Inference: Comparing Two Samples\nWeek 07 - Inference: Non-parametric Methods\nWeek 08 - Inference: Building Statistical Confidence\n\n\n\n\nWeek 09 - Modelling: Describing Relationships\nWeek 10 - Modelling: Simple Linear Regression\nWeek 11 - Modelling: Multiple Linear Regression\nWeek 12 - Modelling: Non-linear Regression\n\n\n\n\nWeek 13 - Revision: Course revision and past exam questions"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#assessments",
    "href": "lectures/L01/lecture-01a.html#assessments",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Assessments",
    "text": "Assessments\n\n\nCode\n# calculate this year's year number\nlibrary(lubridate)\nyear &lt;- year(Sys.Date())\naddress &lt;- paste0(\n  \"https://www.sydney.edu.au/units/ENVX1002/\",\n  year,\n  \"-S1C-ND-CC\"\n)\n\n\nThe most up to date (and slightly more comprehensive) information for 2026 is here. In a nutshell:\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Week\n                Assessment\n                Description\n              \n        \n        \n        \n                \n                  3\n                  Early Feedback Task (individual 5%)\n                  In-person - 15 minutes\n                \n                \n                  5\n                  Describing Data Report (individual 15%)\n                  Written report, 500 words\n                \n                \n                  8\n                  Coding and data skills evaluation (individual 15%)\n                  In-person - 50 minutes\n                \n                \n                  13\n                  Group presentation: Modelling relationships in data (10% + Peer assessment 5%)\n                  Group presentation - 5 minutes\n                \n                \n                  Exam\n                  Final exam (individual 50%)\n                  MCQ + SAQ Questions - 2 hours\n                \n        \n      \n    \n\n\n\n\nWeek 3: The early feedback quiz is a chance for us to gauge your understanding and provide feedback\nWeek 8: Coding and data skills evaluation covers R data manipulation and analysis\nFinal exam will NOT require you to write or interpret code – focus on understanding concepts and interpreting results"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#final-exam-hurdle",
    "href": "lectures/L01/lecture-01a.html#final-exam-hurdle",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Final exam hurdle",
    "text": "Final exam hurdle\nThe final exam is a hurdle assessment for this unit. This means:\n\nYou must attempt the exam and achieve a minimum score of 40% to pass the unit.\nStudents who do not meet this requirement will not be able to pass the unit, regardless of their overall mark.\n\nThis hurdle can be quite daunting, but we will are here to work with you and help you succeed.\nWe will provide the learning materials, guidance, and any support you need, and it is your responsibility to keep up with the content each week and ask us questions if there is something that isn’t quite making sense yet."
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#baby-steps",
    "href": "lectures/L01/lecture-01a.html#baby-steps",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Baby steps…",
    "text": "Baby steps…\n\nThis unit is designed for beginners - no prior statistics or programming required\nWe start with basics – pace increases after week 4\nFocus on understanding concepts first, then tools\nWe provide plenty of support – more on this later"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#our-tech-stack",
    "href": "lectures/L01/lecture-01a.html#our-tech-stack",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Our tech stack",
    "text": "Our tech stack\n\nMS Excel – for data entry and basic analysis\nR – a programming language for data analysis\nRStudio – an integrated development environment (IDE) for R\nQuarto (Markdown) – a key platform for reproducible reports and documents\nGitHub Copilot – AI-powered code completion tool. Optional, but highly recommended"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#ms-excel",
    "href": "lectures/L01/lecture-01a.html#ms-excel",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "MS Excel",
    "text": "MS Excel\n\nWidely used for data entry and basic analysis\nA standard tool in many industries, including science, often to store data\nCan be a useful complement to R for data cleaning and simple calculations\nA stepping stone to more advanced tools?"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#r",
    "href": "lectures/L01/lecture-01a.html#r",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "R",
    "text": "R\n\n\n\n\n  \n\nA free, open-source programming language\nWidely used for data analysis and statistics\nStandard tool in scientific research\nExtensive collection of packages for data science\nStrong support for creating publication-quality graphics\nLarge, active community for help and resources"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#why-r",
    "href": "lectures/L01/lecture-01a.html#why-r",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Why R?",
    "text": "Why R?\n\nBuilt for beginners\nMakes your work reproducible\nPowerful yet accessible\n\n\nImportantly – the skills you learn are highly transferable to other tools and languages.\nMost easily integrated with generative AI tools – more on this soon\nWell-documented and discussed online (so you can find help easily)"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#rstudio",
    "href": "lectures/L01/lecture-01a.html#rstudio",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "RStudio",
    "text": "RStudio\n\nNOT the same as R – it’s an integrated development environment (IDE)\nRuns R (…and Python, and SQL, and more)\nMakes it easier to write and run R code by providing a significantly more user-friendly interface"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#starting-with-r",
    "href": "lectures/L01/lecture-01a.html#starting-with-r",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Starting with R",
    "text": "Starting with R\n\nIt’s normal to feel overwhelmed at first\nWe’ll learn step by step\nPractice is key - a little bit each day helps\nDon’t hesitate to ask questions!"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#satisfying-when-it-works",
    "href": "lectures/L01/lecture-01a.html#satisfying-when-it-works",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Satisfying when it works",
    "text": "Satisfying when it works\n\n\nClick to see the code for this animation\n# Load required packages\nlibrary(gapminder) # Dataset of country statistics over time\nlibrary(gganimate) # For creating animations in ggplot\nlibrary(tidyverse) # Collection of data science packages\n\n# Create an animated plot showing how life expectancy relates to GDP\n# across different continents over time\nggplot(\n  gapminder,\n  aes(gdpPercap, lifeExp, # GDP per capita vs life expectancy\n    size = pop, # Point size represents population\n    colour = country\n  )\n) + # Each country gets its own color\n  geom_point(\n    alpha = 0.7, # Semi-transparent points\n    show.legend = FALSE\n  ) + # Hide legend for cleaner look\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) + # Set min/max point sizes\n  scale_x_log10() + # Log scale for GDP (wide range)\n  facet_wrap(~continent) + # Separate plot for each continent\n  labs(\n    title = \"Year: {frame_time}\",\n    x = \"GDP per capita\",\n    y = \"Life expectancy\"\n  ) +\n  transition_time(year) + # Animate through years\n  ease_aes(\"linear\") # Smooth transitions"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#quarto",
    "href": "lectures/L01/lecture-01a.html#quarto",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "Quarto",
    "text": "Quarto\n\nMarjority of our resources are built using Quarto – a markdown-based document format that you will learn to use in this unit\n\nLecture slides\nTutorials\nLab exercises\n\nQuarto makes everything reproducible - what does it mean?\nFree and open source, available on the ENVX resources GitHub repository – re-use and modify as you wish (but follow CC BY 4.0)\n\n\n## Quarto\n\n- Marjority of our resources are built using [**Quarto**](https://quarto.org/) -- a markdown-based document format that **you will learn to use** in this unit\n  - Lecture slides\n  - Tutorials\n  - Lab exercises\n- Quarto  makes everything **reproducible** - what does it mean?\n- Free and open source, available on the [ENVX resources](https://github.com/ENVX-resources) GitHub repository -- re-use and modify as you wish (but follow [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#r-rstudio-quarto",
    "href": "lectures/L01/lecture-01a.html#r-rstudio-quarto",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "R, RStudio, Quarto!?",
    "text": "R, RStudio, Quarto!?\n\nAgain, it’s normal to feel overwhelmed at first\nThese technologies are complementary – everything is implemented in RStudio\nThe tutorials and labs will guide you through the process"
  },
  {
    "objectID": "lectures/L01/lecture-01a.html#github-copilot",
    "href": "lectures/L01/lecture-01a.html#github-copilot",
    "title": "Lecture 01a – Welcome to ENVX1002",
    "section": "GitHub Copilot",
    "text": "GitHub Copilot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX1002 – Statistics in Life and Environmental Sciences",
    "section": "",
    "text": "This site contains the lecture material for ENVX1002. It is meant for lecturers to publish their lecture content in a structured way and has not been designed for student use. Most of the content is still in development and will be updated throughout the semester. If you happen to stumble upon this site, feel free to have a look around… but be aware that the content is not final and may contain errors.\nENVX1002 students should always navigate to Canvas to access the ENVX1002 page and view the course material in the right context.\nModule 1 – describing data\n\nLecture 01 – Reproducible Science\nLecture 02 – Statistical Programming Basics\nLecture 03 – Exploring and visualising data\nLecture 04 – The Central Limit Theorem\n\nModule 2 – inference\n\nLecture 05 – Introduction to Inference\nLecture 06 – Comparing Two Samples\nLecture 07 – Non-parametric Methods\nLecture 08 – Building Statistical Confidence\n\nModule 3 – modelling\n\nLecture 09 – Describing Relationships\nLecture 10 – Simple Linear Regression\nLecture 11 – Multiple Linear Regression\nLecture 12 – Non-linear Regression",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Home**"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lectures/L01/index.html",
    "href": "lectures/L01/index.html",
    "title": "Lecture 01 – Reproducible science",
    "section": "",
    "text": "Lecture 01a – Welcome to ENVX1002 Full Screen | PDF\n\nLecture 01b – Reproducible science Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 1 describing data**",
      "L01 -- Introduction"
    ]
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#why-learn-statistics",
    "href": "lectures/L01/lecture-01b.html#why-learn-statistics",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Why learn statistics?",
    "text": "Why learn statistics?\nAll of science (and industry) are increasingly data-driven and computational:\n\nResearch papers are filled with statistical analyses\nBusiness and policy decisions are based on data analytics\nEnvironmental policies are guided by statistical models\nMedical treatments are evaluated using statistical methods\n\nMost of you are majoring in a field that will require you to understand and use statistics in some form."
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#benefits",
    "href": "lectures/L01/lecture-01b.html#benefits",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Benefits",
    "text": "Benefits\nEven if you don’t become a data scientist, statistics will help you to:\n\n\nEvaluate claims critically\n\nUnderstand and analyse data in your field\nMake informed decisions based on evidence\n\n\n\n\n\nCommunicate effectively\n\nCreate compelling data visualisations and reports\nPresent findings clearly to different audiences\n\n\n\n\n\nSolve real-world problems\n\nDesign and analyse experiments properly\nMake evidence-based predictions and identify trends"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#the-joy-of-stats",
    "href": "lectures/L01/lecture-01b.html#the-joy-of-stats",
    "title": "Lecture 01b – Reproducible Science",
    "section": "The joy of stats",
    "text": "The joy of stats\n200 countries, 200 years, 4 minutes\nIn your own time: The best stats you’ve ever seen"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#lionel-messi-is-impossible",
    "href": "lectures/L01/lecture-01b.html#lionel-messi-is-impossible",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Lionel Messi is impossible",
    "text": "Lionel Messi is impossible\n\nIt’s not possible to shoot more efficiently from outside the penalty area than many players shoot inside it. It’s not possible to lead the world in weak-kick goals and long-range goals. It’s not possible to score on unassisted plays as well as the best players in the world score on assisted ones. It’s not possible to lead the world’s forwards both in taking on defenders and in dishing the ball to others. And it’s certainly not possible to do most of these things by insanely wide margins.\nBut Messi does all of this and more.\n\n\nMessi playing for ArgentinaImage credit: Кирилл Венедиктов, CC BY-SA 3.0 GFDL, via Wikimedia Commons"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#lionel-messi-is-impossible-1",
    "href": "lectures/L01/lecture-01b.html#lionel-messi-is-impossible-1",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Lionel Messi is impossible",
    "text": "Lionel Messi is impossible\n\nSource: fivethirtyeight"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#lionel-messi-is-impossible-2",
    "href": "lectures/L01/lecture-01b.html#lionel-messi-is-impossible-2",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Lionel Messi is impossible",
    "text": "Lionel Messi is impossible\n\nSource: fivethirtyeight"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#serious-stats",
    "href": "lectures/L01/lecture-01b.html#serious-stats",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Serious stats",
    "text": "Serious stats\nWhich sepal length is longer?\n\nSource: Embedded RoboticsNote: common dataset used in statistics and machine learning."
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#serious-stats-1",
    "href": "lectures/L01/lecture-01b.html#serious-stats-1",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Serious stats",
    "text": "Serious stats\nVisualise\n\n\nCode\n# load libraries\npacman::p_load(ggplot2, rstatix, gt)\n\n# create boxplot\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot() +\n  theme_classic() +\n  labs(y = \"Sepal Length (cm)\", title = \"Sepal Length by Species\")"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#serious-stats-2",
    "href": "lectures/L01/lecture-01b.html#serious-stats-2",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Serious stats",
    "text": "Serious stats\nInfer\nWe use formal statistical tests to determine if differences are statistically significant so that we can make inferences about the population based on the sample data – part of the scientific method.\n\n\nCode\n# run ANOVA\nmodel &lt;- aov(Sepal.Length ~ Species, data = iris)\nf_stat &lt;- summary(model)[[1]]$`F value`[1]\np_val &lt;- summary(model)[[1]]$`Pr(&gt;F)`[1]\nrstatix::anova_summary(model) |&gt;\n  gt() |&gt;\n  tab_caption(\n    caption = \"Table 1: One-way ANOVA results comparing sepal length between iris species\"\n  )\n\n\n\n\n\n\nTable 1: One-way ANOVA results comparing sepal length between iris species\n\n\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\nSpecies\n2\n147\n119.265\n1.67e-31\n*\n0.619\n\n\n\n\n\n\n\nScientific reporting\n\nA one-way ANOVA revealed significant differences in sepal length between species (ANOVA, F(2, 147) = 119.26, p &lt; .001)."
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#not-always-formal",
    "href": "lectures/L01/lecture-01b.html#not-always-formal",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Not always formal",
    "text": "Not always formal\n\n\n\n\n\nSource: You scrolled 69,420 bananas this year\n\n\n\n\n\n\nSource: “Guys where do you pee?”\n\n\n\nThe beauty of statistics – formal hypothesis testing is not always required to make a point!"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#science-as-an-enterprise",
    "href": "lectures/L01/lecture-01b.html#science-as-an-enterprise",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Science as an enterprise",
    "text": "Science as an enterprise\n\nThe scientific method – fundamental to centuries of scientific progress\nIf you discover something (or not), it should be possible for others to verify your findings independently\nYour findings should be reproducible and replicable\n\n\nThe logical framework by Underwood (1997)"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#no-single-method",
    "href": "lectures/L01/lecture-01b.html#no-single-method",
    "title": "Lecture 01b – Reproducible Science",
    "section": "No single method",
    "text": "No single method\nVariation of the scientific method exist– it is a framework that guides the process of scientific inquiry"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#no-single-method-1",
    "href": "lectures/L01/lecture-01b.html#no-single-method-1",
    "title": "Lecture 01b – Reproducible Science",
    "section": "No single method",
    "text": "No single method\nHATPC\nHypothesis – Assumptions – Test statistic – P-value – Conclusion\n\n\n\n\n\n\n\nYou will see some variation of the HATPC in your first-year units – a common framework for report writing"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#key-principles",
    "href": "lectures/L01/lecture-01b.html#key-principles",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Key principles",
    "text": "Key principles\n\nObservation: Identify a phenomenon of interest that can be measured\nQuestion: Formulate a question that can be answered by collecting data\nResearch: Review the literature to understand what is already known – your question may already have been asked by someone else. This step helps in understanding what is already known and what gaps in knowledge may exist.\nHypothesis: Formulate a testable hypothesis – something that can be assessed using data collection and analysis\nExperiment: Design an experiment to test the hypothesis\nData collection: Collect data\nAnalysis: use statistical methods to analyse the data and determine if results are statistically significant or demonstrate a pattern\nConclusion: Interpret the results and draw conclusions. If the results are not significant, this is still a valid conclusion!"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#reproducibility-crisis-despite-the-scientific-method",
    "href": "lectures/L01/lecture-01b.html#reproducibility-crisis-despite-the-scientific-method",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Reproducibility crisis despite the scientific method",
    "text": "Reproducibility crisis despite the scientific method\nFrom Nature (including image sources):\n\nMore than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments.\n\n\n\n\n\n\nStatistical analysis, experimental design and data issues are the main factors affecting research reproducibility."
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#key-definitions",
    "href": "lectures/L01/lecture-01b.html#key-definitions",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Key definitions",
    "text": "Key definitions\n\nReproducibility: the ability to re-run an analysis and obtain the same results\nReplicability: the ability to obtain the same conclusions using a different dataset or study population\n\nScientific findings should be both reproducible and replicable – the tools that we use should facilitate this in the most efficient way possible."
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#reproducibility",
    "href": "lectures/L01/lecture-01b.html#reproducibility",
    "title": "Lecture 01b – Reproducible Science",
    "section": "Reproducibility",
    "text": "Reproducibility\nHow would you explain to someone how to reproduce this plot…\n\nin Excel? Check the guide\nIn SPSS? Check the guide\nIn R?\n\n\n\n\nCode\nlibrary(palmerpenguins)\nboxplot(bill_length_mm ~ species, data = penguins)"
  },
  {
    "objectID": "lectures/L01/lecture-01b.html#an-over-simplification",
    "href": "lectures/L01/lecture-01b.html#an-over-simplification",
    "title": "Lecture 01b – Reproducible Science",
    "section": "An over-simplification",
    "text": "An over-simplification\nThose without programming knowledge will still struggle to understand and use the two lines of R code shown above.\nPre-requisites\n\nUnderstanding of the R programming language\nKnowing how to debug (i.e. read error messages or “play” with code)\nIt takes time, but the payoff is worth it – all programming languages follow similar principles and you will find others easier to learn, even if not for statistics…"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#after-this-week-you-will-be-able-to",
    "href": "lectures/L02/lecture-02.html#after-this-week-you-will-be-able-to",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "After this week, you will be able to:",
    "text": "After this week, you will be able to:\n\nNavigate and use the RStudio interface effectively\nExecute basic R functions and understand their syntax\nFeel confident explaining basic statistical concepts like samples and populations\nUnderstand and explain measures of central tendency (mean, median, mode) clearly and without mathematical jargon\nMaster different measures of spread (range, IQR, variance, standard deviation) through practical examples\nCalculate statistical measures using both R and Excel\nChoose appropriate statistical measures for your biological data and explain your choices"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#quick-checklist",
    "href": "lectures/L02/lecture-02.html#quick-checklist",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Quick checklist",
    "text": "Quick checklist\nBy now you should have…\n\nInstalled R\nInstalled RStudio\nCreated one (or two) documents in Markdown using Quarto"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#from-calculators-to-computers",
    "href": "lectures/L02/lecture-02.html#from-calculators-to-computers",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "From calculators to computers",
    "text": "From calculators to computers\n\n\n\n\n\n\n1800s: Mechanical calculators. Source\n\n\n\n\n\n\n\n1960s: Statistical software BMDP and SPSS (not in image). Source"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#statistical-software-in-the-1970s",
    "href": "lectures/L02/lecture-02.html#statistical-software-in-the-1970s",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Statistical software in the 1970s",
    "text": "Statistical software in the 1970s\n\n\n\n\n\n\n1970s: SAS (Statistical Analysis System) Source\n\n\n\n\n\n\n\n1976: Birth of S at Bell Labs. S-PLUS debuts in 1988. Source"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#r-in-todays-world",
    "href": "lectures/L02/lecture-02.html#r-in-todays-world",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "R in today’s world",
    "text": "R in today’s world\n\nLeading tool in data science and statistics (although Python leads in majority of machine learning workflows)\nOver 22,000 packages on CRAN – extensive statistical capabilities\nIntegration with other modern tools: Python, HTML, Javascript, Excel, AJAX…\nMeets modern academic standards of reproducibility and increasingly preferred by statisticians\n\n\nRStudio IDE. Source: Januar Harianto"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#your-rstudio-workspace",
    "href": "lectures/L02/lecture-02.html#your-rstudio-workspace",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Your RStudio workspace",
    "text": "Your RStudio workspace\n\nLeft: all input, Right: all output"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#we-will-always-work-in-quarto",
    "href": "lectures/L02/lecture-02.html#we-will-always-work-in-quarto",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "We will always work in Quarto",
    "text": "We will always work in Quarto\n\nA Markdown-based authoring tool\nAllows you to write reproducible documents with code and text\nOutputs to various formats: HTML, PDF, Word, and more\nQuarto gallery"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#how-quarto-works",
    "href": "lectures/L02/lecture-02.html#how-quarto-works",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "How Quarto works",
    "text": "How Quarto works\n\nWrite your content in Markdown (left panel)\nView the output in Preview (right panel)\nQuick demo (only available in the live lecture)"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#essential-resources",
    "href": "lectures/L02/lecture-02.html#essential-resources",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Essential Resources",
    "text": "Essential Resources\n\nRead “A brief R guide for surviving ENVX1002” by Dr. Geoffrey Mazue\n\nAvailable in the Tool Kit section on Canvas\nContains essential R programming tips and best practices for this unit\n\nUse the Help panel in RStudio"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#population-vs-sample",
    "href": "lectures/L02/lecture-02.html#population-vs-sample",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Population vs Sample",
    "text": "Population vs Sample\n\n\n\n\n\n\nPopulation\n\nAll possible observations\nUsually too large to measure\nExample: All trees in a forest\n\nSample\n\nSubset of the population\nWhat we actually measure\nExample: 100 trees measured in a forest\n\nMost (if not all) statistical analyses are based on samples, not populations."
  },
  {
    "objectID": "lectures/L02/lecture-02.html#populations-and-their-samples",
    "href": "lectures/L02/lecture-02.html#populations-and-their-samples",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Populations and their samples",
    "text": "Populations and their samples\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\nAll koalas in Australia\n150 koalas studied in NSW\n\n\nEvery fish in Sydney Harbour\n300 fish caught in specific locations\n\n\nAll soil bacteria in a forest\nBacteria from 50 soil cores\n\n\nAll cookies in a bakery\nTasting 3 cookies to judge quality\n\n\nAll students at the university\nThe 200 students in this course\n\n\nWater quality in the entire ocean\nWater samples from specific locations\n\n\nAll trees in a national park\n75 trees measured in random plots\n\n\nAll possible blood test results\nBlood samples from 100 patients\n\n\n\nDisclaimer: use of GenAI for content generation in this slide."
  },
  {
    "objectID": "lectures/L02/lecture-02.html#how-well-does-a-sample-represent-the-population",
    "href": "lectures/L02/lecture-02.html#how-well-does-a-sample-represent-the-population",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "How well does a sample represent the population?",
    "text": "How well does a sample represent the population?\nIt depends:\n\nSample size: Larger samples are more likely to represent the population\nSampling method: Random samples are more likely to be representative\nPopulation variability: More variability means larger samples are needed\n\nIn reality, we often have to balance these factors due to time, cost, and practical constraints."
  },
  {
    "objectID": "lectures/L02/lecture-02.html#samples-vary",
    "href": "lectures/L02/lecture-02.html#samples-vary",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Samples vary",
    "text": "Samples vary\nDifferent samples give different results – suppose we have a population of 1000 trees and we randomly sample 6 tree heights. If this is done 3 times, it is likely that the samples will be different.\nWe have code to demonstrate this but just focus on the results for now:\n\n\nCode\nset.seed(258) \npopulation &lt;- rnorm(1000, mean = 12, sd = 5)\n\n# create samples\nsample1 &lt;- round(sample(population, size = 6), 1)\nsample2 &lt;- round(sample(population, size = 6), 1)\nsample3 &lt;- round(sample(population, size = 6), 1)\n# show samples\nfor (i in 1:3) {\n   cat(sprintf(\"Sample %d: \", i), get(paste0(\"sample\", i)), \"\\n\")\n}\n\n\nSample 1:  13.7 14.6 14.8 9.6 6.5 10 \nSample 2:  7.6 6.1 9.9 10.1 12.5 14.9 \nSample 3:  9.8 7.9 18.4 19.1 7 26.1 \n\n\nAre the samples different? How different are they?"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#descriptive-statistics",
    "href": "lectures/L02/lecture-02.html#descriptive-statistics",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nWe use descriptive statistics to summarise and describe data, helping us compare and contrast.\n\nMeasures of central tendency – describe the “typical” value in a sample\n\nmean, median, mode\n\nMeasures of spread – describe how much the data varies\n\nstandard deviation, variance (commonly used)\nrange, quartiles, IQR (for unique cases)\n\nMeasures of uncertainty – describe how confident we are in our estimates\n\nstandard error\nconfidence intervals"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#mean-also-known-as-the-average",
    "href": "lectures/L02/lecture-02.html#mean-also-known-as-the-average",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Mean – also known as the average",
    "text": "Mean – also known as the average\n\nAdd up all your numbers\nDivide by how many numbers you have\n\n\n\n\n\n\n\nMathematical notation\n\n\n\nPopulation mean: \\(\\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\\)\nSample mean: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\nWhere \\(x_i\\) is each individual value, \\(N\\) is population size, and \\(n\\) is sample size."
  },
  {
    "objectID": "lectures/L02/lecture-02.html#mean-in-excel",
    "href": "lectures/L02/lecture-02.html#mean-in-excel",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Mean in Excel",
    "text": "Mean in Excel\n\n\nExcel offers several ways to calculate the mean:\n\nUsing AVERAGE function\n=AVERAGE(A1:A4)\n\nType =AVERAGE(\nSelect cells with your data\nPress Enter\n\nUsing AutoCalculate\n\nSelect your data cells\nLook at bottom right\nAverage shown automatically"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#mean-in-r",
    "href": "lectures/L02/lecture-02.html#mean-in-r",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Mean in R",
    "text": "Mean in R\nI’ll show you how to do this in RStudio.\nStep 1: Create a vector of values\nIn R, we store data in vectors (lists of values):\n\n# Create a vector of test scores\nscores &lt;- c(80, 85, 90, 95)\n\n# Display the scores\nscores\n\n[1] 80 85 90 95\n\n\nStep 2: Calculate\n\n\nEither calculate the mean manually:\n\n# Sum divided by count\nsum(scores) / length(scores)\n\n[1] 87.5\n\n\n\nOr use the mean() function:\n\n# The mean() function does the work for us\nmean(scores)\n\n[1] 87.5"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#median-the-middle-value",
    "href": "lectures/L02/lecture-02.html#median-the-middle-value",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Median – the middle value",
    "text": "Median – the middle value\nThe median is the middle number when your data is in order:\n\nFirst, put your numbers in order\nFind the middle value\nIf you have an even number of values, take the average of the two middle numbers\n\n\nExample: House prices ($’000s): 450, 1100, 480, 460, 470, 420, 1400, 450, 470\n\nOrder: 450, 450, 420, 460, 470, 470, 480, 1100, 1400\n\n\nHow is it useful?"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#median-in-excel",
    "href": "lectures/L02/lecture-02.html#median-in-excel",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Median in Excel",
    "text": "Median in Excel\nExcel provides two main ways to find the median:\n\nUsing MEDIAN function\n=MEDIAN(A1:A9)\n\nType =MEDIAN(\nSelect your data range\nPress Enter\n\nAlternative method\n\nSort your data first (use the Sort functionality in the Data tab)\nFind middle value(s)\nIf even number of values, average the middle two"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#median-in-r",
    "href": "lectures/L02/lecture-02.html#median-in-r",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Median in R",
    "text": "Median in R\nR does all the ordering and finding the middle for us:\n\n# House prices\nprices &lt;- c(450, 1100, 480, 460, 470, 420, 1400, 450, 470)\n\n# Find median\nmedian(prices)\n\n[1] 470\n\n\n\nComparing the mean and median:\n\n# Compare with mean\nmean(prices)\n\n[1] 633.3333\n\n\n\n\nWhich is a better measure for house prices?"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#mode-most-frequent-value",
    "href": "lectures/L02/lecture-02.html#mode-most-frequent-value",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Mode – most frequent value",
    "text": "Mode – most frequent value\n\nThe mode is the value that appears most frequently in your data. It’s particularly useful for:\n\nCategorical data (like blood types, eye colors)\nFinding the most common item in a group\nData that has clear repeated values\n\n\n\nCalculating the mode can be tricky, especially if there are multiple modes or no mode at all. This is why the mode is not commonly used in statistics.\n\n\nQuestions that the mode can answer\n\nWhat is the most common blood type in a population?\nWhat is the most common eye color in a group of people?"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#mode-in-excel",
    "href": "lectures/L02/lecture-02.html#mode-in-excel",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Mode in Excel",
    "text": "Mode in Excel\nExcel provides several methods to find the mode but the simplest is to use the MODE function:\n=MODE(A1:A10)\n\nType =MODE(\nSelect your data range\nPress Enter"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#mode-in-r",
    "href": "lectures/L02/lecture-02.html#mode-in-r",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Mode in R",
    "text": "Mode in R\nThere is no built-in function to calculate the mode, so we use the modeest package:\n\nif(!require(\"modeest\")) install.packages(\"modeest\")\nlibrary(modeest)\n\ndf &lt;- c(1, 2, 3, 3, 4, 5, 5, 5, 6)\nmlv(df, method = \"mfv\")  # most frequent value\n\n[1] 5\n\n\n\nIf you were to do it yourself, how would you do it in R?\n\n\n\nUsing frequenciesUsing run-length encodingUsing loops\n\n\nUse the table() function to count frequencies:\n\nfreq_table &lt;- table(df) # Count frequencies of each value\n# Find which value(s) appear most often\nmodes &lt;- as.numeric(names(freq_table[freq_table == max(freq_table)]))\nmodes\n\n[1] 5\n\n\n\n\nUse run-length encoding after sorting:\n\nsorted_df &lt;- sort(df) # Sort the vector first\nruns &lt;- rle(sorted_df) # Use run-length encoding to find sequences\nmodes &lt;- runs$values[runs$lengths == max(runs$lengths)] # Find the value(s) with max length\nmodes\n\n[1] 5\n\n\n\n\nLoop through the vector and count occurrences:\n\nunique_vals &lt;- factor(df) # Create a factor of unique values\ncounts &lt;- tapply(df, unique_vals, length) # Count occurrences using tapply\nmodes &lt;- as.numeric(names(counts[counts == max(counts)])) # Find which values have the maximum count\nmodes\n\n[1] 5\n\n\n\n\n\n\n\nThe point is that it doesn’t matter how you calculate the mode, as long as you are able to do it. Also – if you needed this – aren’t you glad R has a package for it?"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#a-biological-example",
    "href": "lectures/L02/lecture-02.html#a-biological-example",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "A biological example",
    "text": "A biological example\n\nSource: Adobe Stock # 85659279Imagine sampling seagrass blade lengths from two different sites in a marine ecosystem, and they have the same mean length of 15.2 cm. Are both sites the same?\n\n\nSite A (Protected Bay): 15.2, 15.0, 15.3, 15.1, 15.2 centimetres\nSite B (Wave-exposed Coast): 12.0, 18.0, 14.5, 16.5, 15.0 centimetres"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#comparing-different-measures",
    "href": "lectures/L02/lecture-02.html#comparing-different-measures",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Comparing Different Measures",
    "text": "Comparing Different Measures\n\n\nCode\n# Plot seagrass lengths\nlibrary(ggplot2)\nlibrary(patchwork)\n\nseagrass_protected &lt;- c(15.2, 15.0, 15.3, 15.1, 15.2)\nseagrass_exposed &lt;- c(12.0, 18.0, 14.5, 16.5, 15.0)\n\n# Create plots for both sites\np1 &lt;- ggplot() +\n   geom_point(aes(x = 1:5, y = seagrass_protected), size = 3) +\n   geom_hline(yintercept = mean(seagrass_protected), linetype = \"dashed\", color = \"red\") +\n   labs(title = \"Site A: Protected Bay\", x = \"Measurement\", y = \"Length (cm)\") +\n   ylim(10, 20)\n\np2 &lt;- ggplot() +\n   geom_point(aes(x = 1:5, y = seagrass_exposed), size = 3) +\n   geom_hline(yintercept = mean(seagrass_exposed), linetype = \"dashed\", color = \"red\") +\n   labs(title = \"Site B: Wave-exposed Coast\", x = \"Measurement\", y = \"Length (cm)\") +\n   ylim(10, 20)\n\n# Combine plots side by side\np1 + p2"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#why-do-we-need-measures-of-spread",
    "href": "lectures/L02/lecture-02.html#why-do-we-need-measures-of-spread",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Why do we need measures of spread?",
    "text": "Why do we need measures of spread?\n\nCentral tendency (mean, median, mode) only tells part of the story\nSpread tells us how much variation exists in our data\nDifferent measures of spread tell us different things:\n\nRange: Overall spread of data\nIQR: Spread of middle 50% of data\nVariance: Average squared deviation from mean\nStandard deviation: Average deviation in original units"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#range-the-simplest-measure-of-spread",
    "href": "lectures/L02/lecture-02.html#range-the-simplest-measure-of-spread",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Range – The simplest measure of spread",
    "text": "Range – The simplest measure of spread\n\n# Create our seagrass data\nseagrass_protected &lt;- c(15.2, 15.0, 15.3, 15.1, 15.2)  # Protected bay\nseagrass_exposed &lt;- c(12.0, 18.0, 14.5, 16.5, 15.0)    # Wave-exposed coast\n\n# Calculate ranges\ncat(\"Protected bay range:\", diff(range(seagrass_protected)), \"cm\\n\")\n\nProtected bay range: 0.3 cm\n\ncat(\"Wave-exposed range:\", diff(range(seagrass_exposed)), \"cm\\n\")\n\nWave-exposed range: 6 cm\n\n\n\n\n\n\n\n\nNote\n\n\nThe range shows us that seagrass lengths are much more variable in the wave-exposed site!"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#interquartile-range-iqr-the-middle-50",
    "href": "lectures/L02/lecture-02.html#interquartile-range-iqr-the-middle-50",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Interquartile range (IQR): The middle 50%",
    "text": "Interquartile range (IQR): The middle 50%\nThe IQR tells us how spread out the middle 50% of our data is:\n\n# Get quartiles for protected bay\nquantile(seagrass_protected)\n\n  0%  25%  50%  75% 100% \n15.0 15.1 15.2 15.2 15.3 \n\n\n\n25% of data below Q1 (1st quartile)\n75% of data below Q3 (3rd quartile)\nIQR = Q3 - Q1\n\nWhy use IQR?\n\nIgnores extreme values\nWorks with skewed data\nMore stable than range"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#comparing-sites-using-iqr",
    "href": "lectures/L02/lecture-02.html#comparing-sites-using-iqr",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Comparing Sites Using IQR",
    "text": "Comparing Sites Using IQR\n\n# Compare IQRs\npbay &lt;- IQR(seagrass_protected)\npbay\n\n[1] 0.1\n\nexbay &lt;- IQR(seagrass_exposed)\nexbay\n\n[1] 2\n\n\n\nProtected bay IQR: 0.1 cm\nWave-exposed IQR: 2 cm\n\n\n\n\n\n\n\nNote\n\n\nThe larger IQR in the wave-exposed site shows more spread in the typical seagrass lengths"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#variance-a-detailed-measure-of-spread",
    "href": "lectures/L02/lecture-02.html#variance-a-detailed-measure-of-spread",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Variance: a detailed measure of spread",
    "text": "Variance: a detailed measure of spread\nVariance measures how far data points are spread from their mean by:\n\nFinding how far each point is from the mean\nSquaring these distances (to handle negative values)\nTaking the average of these squared distances\n\nWhy use variance?\n\nUses all data points (unlike IQR)\nLess sensitive to outliers than range\nShows total spread in both directions\n\n\nKey points\n\nMeasured in squared units (cm²)\nLarger variance = more spread"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#calculating-variance-in-r",
    "href": "lectures/L02/lecture-02.html#calculating-variance-in-r",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Calculating Variance in R",
    "text": "Calculating Variance in R\n\n\nCode\n# Calculate variance for both sites\ncat(\"Protected bay variance:\", var(seagrass_protected), \"cm²\\n\")\n\n\nProtected bay variance: 0.013 cm²\n\n\nCode\ncat(\"Wave-exposed variance:\", var(seagrass_exposed), \"cm²\\n\")\n\n\nWave-exposed variance: 5.075 cm²\n\n\n\n\n\n\n\n\nNote\n\n\nThe larger variance in wave-exposed site shows more spread from the mean!"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#standard-deviation-a-more-interpretable-measure",
    "href": "lectures/L02/lecture-02.html#standard-deviation-a-more-interpretable-measure",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Standard deviation: a more interpretable measure",
    "text": "Standard deviation: a more interpretable measure\nStandard deviation (SD, or \\(\\sigma\\) for population, \\(s\\) for sample) is the square root of variance:\n\nTells us the “typical distance” from the mean\nEasy to understand - similar to saying “± value” after a mean\nSmall SD means values cluster closely around mean\nLarge SD means values are more spread out\n\nWhen and why to use it\n\nValues are in the same units as your data (unlike variance)\nPerfect for describing natural variation (height, weight, temperature)\nUsed in many statistical tests\nGreat for comparing different groups or datasets"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#interpreting-standard-deviation-with-r",
    "href": "lectures/L02/lecture-02.html#interpreting-standard-deviation-with-r",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Interpreting standard deviation (with R)",
    "text": "Interpreting standard deviation (with R)\nWe can describe our seagrass lengths using mean ± standard deviation:\n\n# Protected bay\nmean_p &lt;- mean(seagrass_protected)\nsd_p &lt;- sd(seagrass_protected)\ncat(\"Protected bay:\", round(mean_p, 1), \"±\", round(sd_p, 2), \"cm\\n\")\n\nProtected bay: 15.2 ± 0.11 cm\n\n# Wave-exposed\nmean_e &lt;- mean(seagrass_exposed)\nsd_e &lt;- sd(seagrass_exposed)\ncat(\"Wave-exposed:\", round(mean_e, 1), \"±\", round(sd_e, 2), \"cm\\n\")\n\nWave-exposed: 15.2 ± 2.25 cm\n\n\n\n\n\n\n\n\nTip\n\n\nThe ± tells us about the typical variation around the mean. Larger values indicate more spread!"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#comparing-spread-measures",
    "href": "lectures/L02/lecture-02.html#comparing-spread-measures",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Comparing spread measures",
    "text": "Comparing spread measures\n\n\n\n\n\n\n\n\n\nMeasure\nProtected Bay\nWave-exposed Coast\nWhat it Tells Us\n\n\n\n\nRange\n0.3 cm\n6 cm\nOverall spread (sensitive to outliers)\n\n\nIQR\n0.1 cm\n2 cm\nMiddle 50% spread (ignores extremes)\n\n\nVariance\n0.01 cm²\n5.07 cm²\nAverage squared distance from mean\n\n\nSD\n0.11 cm\n2.25 cm\nAverage distance from mean (in original units)\n\n\n\nKey Observations\n\nWave-exposed site shows consistently more variation\nEach measure gives a different perspective\nChoose based on your data and goals\nStandard deviation is most commonly used in research papers"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#range-and-iqr-in-excel",
    "href": "lectures/L02/lecture-02.html#range-and-iqr-in-excel",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Range and IQR in Excel",
    "text": "Range and IQR in Excel\nCommon Excel functions for measuring spread:\n\nRange: Use MAX() and MIN()\n\n=MAX(A1:A10) - MIN(A1:A10)\n\nQuartiles and IQR: Use QUARTILE.INC()\n\nFor Q1: =QUARTILE.INC(A1:A10, 1)\nFor Q3: =QUARTILE.INC(A1:A10, 3)\nFor IQR: =QUARTILE.INC(A1:A10, 3) - QUARTILE.INC(A1:A10, 1)"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#variance-and-standard-deviation-in-excel",
    "href": "lectures/L02/lecture-02.html#variance-and-standard-deviation-in-excel",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Variance and standard deviation in Excel",
    "text": "Variance and standard deviation in Excel\nStatistical functions for variance and standard deviation:\n\nSample Variance: Use VAR.S()\n\n=VAR.S(A1:A10)\n\nSample Standard Deviation: Use STDEV.S()\n\n=STDEV.S(A1:A10)\n\n\n\n\n\n\nTip\n\n\nUse .P instead of .S for population measures:\n\nVAR.P() for population variance\nSTDEV.P() for population standard deviation"
  },
  {
    "objectID": "lectures/L02/lecture-02.html#core-reading",
    "href": "lectures/L02/lecture-02.html#core-reading",
    "title": "Lecture 02: Introduction to statistical programming",
    "section": "Core Reading",
    "text": "Core Reading\n\nQuinn & Keough (2024). Experimental Design and Data Analysis for Biologists. Cambridge University Press. Chapter 2: Things to know before proceeding.\nCanvas site for lecture notes and additional resources"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#after-this-week-you-will-be-able-to",
    "href": "lectures/L03/lecture-03.html#after-this-week-you-will-be-able-to",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "After this week, you will be able to:",
    "text": "After this week, you will be able to:\n\nUnderstand the importance of data exploration before analysis\nApply exploratory functions to explore and summarise datasets\nIdentify different data types and structures in datasets\nSelect appropriate visualization types based on data characteristics\nUnderstand the Grammar of Graphics approach to data visualisation\nCreate and customise visualisations in R using the ggplot2 package\nBuild plots layer-by-layer using the ggplot2 framework\nInterpret distributions, including skewness, kurtosis, and outliers"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#quick-checklist",
    "href": "lectures/L03/lecture-03.html#quick-checklist",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Quick checklist",
    "text": "Quick checklist\nBy now you should have…\n\nInstalled R and RStudio\nCompleted Lecture 2 content and read the ENVX1002 R guide\nA basic understanding of measures of central tendency and spread\nA basic understanding of what a function(argument = value) is in R\nRendered a few Quarto documents in RStudio"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#data-exploration",
    "href": "lectures/L03/lecture-03.html#data-exploration",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Data exploration",
    "text": "Data exploration\nWhy explore data before analysis?\n\nIdentify patterns, outliers, and relationships\nDetect data quality issues\nGuide selection of appropriate statistical methods\nAvoid incorrect conclusions from flawed data\n\nThe data exploration workflow\n\nUnderstand data structure and types\nExamine distributions and summary statistics\nVisualise relationships between variables\nIdentify patterns and anomalies"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#types-of-data-recap-from-week-1",
    "href": "lectures/L03/lecture-03.html#types-of-data-recap-from-week-1",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Types of data: recap from Week 1",
    "text": "Types of data: recap from Week 1\nData in R can be broadly categorized as either categorical or continuous.\n\nDifferent data types require different analysis approaches\nUnderstanding data types helps select appropriate visualisations\nR stores different data types in specific formats (which is why we need to know what they are when we import data!)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#categorical-data",
    "href": "lectures/L03/lecture-03.html#categorical-data",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Categorical data",
    "text": "Categorical data\n\nNominal: no natural order, e.g.\n\nSpecies (dog, cat, fish)\nHair colour (black, brown, blonde)\nBlood type (A, B, AB, O)\n\nOrdinal: natural order exists, e.g.\n\nEducation (primary, secondary, tertiary)\nPain scale (mild, moderate, severe)\nT-shirt sizes (S, M, L, XL)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#continuous-data",
    "href": "lectures/L03/lecture-03.html#continuous-data",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Continuous data",
    "text": "Continuous data\n\nInterval: equal intervals, no true zero, e.g.\n\nTemperature in °C (0°C isn’t “no temperature”)\nCalendar dates\npH scale\n\nRatio: equal intervals with true zero, e.g.\n\nHeight (0 cm = no height)\nWeight (0 kg = no weight)\nAge (0 years = birth)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#normal-distribution-introduction",
    "href": "lectures/L03/lecture-03.html#normal-distribution-introduction",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Normal distribution: Introduction",
    "text": "Normal distribution: Introduction\n\nBell-shaped, symmetric curve\nDefined by mean (μ) and standard deviation (\\(\\sigma\\))\nMany natural phenomena follow this distribution\n\nHeights of individuals in a population\nMeasurement errors\nMany physiological traits\n\n\n\\[X \\sim N(\\mu, \\sigma^2)\\]\n\nThe random variable X follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#what-does-a-normal-distribution-look-like",
    "href": "lectures/L03/lecture-03.html#what-does-a-normal-distribution-look-like",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "What does a normal distribution look like?",
    "text": "What does a normal distribution look like?\n\n\nCode\n# Generate normal distribution data\nset.seed(123)\nnormal_data &lt;- rnorm(1000, mean = 0, sd = 1)\n\n# Plot normal distribution\nggplot(data.frame(x = normal_data), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 30,\n                 fill = \"skyblue\",\n                 colour = \"black\") +\n  geom_density(colour = \"red\") +\n  labs(title = \"Standard Normal Distribution (μ = 0, σ = 1)\",\n       x = \"Value\",\n       y = \"Density\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#properties-of-normal-distribution-the-empirical-rule",
    "href": "lectures/L03/lecture-03.html#properties-of-normal-distribution-the-empirical-rule",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Properties of normal distribution: The empirical rule",
    "text": "Properties of normal distribution: The empirical rule\n\nMean = median = mode\n~68% of data within 1\\(\\sigma\\) of mean\n~95% of data within 2\\(\\sigma\\) of mean\n~99.7% of data within 3\\(\\sigma\\) of mean"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#the-empirical-rule-visualised",
    "href": "lectures/L03/lecture-03.html#the-empirical-rule-visualised",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "The empirical rule visualised",
    "text": "The empirical rule visualised\n\n\nCode\n# Create a standard normal distribution\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x)\ndf &lt;- data.frame(x = x, y = y)\n\n# Plot with empirical rule highlighted\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  # Add vertical reference lines at standard deviations\n  geom_vline(xintercept = c(-3, -2, -1, 0, 1, 2, 3), linetype = \"dashed\", colour = \"gray50\", alpha = 0.7) +\n  geom_area(data = subset(df, x &gt;= -1 & x &lt;= 1), fill = \"darkblue\", alpha = 0.3) +\n  geom_area(data = subset(df, (x &gt;= -2 & x &lt; -1) | (x &gt; 1 & x &lt;= 2)), fill = \"darkgreen\", alpha = 0.3) +\n  geom_area(data = subset(df, (x &gt;= -3 & x &lt; -2) | (x &gt; 2 & x &lt;= 3)), fill = \"darkred\", alpha = 0.3) +\n  annotate(\"text\", x = 0, y = 0.2, label = \"68%\", colour = \"darkblue\") +\n  annotate(\"text\", x = 1.5, y = 0.1, label = \"95%\", colour = \"darkgreen\") +\n  annotate(\"text\", x = 2.5, y = 0.05, label = \"99.7%\", colour = \"darkred\") +\n  labs(title = \"Normal Distribution: Empirical Rule\",\n       x = \"Standard Deviations from Mean\",\n       y = \"Density\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#why-normal-distributions-matter-in-data-exploration",
    "href": "lectures/L03/lecture-03.html#why-normal-distributions-matter-in-data-exploration",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Why normal distributions matter in data exploration",
    "text": "Why normal distributions matter in data exploration\nWhen exploring data, understanding distributions helps you:\n\nIdentify patterns and anomalies\n\nIs your data normally distributed as expected?\nAre there unexpected skews or outliers?\n\nChoose appropriate analysis methods\n\nMany statistical tests assume normality\nNon-normal data may require different approaches\n\nInterpret results correctly\n\nContext for understanding how unusual a value is\nFramework for making statistical inferences"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#example",
    "href": "lectures/L03/lecture-03.html#example",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Example",
    "text": "Example\nMany biological traits follow normal distributions. For example, plant heights within a species:\n\n\nCode\n# Simulate plant height data\nset.seed(456)\nplant_heights &lt;- rnorm(200, mean = 25, sd = 3)  # Heights in cm\n\n# Plot the distribution\nggplot(data.frame(height = plant_heights), aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 20,\n                 fill = \"#66c2a5\",\n                 colour = \"black\") +\n  geom_density(colour = \"#1f78b4\", linewidth = 1) +\n  geom_vline(xintercept = 25, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\", x = 25.5, y = 0.05, label = \"μ = 25 cm\", colour = \"red\") +\n  labs(title = \"Distribution of Plant Heights in a Population\",\n       subtitle = \"Example of a biological trait following normal distribution\",\n       x = \"Height (cm)\",\n       y = \"Density\")\n\n\n\nThis example shows how plant heights cluster around the mean (25 cm) following a normal distribution pattern. This helps researchers identify outliers, establish experimental categories, and detect environmental effects on growth patterns."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#skewness",
    "href": "lectures/L03/lecture-03.html#skewness",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Skewness",
    "text": "Skewness\nWhat is skewness?\n\nMeasure of asymmetry in a distribution\nIndicates which side of the distribution has a longer tail\nImportant for selecting appropriate statistical tests"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#positive-skew-right-skewed",
    "href": "lectures/L03/lecture-03.html#positive-skew-right-skewed",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Positive skew (right-skewed)",
    "text": "Positive skew (right-skewed)\n\nLong tail on right side\nMean &gt; median\n\n\n\nCode\n# Generate positive skewed distribution\nset.seed(123)\nright_skewed &lt;- exp(rnorm(1000, 0, 0.5))\n\n# Calculate statistics\nmean_val &lt;- mean(right_skewed)\nmedian_val &lt;- median(right_skewed)\n\n# Plot positive skewed distribution\nggplot(data.frame(x = right_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 30,\n                 fill = \"#66c2a5\", # colourblind-friendly green\n                 colour = \"black\") +\n  geom_density(colour = \"#1f78b4\", linewidth = 1) +\n  # Add vertical lines for mean and median\n  geom_vline(xintercept = mean_val, colour = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_val, colour = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  # Add annotations\n  annotate(\"text\", x = mean_val + 0.2, y = 0.5, label = paste(\"Mean =\", round(mean_val, 2)), colour = \"red\") +\n  annotate(\"text\", x = median_val - 0.2, y = 0.4, label = paste(\"Median =\", round(median_val, 2)), colour = \"blue\") +\n  labs(title = \"Positive skew (right-skewed)\",\n       subtitle = \"Note that Mean &gt; Median\",\n       x = \"Value\",\n       y = \"Density\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#negative-skew-left-skewed",
    "href": "lectures/L03/lecture-03.html#negative-skew-left-skewed",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Negative skew (left-skewed)",
    "text": "Negative skew (left-skewed)\n\nLong tail on left side\nMean &lt; median\n\n\n\nCode\n# Generate skewed distributions\nset.seed(123)\nleft_skewed &lt;- max(right_skewed) - right_skewed + min(right_skewed)\n\n# Calculate statistics\nmean_val &lt;- mean(left_skewed)\nmedian_val &lt;- median(left_skewed)\n\n# Plot negative skewed distribution\nggplot(data.frame(x = left_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 30,\n                 fill = \"#8da0cb\", # colourblind-friendly blue\n                 colour = \"black\") +\n  geom_density(colour = \"#1f78b4\", linewidth = 1) +\n  # Add vertical lines for mean and median\n  geom_vline(xintercept = mean_val, colour = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_val, colour = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  # Add annotations\n  annotate(\"text\", x = mean_val - 0.2, y = 0.5, label = paste(\"Mean =\", round(mean_val, 2)), colour = \"red\") +\n  annotate(\"text\", x = median_val + 0.2, y = 0.4, label = paste(\"Median =\", round(median_val, 2)), colour = \"blue\") +\n  labs(title = \"Negative skew (left-skewed)\",\n       subtitle = \"Note that Mean &lt; Median\",\n       x = \"Value\",\n       y = \"Density\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#kurtosis",
    "href": "lectures/L03/lecture-03.html#kurtosis",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Kurtosis",
    "text": "Kurtosis\n\nMeasure of “tailedness” of a distribution\nDescribes the shape of a distribution’s tails relative to its overall shape\nAffects the choice of statistical methods\n\nTypes of kurtosis\n\nMesokurtic: Normal distribution (kurtosis = 3)\nLeptokurtic: Sharper peak, heavier tails (kurtosis &gt; 3)\nPlatykurtic: Flatter peak, thinner tails (kurtosis &lt; 3)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#visualising-kurtosis",
    "href": "lectures/L03/lecture-03.html#visualising-kurtosis",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Visualising kurtosis",
    "text": "Visualising kurtosis\n\n\nCode\n# Generate distributions with different kurtosis\nset.seed(123)\nnormal &lt;- rnorm(1000, 0, 1)  # Mesokurtic\nleptokurtic &lt;- rt(1000, df = 5)  # t-distribution with 5 df is leptokurtic\nplatykurtic &lt;- runif(1000, -3, 3)  # Uniform distribution is platykurtic\n\n# Calculate kurtosis values (using e1071 package)\nlibrary(e1071)\nk_normal &lt;- kurtosis(normal)\nk_lepto &lt;- kurtosis(leptokurtic)\nk_platy &lt;- kurtosis(platykurtic)\n\n# Plot distributions\np1 &lt;- ggplot(data.frame(x = normal), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"#a6cee3\", # colourblind-friendly blue\n                 colour = \"black\") +\n  geom_density(colour = \"#1f78b4\", linewidth = 1) + # Darker blue\n  annotate(\"text\", x = 2, y = 0.3,\n           label = paste(\"Kurtosis =\", round(k_normal, 2)),\n           colour = \"#1f78b4\") +\n  labs(title = \"Mesokurtic (normal)\",\n       subtitle = \"Normal distribution with balanced tails\",\n       x = \"Value\",\n       y = \"Density\")\n\np2 &lt;- ggplot(data.frame(x = leptokurtic), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"#fb9a99\", # colourblind-friendly pink\n                 colour = \"black\") +\n  geom_density(colour = \"#e31a1c\", linewidth = 1) + # Darker red\n  annotate(\"text\", x = 2, y = 0.3,\n           label = paste(\"Kurtosis =\", round(k_lepto, 2)),\n           colour = \"#e31a1c\") +\n  labs(title = \"Leptokurtic (heavy-tailed)\",\n       subtitle = \"Sharper peak, heavier tails\",\n       x = \"Value\",\n       y = \"Density\")\n\np3 &lt;- ggplot(data.frame(x = platykurtic), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"#b2df8a\", # colourblind-friendly green\n                 colour = \"black\") +\n  geom_density(colour = \"#33a02c\", linewidth = 1) + # Darker green\n  annotate(\"text\", x = 0, y = 0.15,\n           label = paste(\"Kurtosis =\", round(k_platy, 2)),\n           colour = \"#33a02c\") +\n  labs(title = \"Platykurtic (light-tailed)\",\n       subtitle = \"Flatter peak, thinner tails\",\n       x = \"Value\",\n       y = \"Density\")\n\n# Display plots vertically\nlibrary(patchwork)\np1 + p2 + p3"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#data-structures-in-r",
    "href": "lectures/L03/lecture-03.html#data-structures-in-r",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Data structures in R",
    "text": "Data structures in R\nData is stored in R in various structures, each with specific purposes:\nVectors: 1-dimensional collection of elements\n\n\nCode\n# Vector - 1-dimensional collection of elements\nheights &lt;- c(1.65, 1.70, 1.75, 1.80, 1.85)\nheights\n\n\n[1] 1.65 1.70 1.75 1.80 1.85\n\n\nData frames: 2-dimensional tables with rows and columns\n\n\nCode\n# Data frame - 2-dimensional table\ndf &lt;- data.frame(\n  species = c(\"A\", \"B\", \"C\", \"A\", \"B\"),\n  height = c(1.65, 1.70, 1.75, 1.80, 1.85),\n  weight = c(60, 65, 70, 75, 80)\n)\ndf\n\n\n  species height weight\n1       A   1.65     60\n2       B   1.70     65\n3       C   1.75     70\n4       A   1.80     75\n5       B   1.85     80\n\n\nOther data structures include lists, matrices, arrays, and factors, but these are less common at your level."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#common-functions",
    "href": "lectures/L03/lecture-03.html#common-functions",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Common functions",
    "text": "Common functions\nUse these essential functions to understand your data structure and summary statistics:\n\n# Core function 1: Structure overview\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ species: chr  \"A\" \"B\" \"C\" \"A\" ...\n $ height : num  1.65 1.7 1.75 1.8 1.85\n $ weight : num  60 65 70 75 80\n\n# Core function 2: Statistical summary\nsummary(df)\n\n   species              height         weight  \n Length:5           Min.   :1.65   Min.   :60  \n Class :character   1st Qu.:1.70   1st Qu.:65  \n Mode  :character   Median :1.75   Median :70  \n                    Mean   :1.75   Mean   :70  \n                    3rd Qu.:1.80   3rd Qu.:75  \n                    Max.   :1.85   Max.   :80  \n\n\nThe summary() function provides a quick overview of your data and can help identify skewness, outliers, and missing values…but it isn’t always enough."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#your-options-are-endless-almost",
    "href": "lectures/L03/lecture-03.html#your-options-are-endless-almost",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Your options are endless (almost)",
    "text": "Your options are endless (almost)\nThere are many specialised functions for exploring different aspects of your data:\n\n# Check for unique values in categorical variables\nunique(df$species)\n\n[1] \"A\" \"B\" \"C\"\n\n# Visualise missing data patterns\nlibrary(naniar)\nvis_miss(airquality)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#the-value-of-data-visualisation",
    "href": "lectures/L03/lecture-03.html#the-value-of-data-visualisation",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "The value of data visualisation",
    "text": "The value of data visualisation\nThe output of vis_mis() cleary demonstrates the advantage of a visual approach to data exploration.\n\nCompare the visualisation to looking at the raw data or a summary of the raw data\n\nairquality$Ozone\n\n  [1]  41  36  12  18  NA  28  23  19   8  NA   7  16  11  14  18  14  34   6\n [19]  30  11   1  11   4  32  NA  NA  NA  23  45 115  37  NA  NA  NA  NA  NA\n [37]  NA  29  NA  71  39  NA  NA  23  NA  NA  21  37  20  12  13  NA  NA  NA\n [55]  NA  NA  NA  NA  NA  NA  NA 135  49  32  NA  64  40  77  97  97  85  NA\n [73]  10  27  NA   7  48  35  61  79  63  16  NA  NA  80 108  20  52  82  50\n [91]  64  59  39   9  16  78  35  66 122  89 110  NA  NA  44  28  65  NA  22\n[109]  59  23  31  44  21   9  NA  45 168  73  NA  76 118  84  85  96  78  73\n[127]  91  47  32  20  23  21  24  44  21  28   9  13  46  18  13  24  16  13\n[145]  23  36   7  14  30  NA  14  18  20\n\nsummary(airquality$Ozone)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00   18.00   31.50   42.13   63.25  168.00      37"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#common-plot-types-and-their-applications",
    "href": "lectures/L03/lecture-03.html#common-plot-types-and-their-applications",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Common plot types and their applications",
    "text": "Common plot types and their applications\nDifferent types of data require different visualisation approaches. Let’s explore the most common plot types and when to use them."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#histograms",
    "href": "lectures/L03/lecture-03.html#histograms",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Histograms",
    "text": "Histograms\nPurpose and applications:\n\nVisualise distribution of continuous data\nIdentify central tendency, spread, outliers, and skewness\nExamine distributions of measurements in biological data\n\nWhen to use:\n\nFor continuous variables (interval or ratio data)\nWhen you want to understand the shape of a distribution\nExamples: heights, weights, temperatures, measurements"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#section",
    "href": "lectures/L03/lecture-03.html#section",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "",
    "text": "Code\n# Example using base R with palmerpenguins data\nhist(penguins$body_mass_g,\n     main = \"Distribution of Penguin Body Mass\",\n     xlab = \"Body Mass (g)\",\n     col = \"skyblue\",\n     border = \"white\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#bar-plots",
    "href": "lectures/L03/lecture-03.html#bar-plots",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Bar plots",
    "text": "Bar plots\nPurpose and applications:\n\nCompare values across categories\nShow proportions or counts in categorical data\nVisualise species abundance, treatment effects\n\nWhen to use:\n\nFor categorical variables (nominal or ordinal data)\nWhen comparing frequencies or counts across groups\nExamples: species counts, treatment groups, survey responses"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#section-1",
    "href": "lectures/L03/lecture-03.html#section-1",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "",
    "text": "Code\n# Example using base R with palmerpenguins data\nspecies_counts &lt;- table(penguins$species)\nbarplot(species_counts,\n        main = \"Count of Penguins by Species\",\n        xlab = \"Species\",\n        ylab = \"Count\",\n        col = c(\"darkorange\", \"purple\", \"cyan4\"),\n        border = \"white\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#scatterplots",
    "href": "lectures/L03/lecture-03.html#scatterplots",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Scatterplots",
    "text": "Scatterplots\nPurpose and applications:\n\nExamine relationships between continuous variables\nIdentify correlations, patterns, and outliers\nExplore relationships between measurements\n\nWhen to use:\n\nWhen examining relationships between two continuous variables\nWhen looking for correlations or patterns\nExamples: height vs. weight, temperature vs. growth rate"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#section-2",
    "href": "lectures/L03/lecture-03.html#section-2",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "",
    "text": "Code\n# Example using base R with palmerpenguins data\n# Remove NA values for this example\npenguins_clean &lt;- na.omit(penguins[, c(\"flipper_length_mm\", \"body_mass_g\")])\nplot(penguins_clean$flipper_length_mm, penguins_clean$body_mass_g,\n     main = \"Relationship Between Flipper Length and Body Mass\",\n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Body Mass (g)\",\n     pch = 19,\n     col = \"darkblue\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#boxplots",
    "href": "lectures/L03/lecture-03.html#boxplots",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Boxplots",
    "text": "Boxplots\nPurpose and applications:\n\nCompare distributions across groups\nVisualise median, quartiles, and outliers\nCompare measurements across treatments\n\nWhen to use:\n\nWhen comparing a continuous variable across categorical groups\nWhen you need to show the spread and central tendency\nExamples: comparing heights across species, measurements across treatments"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#section-3",
    "href": "lectures/L03/lecture-03.html#section-3",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "",
    "text": "Code\n# Example using base R with palmerpenguins data\nboxplot(body_mass_g ~ species, data = penguins,\n        main = \"Body Mass by Penguin Species\",\n        xlab = \"Species\",\n        ylab = \"Body Mass (g)\",\n        col = c(\"darkorange\", \"purple\", \"cyan4\"),\n        border = \"black\")"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#the-grammar-of-graphics",
    "href": "lectures/L03/lecture-03.html#the-grammar-of-graphics",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nggplot2 is based on the Grammar of Graphics, a systematic approach to creating visualisations by combining different components:\n\nData: The dataset you want to visualise\nAesthetics: Mapping variables to visual properties (position, colour, size, etc.)\nGeometries: The shapes used to represent the data (points, lines, bars, etc.)\nScales: How values are mapped to visual properties\nFacets: How to split the data into subplots\nCoordinates: The coordinate system to use\nThemes: Visual styling of the plot\n\nThis grammar allows you to build complex visualisations layer by layer."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#why-use-ggplot2",
    "href": "lectures/L03/lecture-03.html#why-use-ggplot2",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Why use ggplot2?",
    "text": "Why use ggplot2?\n\nConsistent syntax across different plot types\nLayered approach makes it easy to build complex visualisations\nExcellent defaults that produce publication-quality graphics\nHighly customisable with extensive options for fine-tuning\nLarge community with extensive documentation and examples"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-1---start-with-data",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-1---start-with-data",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 1 - Start with data",
    "text": "Building a plot: Step 1 - Start with data\nLet’s build a scatterplot of penguin flipper length vs. body mass using the palmerpenguins dataset.\nFirst, we need to load the ggplot2 package and prepare our data:\n\n\nCode\nlibrary(ggplot2)\n# Remove missing values for this example\npenguins_clean &lt;- na.omit(penguins)\n\n# Look at the first few rows of our data\nhead(penguins_clean[, c(\"species\", \"flipper_length_mm\", \"body_mass_g\")])\n\n\n# A tibble: 6 × 3\n  species flipper_length_mm body_mass_g\n  &lt;fct&gt;               &lt;int&gt;       &lt;int&gt;\n1 Adelie                181        3750\n2 Adelie                186        3800\n3 Adelie                195        3250\n4 Adelie                193        3450\n5 Adelie                190        3650\n6 Adelie                181        3625"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-2---create-a-blank-canvas",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-2---create-a-blank-canvas",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 2 - Create a blank canvas",
    "text": "Building a plot: Step 2 - Create a blank canvas\nThe ggplot() function initialises a plot with data:\n\n\nCode\n# Create a blank canvas with our data\np &lt;- ggplot(penguins_clean)\np\n\n\n\nThis creates an empty plot. We need to add layers to visualise our data."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-3---add-aesthetics",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-3---add-aesthetics",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 3 - Add aesthetics",
    "text": "Building a plot: Step 3 - Add aesthetics\nAesthetics map variables in the data to visual properties:\n\n\nCode\n# Add aesthetics mapping\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g))\np\n\n\n\nWe’ve defined which variables go on which axes, but we still need to specify how to represent the data."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-4---add-a-geometry",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-4---add-a-geometry",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 4 - Add a geometry",
    "text": "Building a plot: Step 4 - Add a geometry\nGeometries define how the data is represented visually:\n\n\nCode\n# Add points geometry\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\np\n\n\n\nNow we can see the relationship between flipper length and body mass!"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-5---add-colour-by-species",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-5---add-colour-by-species",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 5 - Add colour by species",
    "text": "Building a plot: Step 5 - Add colour by species\nWe can map the species variable to the colour aesthetic:\n\n\nCode\n# colour points by species\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  geom_point()\np\n\n\n\nNotice how ggplot2 automatically creates a legend for the species colours."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-6---customise-point-appearance",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-6---customise-point-appearance",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 6 - Customise point appearance",
    "text": "Building a plot: Step 6 - Customise point appearance\nWe can adjust the size and transparency of points:\n\n\nCode\n# Customize point appearance\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  geom_point(size = 3, alpha = 0.7)\np"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-7---add-labels-and-title",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-7---add-labels-and-title",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 7 - Add labels and title",
    "text": "Building a plot: Step 7 - Add labels and title\nLet’s add informative labels and a title:\n\n\nCode\n# Add labels and title\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(\n    title = \"Relationship Between Flipper Length and Body Mass\",\n    subtitle = \"Palmer Penguins Dataset\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    colour = \"Penguin Species\"\n  )\np"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-8---customise-colours",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-8---customise-colours",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 8 - Customise colours",
    "text": "Building a plot: Step 8 - Customise colours\nWe can use a custom colour palette:\n\n\nCode\n# Customize colours\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_colour_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n  labs(\n    title = \"Relationship Between Flipper Length and Body Mass\",\n    subtitle = \"Palmer Penguins Dataset\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    colour = \"Penguin Species\"\n  )\np"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#building-a-plot-step-9---apply-a-theme",
    "href": "lectures/L03/lecture-03.html#building-a-plot-step-9---apply-a-theme",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Building a plot: Step 9 - Apply a theme",
    "text": "Building a plot: Step 9 - Apply a theme\nFinally, let’s apply a theme to change the overall appearance:\n\n\nCode\n# Apply a theme\np &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_colour_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n  labs(\n    title = \"Relationship Between Flipper Length and Body Mass\",\n    subtitle = \"Palmer Penguins Dataset\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    colour = \"Penguin Species\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"italic\")\n  )\np"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#adding-more-layers-trend-lines",
    "href": "lectures/L03/lecture-03.html#adding-more-layers-trend-lines",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Adding more layers: Trend lines",
    "text": "Adding more layers: Trend lines\nOne of the strengths of ggplot2 is the ability to add multiple layers:\n\n\nCode\n# Add trend lines for each species\np + geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#faceting-split-by-species",
    "href": "lectures/L03/lecture-03.html#faceting-split-by-species",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Faceting: Split by species",
    "text": "Faceting: Split by species\nWe can also split the plot into facets by species:\n\n\nCode\n# Create facets by species\np + facet_wrap(~ species)"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#the-complete-ggplot2-code",
    "href": "lectures/L03/lecture-03.html#the-complete-ggplot2-code",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "The complete ggplot2 code",
    "text": "The complete ggplot2 code\nHere’s the complete code for our final plot:\n\n\nCode\nggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, colour = species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_colour_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n  labs(\n    title = \"Relationship Between Flipper Length and Body Mass\",\n    subtitle = \"Palmer Penguins Dataset\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    colour = \"Penguin Species\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"italic\")\n  )"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#resources-for-further-learning",
    "href": "lectures/L03/lecture-03.html#resources-for-further-learning",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Resources for further learning",
    "text": "Resources for further learning\n\nR Graphics Cookbook\nggplot2 documentation\nR for Data Science"
  },
  {
    "objectID": "lectures/L03/lecture-03.html#core-reading",
    "href": "lectures/L03/lecture-03.html#core-reading",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Core reading",
    "text": "Core reading\n\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer.\nChang, W. (2018). R Graphics Cookbook. O’Reilly Media.\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media."
  },
  {
    "objectID": "lectures/L03/lecture-03.html#online-resources",
    "href": "lectures/L03/lecture-03.html#online-resources",
    "title": "Lecture 03: Exploring and visualising data",
    "section": "Online resources",
    "text": "Online resources\n\nggplot2 documentation\nR for Data Science - Data Visualization chapter\nThe R Graph Gallery\nCookbook for R - Graphs"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#learning-outcomes",
    "href": "lectures/L04/lecture-04.html#learning-outcomes",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAt the end of this week, you will be able to:\n\nDefine probability distributions and their key properties\nApply the normal distribution to calculate probabilities\nStandardise variables using the normal distribution\nDifferentiate between population, sample and sampling distributions\nExplain the difference between standard deviation and standard error\nApply the central limit theorem and understand its significance\nUse R to implement these statistical concepts"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#quick-checklist",
    "href": "lectures/L04/lecture-04.html#quick-checklist",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Quick checklist",
    "text": "Quick checklist\n\nCreate and edit Quarto documents\nUnderstand the concept of central tendency and spread/dispersion\nInterpret skewness (and kurtosis) in a distribution of data\nKnow that different data types require different approaches to summarising data\nGenerate some plots in ggplot2"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#example-rainfall",
    "href": "lectures/L04/lecture-04.html#example-rainfall",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Example: rainfall",
    "text": "Example: rainfall\nConsider measuring daily rainfall amounts in a coastal wetland. Some days have no rain. Most days have small to moderate amounts. Few days have very heavy rainfall.\n\nThis creates a pattern of variation in rainfall amounts, which at some point we can predict. This pattern is what we call a probability distribution (i.e. we can calculate the probability of a certain amount of rainfall on a given day).\n\n\nCode\nset.seed(123)\nrainfall_data &lt;- data.frame(\n  rainfall = rgamma(1000, shape = 2, scale = 5)\n)\n\np_rainfall &lt;- ggplot(rainfall_data, aes(x = rainfall)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", colour = \"white\") +\n  labs(\n    x = \"Daily Rainfall (mm)\",\n    y = \"Frequency\"\n  )\np_rainfall"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#probability-distribution",
    "href": "lectures/L04/lecture-04.html#probability-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Probability distribution?",
    "text": "Probability distribution?\nA probability distribution is a function or equation that tells us:\n\nHow likely different values are to occur\nThe pattern of variation in our measurements\nThe range and spread of possible values\n\nIt is like a mathematical recipe for how values are distributed.\n\nDifferent types of distributions\nDepending on the data, distributions can be:\n\nDiscrete (e.g. number of trees in 100m2 of forest) – probability distribution function \\(P = f(x)\\)\n\nWhat is the probability that 100m2 of forest contains 50 trees?\n\nContinuous (e.g. rainfall amounts over a period) – probability density function \\(f(x)\\)\n\nWhat is the probability of rainfall between 10-20mm a day?\n\nBoth discrete and continuous (e.g. age of trees) \\(F(x) = P(X \\leq x)\\)\n\nWhat is the probability that a tree is older than 50 years?"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#properties-of-distributions",
    "href": "lectures/L04/lecture-04.html#properties-of-distributions",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Properties of distributions",
    "text": "Properties of distributions\nWe now combine all of what we have learnt about data in the last 3 weeks:\n\nCentral tendency – mean, median, mode\nSpread – standard deviation, variance\nShape – symmetry, skewness\nTails – how quickly they “decay” towards zero\n\nBut how do we use these properties to make predictions? Do some distributions have special properties?\n\n\nCode\n# Annotate our rainfall distribution with key properties\np_rainfall +\n  geom_vline(\n    xintercept = mean(rainfall_data$rainfall),\n    colour = \"red\", linetype = \"dashed\"\n  ) +\n  annotate(\"text\",\n    x = mean(rainfall_data$rainfall), y = 100,\n    label = \"Mean\", angle = 90, vjust = -0.5\n  ) +\n  labs(\n    x = \"Daily Rainfall (mm)\",\n    y = \"Frequency\",\n    title = \"Can we use the mean to predict rainfall?\"\n  )"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#tree-heights",
    "href": "lectures/L04/lecture-04.html#tree-heights",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Tree heights",
    "text": "Tree heights\nTree heights in a forest often follow a normal distribution:\n\nMost trees cluster around the average height\nFewer trees are very short or very tall\nThe pattern is symmetrical – a bell-shaped curve with two parameters (mean \\(\\mu\\) and standard deviation \\(\\sigma\\)) such that \\[X \\sim N(\\mu, \\sigma^2)\\]"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#comparing-distributions",
    "href": "lectures/L04/lecture-04.html#comparing-distributions",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Comparing distributions",
    "text": "Comparing distributions\n\n\nCode\n# Set up the data\nset.seed(123)\ntree_heights &lt;- data.frame(\n  height = rnorm(1000, mean = 20, sd = 3)\n)\n\n# Create plots for comparison\np1 &lt;- ggplot(rainfall_data, aes(x = rainfall)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 2,\n    fill = \"steelblue\",\n    colour = \"white\"\n  ) +\n  labs(\n    x = \"Daily Rainfall (mm)\",\n    y = \"Density\",\n    title = \"Skewed Distribution\\n(Rainfall)\"\n  )\n\np2 &lt;- ggplot(tree_heights, aes(x = height)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 0.5,\n    fill = \"forestgreen\",\n    colour = \"white\"\n  ) +\n  labs(\n    x = \"Tree Height (m)\",\n    y = \"Density\",\n    title = \"Normal Distribution\\n(Tree Heights)\"\n  )\n\n# Arrange plots side by side\nplot_grid(p1, p2, ncol = 2)"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#why-do-tree-heights-follow-a-normal-distribution",
    "href": "lectures/L04/lecture-04.html#why-do-tree-heights-follow-a-normal-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Why do tree heights follow a normal distribution?",
    "text": "Why do tree heights follow a normal distribution?\n\nThe middle (mean) height is most common – most trees are around 20 metres tall\nHeights spread out evenly on both sides – as many short trees as tall trees, like a mirror image around the middle\nExtreme heights are rare – very few extremely short or tall trees – numbers decrease gradually as we move away from the middle\n\nThe normal distribution is a natural phenomenon in many real-world situations – a fundamental concept in statistics.\n\n\nCode\np2"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#a-continuous-distribution",
    "href": "lectures/L04/lecture-04.html#a-continuous-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "A continuous distribution",
    "text": "A continuous distribution\n\nThere is an infinite number of possible heights within a certain range (e.g. 0-40m)\nWe can calculate the probability of a tree being a certain height or within a range\nSome heights have a probability of zero (e.g. negative heights, or heights above 40m)\n\nWe can use these properties to make reasonable predictions about tree heights:\n\n\nCode\n# Calculate mean and standard deviations\nmean_height &lt;- mean(tree_heights$height)\nsd_height &lt;- sd(tree_heights$height)\n\n# Create plot with standard deviation ranges\np2 +\n  # Add vertical lines for mean and SDs\n  geom_vline(\n    xintercept = mean_height,\n    colour = \"darkred\",\n    size = 1,\n    linetype = \"dashed\"\n  ) +\n  # Add SD ranges\n  annotate(\"rect\",\n    xmin = mean_height - sd_height,\n    xmax = mean_height + sd_height,\n    ymin = 0, ymax = Inf,\n    fill = \"yellow\", alpha = 0.2\n  ) +\n  annotate(\"rect\",\n    xmin = mean_height - 2 * sd_height,\n    xmax = mean_height + 2 * sd_height,\n    ymin = 0, ymax = Inf,\n    fill = \"orange\", alpha = 0.1\n  ) +\n  # Add labels\n  annotate(\"text\",\n    x = mean_height, y = 0.15,\n    label = \"Mean\", angle = 90, vjust = -0.5\n  ) +\n  annotate(\"text\",\n    x = mean_height - sd_height/2, y = 0.12,\n    label = \"68% within\\n1 SD\", angle = 90, vjust = -0.5\n  ) +\n  annotate(\"text\",\n    x = mean_height - sd_height, y = 0.09,\n    label = \"95% within\\n2 SD\", angle = 90, vjust = -0.5\n  ) +\n  labs(\n    x = \"Tree Height (m)\",\n    y = \"Density\",\n    title = \"Normal Distribution\\n(Tree Heights)\"\n  )"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#the-68-95-99.7-rule",
    "href": "lectures/L04/lecture-04.html#the-68-95-99.7-rule",
    "title": "Lecture 04 – The central limit theorem",
    "section": "The 68-95-99.7 rule",
    "text": "The 68-95-99.7 rule\nA data that follows a normal distribution:\n\nAbout 68% of values fall within 1 standard deviation\nAbout 95% fall within 2 standard deviations\nAbout 99.7% fall within 3 standard deviations\nThe remaining 0.3% are extreme values and extend to infinity\n\nThis pattern is the same for ALL normal distributions – a universal rule.\nImportantly, it is a predictable pattern (although not exact)."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#from-data-to-normal-distribution",
    "href": "lectures/L04/lecture-04.html#from-data-to-normal-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "From data to normal distribution",
    "text": "From data to normal distribution\n\nWhen data follows a reasonably normal pattern, we can “fit” a normal distribution to make equally reasonable predictions.\nIt is possible to fit a normal distribution to a skewed distribution (but not recommended – other techniques exist)\n\n\n\nCode\n# Calculate statistics for both distributions\ntree_mean &lt;- mean(tree_heights$height)\ntree_sd &lt;- sd(tree_heights$height)\nrain_mean &lt;- mean(rainfall_data$rainfall)\nrain_sd &lt;- sd(rainfall_data$rainfall)\n\n# Create comparison plots showing fitted normal curves\np1 &lt;- ggplot(tree_heights, aes(x = height)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 0.5,\n    fill = \"forestgreen\",\n    colour = \"white\",\n    alpha = 0.7\n  ) +\n  # Add fitted normal curve\n  stat_function(\n    fun = dnorm,\n    args = list(mean = tree_mean, sd = tree_sd),\n    colour = \"darkred\",\n    size = 1\n  ) +\n  # Add mean line and 1 SD range\n  geom_vline(xintercept = tree_mean, colour = \"blue\", linetype = \"dashed\") +\n  annotate(\"rect\",\n    xmin = tree_mean - tree_sd,\n    xmax = tree_mean + tree_sd,\n    ymin = 0, ymax = Inf,\n    fill = \"yellow\", alpha = 0.2\n  ) +\n  labs(\n    x = \"Tree Height (m)\",\n    y = \"Density\",\n    title = \"Normal Data:\\nCan Fit Normal Distribution\"\n  )\n\np2 &lt;- ggplot(rainfall_data, aes(x = rainfall)) +\n  geom_histogram(aes(y = ..density..),\n    binwidth = 2,\n    fill = \"steelblue\",\n    colour = \"white\",\n    alpha = 0.7\n  ) +\n  # Try to fit normal curve (shows poor fit)\n  stat_function(\n    fun = dnorm,\n    args = list(mean = rain_mean, sd = rain_sd),\n    colour = \"darkred\",\n    size = 1\n  ) +\n  geom_vline(xintercept = rain_mean, colour = \"blue\", linetype = \"dashed\") +\n  annotate(\"rect\",\n    xmin = rain_mean - rain_sd,\n    xmax = rain_mean + rain_sd,\n    ymin = 0, ymax = Inf,\n    fill = \"yellow\", alpha = 0.2\n  ) +\n  labs(\n    x = \"Rainfall (mm)\",\n    y = \"Density\",\n    title = \"Skewed Data:\\nPoor Normal Fit\"\n  )\n\nplot_grid(p1, p2, ncol = 2)"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#when-data-is-normal",
    "href": "lectures/L04/lecture-04.html#when-data-is-normal",
    "title": "Lecture 04 – The central limit theorem",
    "section": "When data is normal…",
    "text": "When data is normal…\n\nWe can use normal probability functions to:\n\nCalculate exact probabilities\nFind specific percentiles\nMake reliable predictions\n\nProperties are predictable (the 68-95-99.7 rule)\nCan standardise measurements (more on this later)\n\nCompare different variables\nUse z-scores\nApply statistical tests"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#working-with-normal-distributions-in-r",
    "href": "lectures/L04/lecture-04.html#working-with-normal-distributions-in-r",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Working with normal distributions in R",
    "text": "Working with normal distributions in R\nR provides three main functions that help us work with normal distributions. Think of them as different ways to ask questions about our data:\n1. pnorm(): “What’s the probability?”\n\nLike asking “What proportion of trees are shorter than X meters?”\n\nProbability of a tree being under 15m tall\nChance of rainfall being less than 50mm\nProportion of measurements below average"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#working-with-normal-distributions-in-r-1",
    "href": "lectures/L04/lecture-04.html#working-with-normal-distributions-in-r-1",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Working with normal distributions in R",
    "text": "Working with normal distributions in R\nR provides three main functions that help us work with normal distributions. Think of them as different ways to ask questions about our data:\n2. qnorm(): “What’s the value?”\n\nLike asking “How tall are the shortest 10% of trees?”\n\nFinding the height that only 5% of trees exceed\nDetermining rainfall threshold for “extreme” events\nIdentifying typical range boundaries"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#working-with-normal-distributions-in-r-2",
    "href": "lectures/L04/lecture-04.html#working-with-normal-distributions-in-r-2",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Working with normal distributions in R",
    "text": "Working with normal distributions in R\nR provides three main functions that help us work with normal distributions. Think of them as different ways to ask questions about our data:\n3. dnorm(): “What’s the relative likelihood?”\n\nLike asking “How common is this exact height?”\n\nComparing how likely different heights are\nFinding peak probability\nPlotting the normal curve shape"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#using-pnorm-for-probabilities",
    "href": "lectures/L04/lecture-04.html#using-pnorm-for-probabilities",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Using pnorm() for probabilities",
    "text": "Using pnorm() for probabilities\nWhat proportion of trees in our forest are shorter than 23m, knowing that mean height is 20m and the sd is 3m?\n\nprob_under_23 &lt;- pnorm(q = 23, mean = 20, sd = 3) # Calculate proportion of trees under 23m\nprob_under_23\n\n[1] 0.8413447\n\n\n\n\nCode\n# Visualise the probability\npercent_under_23 &lt;- prob_under_23 * 100\n\nggplot(data.frame(x = c(10, 30)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 20, sd = 3)) +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = 20, sd = 3),\n    fill = \"lightblue\", alpha = 0.5, xlim = c(10, 23)\n  ) +\n  geom_vline(xintercept = 23, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\",\n    x = 23, y = 0.02,\n    label = sprintf(\"%.1f%% of trees\", percent_under_23),\n    hjust = -0.1\n  ) +\n  labs(\n    x = \"Tree Height (m)\", y = \"Density\",\n    title = \"Tree Heights Below 23m\"\n  )"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#using-qnorm-for-thresholds",
    "href": "lectures/L04/lecture-04.html#using-qnorm-for-thresholds",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Using qnorm() for thresholds",
    "text": "Using qnorm() for thresholds\nHow tall are the largest 10% of trees in our forest?\n\n# Find the height threshold for tallest 10%\nheight_90th &lt;- qnorm(p = 0.90, mean = 20, sd = 3)\nheight_90th\n\n[1] 23.84465\n\n\n\n\nCode\n# Visualise the threshold\nggplot(data.frame(x = c(10, 30)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 20, sd = 3)) +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = 20, sd = 3),\n            fill = \"lightgreen\", alpha = 0.5, xlim = c(height_90th, 30)) +\n  geom_vline(xintercept = height_90th, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\", x = height_90th, y = 0.02,\n           label = sprintf(\"Trees above %.1f m\\nare in tallest 10%%\", height_90th),\n           hjust = 1.1) +\n  labs(x = \"Tree Height (m)\", y = \"Density\",\n       title = \"Identifying Tallest Trees\")"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#using-dnorm-for-relative-likelihood",
    "href": "lectures/L04/lecture-04.html#using-dnorm-for-relative-likelihood",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Using dnorm() for relative likelihood",
    "text": "Using dnorm() for relative likelihood\nWhich tree height is most common in our forest: 17m, 20m, or 23m? (Or, how likely are these heights?)\n\n# Find relative likelihood at different heights\nheights &lt;- c(17, 20, 23) # Example heights to compare\ndensities &lt;- dnorm(heights, mean = 20, sd = 3)\ndensities\n\n[1] 0.08065691 0.13298076 0.08065691\n\n\n\n\nCode\n# Plot relative likelihoods\nggplot(data.frame(x = c(10, 30)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 20, sd = 3)) +\n  geom_point(data = data.frame(x = heights, y = densities),\n             aes(y = y), colour = \"red\", size = 3) +\n  geom_segment(data = data.frame(x = heights),\n              aes(x = x, xend = x, y = 0, yend = dnorm(x, 20, 3)),\n              linetype = \"dashed\", colour = \"blue\") +\n  labs(x = \"Tree Height (m)\", y = \"Relative Likelihood\",\n       title = \"Height Probabilities in the Forest\")\n\n\n\nTrees closest to 20m are most common, with likelihood decreasing as we move away."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#practical-uses-of-normal-distribution-functions",
    "href": "lectures/L04/lecture-04.html#practical-uses-of-normal-distribution-functions",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Practical uses of normal distribution functions",
    "text": "Practical uses of normal distribution functions\nThese functions help us answer questions like:\n\nWhat proportion of trees are between 18m and 22m tall? ::: {.cell}\n\n# Probability between 18m and 22m - calculate the difference\nprob_between &lt;- pnorm(22, 20, 3) - pnorm(18, 20, 3)\nprob_between\n\n[1] 0.4950149\n\n:::\n\nHow tall are the top 5% of trees? ::: {.cell}\n\n# Height of tallest 5% of trees\ntall_threshold &lt;- qnorm(0.95, 20, 3)\ntall_threshold\n\n[1] 24.93456\n\n:::\n\nWhat are the height thresholds for “extreme” trees (top 1% and bottom 1%)?\n\n\n# Height thresholds for \"extreme\" trees\nextreme_thresholds &lt;- qnorm(c(0.01, 0.99), 20, 3)\nextreme_thresholds\n\n[1] 13.02096 26.97904"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#standardising-variables",
    "href": "lectures/L04/lecture-04.html#standardising-variables",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Standardising variables",
    "text": "Standardising variables\nWe can convert any normal distribution to a standard form where:\n\\[ X \\sim N(\\mu, \\sigma^2) \\rightarrow Z \\sim N(0, 1) \\]\nWhy standardise?\nStandardising helps us:\n\nCompare measurements on different scales\nIdentify unusual values (e.g., z &gt; 2 is unusual)\nCalculate probabilities using standard normal tables\n\nFormula: \\[z = \\frac{x - \\mu}{\\sigma}\\] (subtract mean, divide by SD)"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#comparing-different-species",
    "href": "lectures/L04/lecture-04.html#comparing-different-species",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Comparing different species",
    "text": "Comparing different species\n\n\nCode\n# Create data for two tree species\nset.seed(123)\nspecies1 &lt;- rnorm(1000, mean = 20, sd = 3)  # Tall species\nspecies2 &lt;- rnorm(1000, mean = 15, sd = 2)  # Shorter species\n\n# Plot original heights\ndata.frame(\n  height = c(species1, species2),\n  species = rep(c(\"Species 1\", \"Species 2\"), each = 1000)\n) |&gt;\n  ggplot(aes(x = height, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Tree Height (m)\", y = \"Density\",\n       title = \"Original Height Distributions\")"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#after-standardisation",
    "href": "lectures/L04/lecture-04.html#after-standardisation",
    "title": "Lecture 04 – The central limit theorem",
    "section": "After standardisation",
    "text": "After standardisation\n\n\nCode\n# Convert to z-scores\nz1 &lt;- scale(species1)\nz2 &lt;- scale(species2)\n\n# Plot standardised distributions\ndata.frame(\n  zscore = c(z1, z2),\n  species = rep(c(\"Species 1\", \"Species 2\"), each = 1000)\n) |&gt;\n  ggplot(aes(x = zscore, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Z-score\", y = \"Density\",\n       title = \"Standardised Distributions\")"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#example",
    "href": "lectures/L04/lecture-04.html#example",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Example",
    "text": "Example\nA z-score of 2 means:\n\nSpecies 1: trees above 26 m\nSpecies 2: trees above 19 m\nBoth are equally tall for their species\n\n\n# Calculate z-scores for specific heights\nz_26 &lt;- (26 - 20) / 3\nz_26\n\n[1] 2\n\nz_19 &lt;- (19 - 15) / 2\nz_19\n\n[1] 2"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#working-with-standardised-data-the-standard-normal-curve",
    "href": "lectures/L04/lecture-04.html#working-with-standardised-data-the-standard-normal-curve",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Working with standardised data: The standard normal curve",
    "text": "Working with standardised data: The standard normal curve\nOnce we standardise our data to z-scores, we can calculate probabilities without knowing the original mean or standard deviation.\n\n\nCode\n# Create a visual of the standard normal curve\nggplot(data.frame(x = c(-3.5, 3.5)), aes(x = x)) +\n  stat_function(fun = dnorm, size = 1, colour = \"black\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"blue\") +\n  annotate(\"text\", x = 0.3, y = 0.1, label = \"Mean = 0\") +\n  labs(x = \"Z-score\", y = \"Density\",\n       title = \"Standard Normal Distribution\")"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#answering-questions-with-standardised-data",
    "href": "lectures/L04/lecture-04.html#answering-questions-with-standardised-data",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Answering questions with standardised data",
    "text": "Answering questions with standardised data\n\npnorm(): What proportion of values fall below a given z-score?\nqnorm(): What z-score corresponds to a specific percentile?\ndnorm(): How likely is a specific z-score to occur?\n\nLet’s see how these help us understand our tree height data:\n\n# Example: For a tree with z-score = 1.5 (taller than average)\n# What percentage of trees are shorter than this tree?\npnorm(1.5)\n\n[1] 0.9331928\n\n# Example: How tall must a tree be to be in the tallest 5%?\n# (Find the z-score at the 95th percentile)\nqnorm(0.95)\n\n[1] 1.644854\n\n# Example: Compare likelihood of average height vs. very tall trees\ndnorm(0)/dnorm(2)  # How much more common is z=0 compared to z=2\n\n[1] 7.389056"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#visualising-standard-normal-functions",
    "href": "lectures/L04/lecture-04.html#visualising-standard-normal-functions",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Visualising standard normal functions",
    "text": "Visualising standard normal functions\nThe blue shaded area shows pnorm(1) = 0.84, meaning 84% of trees have a z-score less than 1.\n\n\nCode\n# Create a visual showing pnorm in action\nggplot(data.frame(x = c(-3.5, 3.5)), aes(x = x)) +\n  stat_function(fun = dnorm, size = 1, colour = \"black\") +\n  geom_area(stat = \"function\", fun = dnorm,\n            fill = \"lightblue\", alpha = 0.5, xlim = c(-3.5, 1)) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", colour = \"red\") +\n  annotate(\"text\", x = -1, y = 0.3,\n           label = \"pnorm(1) = 0.84\\n84% of values\\nbelow z=1\") +\n  labs(x = \"Z-score\", y = \"Density\",\n       title = \"Using pnorm() with the Standard Normal Curve\")"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#beyond-the-normal-distribution",
    "href": "lectures/L04/lecture-04.html#beyond-the-normal-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Beyond the normal distribution",
    "text": "Beyond the normal distribution\nDifferent data types need different distributions:\n\n\nCode\n# Create sequence for x-axis\nx &lt;- seq(-4, 4, length = 100)\n\n# Create data frame for plotting\ndistributions &lt;- data.frame(\n  x = rep(x, 3),\n  density = c(\n    dt(x, df = 5),        # t-distribution\n    dchisq(x + 4, df = 3),# chi-square (shifted)\n    dnorm(x)              # normal for comparison\n  ),\n  Distribution = rep(c(\"t\", \"Chi-square\", \"Normal\"), each = 100)\n)\n\n# Plot distributions\nggplot(distributions, aes(x = x, y = density, colour = Distribution)) +\n  geom_line(size = 1) +\n  labs(x = \"Value\", y = \"Density\",\n       title = \"Different Types of Data, Different Distributions\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#beyond-the-normal-distribution-1",
    "href": "lectures/L04/lecture-04.html#beyond-the-normal-distribution-1",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Beyond the normal distribution",
    "text": "Beyond the normal distribution\n\n\nCode\n# Plot distributions\nggplot(distributions, aes(x = x, y = density, colour = Distribution)) +\n  geom_line(size = 1) +\n  labs(\n    x = \"Value\", y = \"Density\",\n    title = \"Different Types of Data, Different Distributions\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\nWe use these for:\n\nComparing means of small samples (t-distribution)\nAnalysing counts and proportions (Chi-square)\nTesting other hypotheses (many more distributions)\n\nWe’ll learn when to use each one as we need them."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#what-is-a-sample",
    "href": "lectures/L04/lecture-04.html#what-is-a-sample",
    "title": "Lecture 04 – The central limit theorem",
    "section": "What is a sample?",
    "text": "What is a sample?\nA sample is a subset of a population used to represent the entire group. It allows us to make inferences about the population without examining every member.\n\nPopulation: all possible measurements\nSample: a subset of the population\nSample size: number of measurements in the sample\nSampling distribution: the distribution of sample means from multiple samples"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#sampling-distribution",
    "href": "lectures/L04/lecture-04.html#sampling-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nA sampling distribution tells us:\n\nHow sample statistics (like means) vary\nWhat patterns emerge when we take many samples\nWhy we get different results with different samples\n\nThink of it as distribution of sample statistics, not individual measurements. Your variable and all its values are a single number in this distribution."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#taking-samples-from-our-rainfall-data",
    "href": "lectures/L04/lecture-04.html#taking-samples-from-our-rainfall-data",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Taking samples from our rainfall data",
    "text": "Taking samples from our rainfall data\nLet’s take samples from our skewed rainfall data and see what happens:\n\n\nCode\n# Our population: all rainfall measurements (simulated)\nset.seed(123)\nrainfall_population &lt;- rainfall_data$rainfall\n\n# Take a small sample of 5 days\nsmall_sample &lt;- sample(rainfall_population, size = 5)\n\n# Calculate the sample mean\nmean(small_sample)\n\n\n[1] 6.49786\n\n\nIf we take another sample, we get a different mean:\n\n# Take another sample\nanother_sample &lt;- sample(rainfall_population, size = 5)\nmean(another_sample)\n\n[1] 6.656836\n\n\nEach sample gives us a slightly different estimate of the true mean rainfall."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#distribution-of-sample-means-for-rainfall",
    "href": "lectures/L04/lecture-04.html#distribution-of-sample-means-for-rainfall",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Distribution of sample means for rainfall",
    "text": "Distribution of sample means for rainfall\nWhat happens if we take many samples and look at all their means?\n\n\nCode\n# Take 1000 samples of size 5 and calculate their means\nset.seed(456)\nrain_means_n5 &lt;- replicate(1000, mean(sample(rainfall_population, size = 5)))\n\n# Plot the distribution of sample means\nggplot(data.frame(sample_mean = rain_means_n5), aes(x = sample_mean)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", colour = \"white\") +\n  geom_vline(xintercept = mean(rainfall_population),\n             colour = \"red\", linetype = \"dashed\") +\n  annotate(\"text\", x = mean(rainfall_population) + 1, y = 80,\n           label = \"Population mean\", colour = \"red\") +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Frequency\",\n       title = \"Distribution of Sample Means (n = 5)\")\n\n\n\nNotice how the sample means are distributed around the true population mean, but the distribution is still somewhat skewed."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#effect-of-sample-size-on-rainfall-means",
    "href": "lectures/L04/lecture-04.html#effect-of-sample-size-on-rainfall-means",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Effect of sample size on rainfall means",
    "text": "Effect of sample size on rainfall means\nWhat happens when we increase our sample size?\n\n\nCode\n# Take 1000 samples of different sizes\nset.seed(404)\nrain_means_n1 &lt;- replicate(1000, mean(sample(rainfall_population, size = 1)))\n# We already have n5 from previous slide\nrain_means_n15 &lt;- replicate(1000, mean(sample(rainfall_population, size = 15)))\nrain_means_n20 &lt;- replicate(1000, mean(sample(rainfall_population, size = 20)))\nrain_means_n30 &lt;- replicate(1000, mean(sample(rainfall_population, size = 30)))\nrain_means_n100 &lt;- replicate(1000, mean(sample(rainfall_population, size = 100)))\n\n# Combine data for comparison with more sample sizes\nrain_means_df &lt;- data.frame(\n  sample_mean = c(rain_means_n1, rain_means_n5, rain_means_n15,\n                 rain_means_n20, rain_means_n30, rain_means_n100),\n  sample_size = factor(rep(c(1, 5, 15, 20, 30, 100), each = 1000),\n                      levels = c(1, 5, 15, 20, 30, 100))\n)\n\n# Create faceted plot\nggplot(rain_means_df, aes(x = sample_mean)) +\n  geom_density(fill = \"skyblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(rainfall_population),\n             colour = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ sample_size, ncol = 3, scales = \"free_y\",\n             labeller = labeller(sample_size = function(x) paste(\"n =\", x))) +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Density\",\n       title = \"Effect of Sample Size on Distribution of Sample Means\") +\n  theme(strip.background = element_rect(fill = \"lightgrey\"),\n        strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\nAs sample size increases, something remarkable happens:\n\nThe distribution becomes more symmetrical and bell-shaped\nThe spread of sample means decreases\nThe distribution approaches a normal distribution - did it become better with 100 samples?"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#zoomed-in",
    "href": "lectures/L04/lecture-04.html#zoomed-in",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Zoomed-in",
    "text": "Zoomed-in\nWhat happens when we increase our sample size?\n\n\nCode\nggplot(rain_means_df, aes(x = sample_mean)) +\n  geom_density(fill = \"skyblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(rainfall_population),\n             colour = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ sample_size, ncol = 3, scales = \"free\",\n             labeller = labeller(sample_size = function(x) paste(\"n =\", x))) +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Density\",\n       title = \"Effect of Sample Size on Distribution of Sample Means\") +\n  theme(strip.background = element_rect(fill = \"lightgrey\"),\n        strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\nAs sample size increases, something remarkable happens:\n\nThe distribution becomes more symmetrical and bell-shaped\nThe spread of sample means decreases\nThe distribution approaches a normal distribution"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#what-is-the-clt",
    "href": "lectures/L04/lecture-04.html#what-is-the-clt",
    "title": "Lecture 04 – The central limit theorem",
    "section": "What is the CLT?",
    "text": "What is the CLT?\nThe Central Limit Theorem (CLT) states that:\nWhen we take sufficiently large random samples from any population:\n\nThe distribution of sample means will be approximately normal\nThe mean of the sample means will equal the population mean\nThe standard deviation of sample means (standard error) equals \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nThis is true regardless of the shape of the original population distribution."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#comparing-original-data-with-sampling-distribution",
    "href": "lectures/L04/lecture-04.html#comparing-original-data-with-sampling-distribution",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Comparing original data with sampling distribution",
    "text": "Comparing original data with sampling distribution\nLet’s compare our original skewed rainfall data with the distribution of sample means:\n\n\nCode\n# Plot original population and sampling distributions\np1 &lt;- ggplot(data.frame(x = rainfall_population), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1,\n                fill = \"steelblue\", colour = \"white\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(rainfall_population),\n                           sd = sd(rainfall_population)),\n                colour = \"red\", linetype = \"dashed\") +\n  labs(x = \"Rainfall (mm)\", y = \"Density\",\n       title = \"Original Population\\n(Skewed)\")\n\np2 &lt;- ggplot(data.frame(x = rain_means_n30), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.5,\n                fill = \"skyblue\", colour = \"white\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(rain_means_n30),\n                           sd = sd(rain_means_n30)),\n                colour = \"red\") +\n  labs(x = \"Sample Mean Rainfall (mm)\", y = \"Density\",\n       title = \"Sampling Distribution\\n(n = 30)\")\n\nplot_grid(p1, p2, ncol = 2)\n\n\n\nEven with skewed data, the sampling distribution becomes normal as sample size increases!"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#the-clt-with-binary-biological-data",
    "href": "lectures/L04/lecture-04.html#the-clt-with-binary-biological-data",
    "title": "Lecture 04 – The central limit theorem",
    "section": "The CLT with binary biological data",
    "text": "The CLT with binary biological data\nIn biology, many processes have binary outcomes: a gene is expressed or not, a cell divides or not, an organism survives or dies. Let’s demonstrate the CLT with gene expression data:\n\n\nCode\n# Create a binary population (gene expression: 0 = off, 1 = on)\n# Assume the gene is expressed in 30% of cells\nset.seed(123)\nn_cells &lt;- 100000\np_expressed &lt;- 0.3\ngene_expression &lt;- rbinom(n_cells, size = 1, prob = p_expressed)\n\n# Plot the original population distribution\nggplot(data.frame(expression = factor(gene_expression)), aes(x = expression)) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Gene Expression (0 = off, 1 = on)\", y = \"Frequency\",\n       title = \"Original Population: Binary Gene Expression\",\n       subtitle = \"Gene expressed in 30% of cells\")\n\n\n\nThis is a highly non-normal distribution - it’s discrete with only two possible values!"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#sampling-distribution-with-increasing-sample-sizes",
    "href": "lectures/L04/lecture-04.html#sampling-distribution-with-increasing-sample-sizes",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Sampling distribution with increasing sample sizes",
    "text": "Sampling distribution with increasing sample sizes\n\n\nCode\n# Define sample sizes (powers of 2)\nsample_sizes &lt;- c(1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048)\n\n# Function to generate sample means for a given sample size\ngenerate_sample_means &lt;- function(sample_size, n_samples = 1000) {\n  replicate(n_samples, mean(sample(gene_expression, size = sample_size, replace = TRUE)))\n}\n\n# Generate sample means for each sample size\nset.seed(456)\nall_sample_means &lt;- lapply(sample_sizes, generate_sample_means)\nnames(all_sample_means) &lt;- paste(\"n =\", sample_sizes)\n\n# Combine into a data frame\nsample_means_df &lt;- data.frame(\n  sample_mean = unlist(all_sample_means),\n  sample_size = factor(rep(names(all_sample_means), each = 1000),\n                      levels = names(all_sample_means))\n)\n\n# Create faceted plot with density plots instead of histograms\nggplot(sample_means_df, aes(x = sample_mean)) +\n  geom_density(fill = \"purple\", color = \"white\", alpha = 0.7) +\n  geom_vline(xintercept = mean(gene_expression),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~ sample_size, scales = \"free_y\", ncol = 4) +\n  labs(x = \"Sample Mean (Proportion of Cells with Gene Expressed)\",\n       y = \"Density\",\n       subtitle = \"Red line shows true population mean (0.3)\") +\n  theme(strip.background = element_rect(fill = \"lavender\"),\n        strip.text = element_text(size = 10, face = \"bold\"))\n\n\n\nEven with the most non-normal data possible (binary data), the sampling distribution of means becomes normal with sufficient sample size."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#another-example-seed-germination-discrete-data",
    "href": "lectures/L04/lecture-04.html#another-example-seed-germination-discrete-data",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Another example: Seed germination (discrete data)",
    "text": "Another example: Seed germination (discrete data)\nLet’s explore another example with a discrete, highly skewed distribution:\nIn ecological restoration, practitioners need to estimate germination rates of native seeds. Imagine each seed has only a 10% chance of germinating (p = 0.1).\n\n\nCode\n# Create a binomial population - number of germinated seeds in batches of 20\nset.seed(789)\nn_trials &lt;- 20  # 20 seeds per batch\np_success &lt;- 0.1  # 10% germination rate\nn_batches &lt;- 10000  # 10,000 batches to create our \"population\"\n\n# Generate the population data\ngermination_population &lt;- rbinom(n_batches, size = n_trials, prob = p_success)\n\n# Plot the population distribution\nggplot(data.frame(germinated = germination_population), aes(x = germinated)) +\n  geom_bar(fill = \"darkgreen\", color = \"white\") +\n  scale_x_continuous(breaks = 0:10) +\n  labs(x = \"Number of Seeds Germinated (out of 20)\",\n       y = \"Frequency\",\n       title = \"Seed Germination Population Distribution\",\n       subtitle = \"Binomial(n=20, p=0.1): Discrete and highly skewed\")\n\n\n\nThis distribution is: - Discrete (only whole numbers of seeds can germinate) - Highly skewed (most batches have very few germinating seeds) - Very different from our continuous rainfall example"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#sampling-distribution-for-seed-germination-data",
    "href": "lectures/L04/lecture-04.html#sampling-distribution-for-seed-germination-data",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Sampling distribution for seed germination data",
    "text": "Sampling distribution for seed germination data\nWhat happens when we take samples from this discrete, skewed distribution?\n\n\nCode\n# Take samples of different sizes and calculate means\nset.seed(123)\nseed_means_n5 &lt;- replicate(1000, mean(sample(germination_population, size = 5)))\nseed_means_n30 &lt;- replicate(1000, mean(sample(germination_population, size = 30)))\n\n# Calculate theoretical standard error for n=30\nse_n30 &lt;- sd(germination_population)/sqrt(30)\n\n# Plot the original population vs. sampling distributions\np1 &lt;- ggplot(data.frame(x = germination_population), aes(x = x)) +\n  geom_bar(fill = \"darkgreen\", color = \"white\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 2)) +\n  labs(x = \"Seeds Germinated\", y = \"Frequency\",\n       title = \"Original Population\\n(Discrete & Skewed)\")\n\np2 &lt;- ggplot(data.frame(x = seed_means_n5), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.2,\n                fill = \"lightgreen\", colour = \"white\") +\n  geom_vline(xintercept = mean(germination_population),\n             colour = \"red\", linetype = \"dashed\") +\n  labs(x = \"Sample Mean (n = 5)\", y = \"Density\",\n       title = \"Sampling Distribution\\nSmall Samples\")\n\n# For n=30, use density scale and properly scaled normal curve\np3 &lt;- ggplot(data.frame(x = seed_means_n30), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.1,\n                fill = \"lightgreen\", colour = \"white\") +\n  geom_vline(xintercept = mean(germination_population),\n             colour = \"red\", linetype = \"dashed\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(germination_population),\n                           sd = se_n30),\n                colour = \"red\", size = 1) +\n  labs(x = \"Sample Mean (n = 30)\", y = \"Density\",\n       title = \"Sampling Distribution\\nLarger Samples\") +\n  # Set x-axis limits to focus on the relevant range\n  xlim(mean(germination_population) - 4*se_n30,\n       mean(germination_population) + 4*se_n30)\n\n# Arrange plots\nplot_grid(p1, p2, p3, ncol = 3)\n\n\n\nEven with discrete, highly skewed data, the CLT still applies:\n\nThe sampling distribution becomes more normal as sample size increases\nThe sampling distribution centers on the population mean (≈ 2, since 20 × 0.1 = 2)\nThe standard error decreases as sample size increases"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#standard-error-measuring-the-precision-of-sample-means",
    "href": "lectures/L04/lecture-04.html#standard-error-measuring-the-precision-of-sample-means",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Standard error: Measuring the precision of sample means",
    "text": "Standard error: Measuring the precision of sample means\nThe standard error (SE) tells us how much sample means typically vary:\n\\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nWhere: - \\(\\sigma\\) is the population standard deviation - \\(n\\) is the sample size\n\n# Calculate theoretical standard errors\npop_sd &lt;- sd(rainfall_population)\nse_n5 &lt;- pop_sd / sqrt(5)\nse_n15 &lt;- pop_sd / sqrt(15)\nse_n30 &lt;- pop_sd / sqrt(30)\n\n# Compare standard errors\nse_n5\n\n[1] 2.89159\n\nse_n15\n\n[1] 1.66946\n\nse_n30\n\n[1] 1.180487\n\n\nAs sample size increases, standard error decreases, making our estimates more precise."
  },
  {
    "objectID": "lectures/L04/lecture-04.html#standard-error-vs.-standard-deviation",
    "href": "lectures/L04/lecture-04.html#standard-error-vs.-standard-deviation",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Standard error vs. standard deviation",
    "text": "Standard error vs. standard deviation\nWhile we have been using the standard deviation to measure variability in our data, the standard error measures variability in our sample means.\n\n\nCode\n# Create comparison table between Standard Deviation and Standard Error\nsd_se_comparison &lt;- data.frame(\n  Aspect = c(\"Definition\", \"Formula\", \"Measures\", \"Effect of sample size\", \"Typical use\"),\n  Standard_Deviation = c(\n    \"Measures spread of individual data points around their mean\",\n    \"σ = √[Σ(x-μ)²/N] or s = √[Σ(x-x̄)²/(n-1)]\",\n    \"Variability within a dataset\",\n    \"Unaffected by sample size\",\n    \"Describing the spread of data in a single sample\"\n  ),\n  Standard_Error = c(\n    \"Measures precision of a sample statistic (usually the mean)\",\n    \"SE = σ/√n or SE = s/√n\",\n    \"Variability between sample means from the same population\",\n    \"Decreases as sample size increases (∝ 1/√n)\",\n    \"Inferential statistics, confidence intervals, hypothesis testing\"\n  )\n)\n\nkable(sd_se_comparison, \n      col.names = c(\"Aspect\", \"Standard Deviation (SD)\", \"Standard Error (SE)\"),\n      caption = \"Standard Deviation vs. Standard Error\")\n\n\n\nStandard Deviation vs. Standard Error\n\n\n\n\n\n\n\nAspect\nStandard Deviation (SD)\nStandard Error (SE)\n\n\n\n\nDefinition\nMeasures spread of individual data points around their mean\nMeasures precision of a sample statistic (usually the mean)\n\n\nFormula\nσ = √[Σ(x-μ)²/N] or s = √[Σ(x-x̄)²/(n-1)]\nSE = σ/√n or SE = s/√n\n\n\nMeasures\nVariability within a dataset\nVariability between sample means from the same population\n\n\nEffect of sample size\nUnaffected by sample size\nDecreases as sample size increases (∝ 1/√n)\n\n\nTypical use\nDescribing the spread of data in a single sample\nInferential statistics, confidence intervals, hypothesis testing"
  },
  {
    "objectID": "lectures/L04/lecture-04.html#practical-implications-of-the-clt",
    "href": "lectures/L04/lecture-04.html#practical-implications-of-the-clt",
    "title": "Lecture 04 – The central limit theorem",
    "section": "Practical implications of the CLT",
    "text": "Practical implications of the CLT\nThe Central Limit Theorem enables modern inferential statistics:\n\nMaking inferences from samples to populations\n\nEstimate population parameters with quantifiable precision\nCalculate confidence intervals for parameters (coming soon!)\nPerform hypothesis tests with known error rates\n\nWorks regardless of population distribution\n\nNon-normal data still produces normally distributed sample means\nEnables parametric tests even when original data is skewed\nOnly requires sufficiently large samples (n ≥ 30 is often adequate)\n\nFoundation for statistical theory"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#recap-we-have-one-sample",
    "href": "lectures/L06/lecture-06.html#recap-we-have-one-sample",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Recap: we have one sample",
    "text": "Recap: we have one sample\n\nOne-sample t-test: compare the sample of data to a fixed value of interest (e.g. a hypothesised value, or a population mean).\n\n\nExamples\n\nIs the mean height of students in ENVX different from the population mean of 170 cm?\nIs the mean heart rate of students in ENVX different from the population mean of 70 bpm\n\n\n\nCode\nlibrary(patchwork)\nlibrary(ggplot2)\n\nset.seed(108)\nheights &lt;- rnorm(100, mean = 170, sd = 10)\nheart &lt;- rnorm(100, mean = 70, sd = 10)\n\n# heights\np1 &lt;- ggplot(data.frame(heights), aes(x = heights)) +\n  geom_histogram(aes(y = after_stat(density)),\n    binwidth = 5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(heights)), \n    color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Height of students\") +\n  theme_classic()\n\n\n# heart rate\np2 &lt;- ggplot(data.frame(heart), aes(x = heart)) +\n  geom_histogram(aes(y = after_stat(density)), \n    binwidth = 5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean(heart)), \n    color = \"red\", linetype = \"dashed\") +\n  ggtitle(\"Heart rate\") +\n  theme_classic()\n\n\np1 + p2"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#what-if-we-want-to-compare-a-sample-of-data-to-another-sample",
    "href": "lectures/L06/lecture-06.html#what-if-we-want-to-compare-a-sample-of-data-to-another-sample",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "What if we want to compare a sample of data to another sample?",
    "text": "What if we want to compare a sample of data to another sample?\n\nExamples\n\nIs the mean height of students in ENVX1002 different from ENVX2001?\nIs the mean heart rate of students in ENVX1002 different from ENVX2001?\n\n\n\nCode\nset.seed(129)\n## Generate data for 2 groups\nheights_all &lt;- data.frame(\n  group = rep(c(\"ENVX1002\", \"ENVX2001\"), each = 100),\n  heights = c(rnorm(100, mean = 165, sd = 7), \n    rnorm(100, mean = 185, sd = 9)))\nheart_all &lt;- data.frame(\n  group = rep(c(\"ENVX1002\", \"ENVX2001\"), each = 100),\n  heart_rates = c(rnorm(100, mean = 70, sd = 10), \n    rnorm(100, mean = 72, sd = 5)))\n\n## Plot\np3 &lt;- ggplot(heights_all, aes(x = heights, fill = group)) +\n  geom_histogram(aes(y = ..density..), \n    binwidth = 5, color = \"black\", alpha = .2) +\n  geom_vline(aes(xintercept = 170), \n    color = \"red\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 185),\n    color = \"blue\", linetype = \"dashed\") +\n  ggtitle(\"Height of students (cm)\") +\n  theme_classic()\np4 &lt;- ggplot(heart_all, aes(x = heart_rates, fill = group)) +\n  geom_histogram(aes(y = ..density..), \n    binwidth = 5, color = \"black\", alpha = .2) +\n  geom_vline(aes(xintercept = 70), \n    color = \"red\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = 72), \n    color = \"blue\", linetype = \"dashed\") +\n  ggtitle(\"Heart rates (bpm)\") +\n  theme_classic()\n\np3 + p4"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#comparing-two-samples-visualisation",
    "href": "lectures/L06/lecture-06.html#comparing-two-samples-visualisation",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Comparing two samples: visualisation",
    "text": "Comparing two samples: visualisation\n\n\n\n\n\n\n\n\n\n\n\n\nCode\np5 &lt;- ggplot(heights_all, aes(x = group, y = heights, fill = group)) +\n  geom_boxplot(alpha = .2) +\n  ggtitle(\"Height of students (cm)\") +\n  theme_classic()\np6 &lt;- ggplot(heart_all, aes(x = group, y = heart_rates, fill = group)) +\n  geom_boxplot(alpha = .2) +\n  ggtitle(\"Heart rates (bpm)\") +\n  theme_classic()\n\np5 + p6"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#some-considerations-the-boxplot",
    "href": "lectures/L06/lecture-06.html#some-considerations-the-boxplot",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Some considerations: the boxplot",
    "text": "Some considerations: the boxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-off between being able to see the distribution of the data and being able to compare between groups.\nThe recommended approach when comparing two or more groups of data in most cases."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#data",
    "href": "lectures/L06/lecture-06.html#data",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Data",
    "text": "Data\nA simulated example (data is not real):\n\n\nCode\nredbull &lt;- data.frame(\n  group = c(rep(\"redbull\", 12), rep(\"control\", 12)), \n  heart_rate = c(72, 88, 72, 88, 76, 75, 84, 80, 60, 96, 80,  84, 84, 76, 68, 80, 64, 62, 74, 84, 68, 96, 80, 64))\n\n\n\nExperimental design: two groups of students selected at random, without replacement, from the ENVX1002 cohort.\n\nredbull group: students who consumed 250 ml of Red Bull.\ncontrol group: students who consumed 250 ml of water (control group).\n\nHeart rate in beats per minute (bpm) was measured 20 minutes after consumption.\n\n\nStructure of data\n\n\nCode\nstr(redbull)\n\n\n'data.frame':   24 obs. of  2 variables:\n $ group     : chr  \"redbull\" \"redbull\" \"redbull\" \"redbull\" ...\n $ heart_rate: num  72 88 72 88 76 75 84 80 60 96 ..."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#hypothesis",
    "href": "lectures/L06/lecture-06.html#hypothesis",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor a two-sample t-test, the null hypothesis is that the means of the two groups are equal, and the alternative hypothesis is that the means are different.\n\\[H_0: \\mu_{\\text{redbull}} = \\mu_{\\text{control}}\\] \\[H_1: \\mu_{\\text{redbull}} \\neq \\mu_{\\text{control}}\\]\n\nCompare this to the one-sample t-test, where the null hypothesis is that the sample mean is equal to a fixed value: \\[H_0: \\mu = \\mu_0\\] \\[H_1: \\mu \\neq \\mu_0\\]"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#assumptions",
    "href": "lectures/L06/lecture-06.html#assumptions",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Assumptions",
    "text": "Assumptions\nThe assumptions of the two-sample t-test include:\n\nNormality: the data are normally distributed.\nHomogeneity of variance: the variances of the two groups are equal.\n\n\nWhy are these assumptions important?\n\nSince the t-test compares the means of two groups, normality ensures that the means are the best estimate of the population means.\nEqual variances indicates that the two groups have similar “noise” influencing their means, except for the “treatment” effect.\n\nIn the Red Bull example, this means that the range of heart rate values in students for both groups is similar, except for the effect of consuming Red Bull."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#normality-histogram",
    "href": "lectures/L06/lecture-06.html#normality-histogram",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Normality: histogram",
    "text": "Normality: histogram\n\nWe visually inspect the distribution of the data using histograms, generally for each group.\nLook out for: symmetry, skewness, and multimodality.\nHard to visualise when n (sample size is small)\n\n\nR base graphicsggplot2ggpubr\n\n\n\n\nCode\npar(mfrow = c(1, 2))\nhist(redbull$heart_rate[redbull$group == \"redbull\"], \n  main = \"Red Bull group\", xlab = \"Heart rate (bpm)\", \n  col = \"lightblue\", border = \"black\")\nhist(redbull$heart_rate[redbull$group == \"control\"],\n  main = \"Control group\", xlab = \"Heart rate (bpm)\", \n  col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nggplot(redbull, aes(x = heart_rate, fill = group)) +\n    geom_histogram(aes(y = ..density..),\n        binwidth = 12, color = \"black\", alpha = .5\n    ) +\n    facet_wrap(~group) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggpubr)\ngghistogram(redbull, x = \"heart_rate\", fill = \"group\", binwidth = 6) +\n    facet_wrap(~group) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The data appear to be normally distributed, but it is better to confirm this with a QQ-plot."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#normality-qq-plot",
    "href": "lectures/L06/lecture-06.html#normality-qq-plot",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Normality: QQ-plot",
    "text": "Normality: QQ-plot\n\nThe qq-plot is a graphical method to specifically assess the normality of the data. Again, we look at the data for each group.\nLook out for: deviations from the straight line.\n\n\nR base graphicsggplot2ggpubr\n\n\n\n\nCode\npar(mfrow = c(1, 2))\nqqnorm(redbull$heart_rate[redbull$group == \"redbull\"],\n    main = \"Red Bull group\", xlab = \"Theoretical quantiles\",\n    ylab = \"Sample quantiles\", col = \"blue\"\n)\nqqline(redbull$heart_rate[redbull$group == \"redbull\"], col = \"red\")\n\nqqnorm(redbull$heart_rate[redbull$group == \"control\"],\n    main = \"Control group\", xlab = \"Theoretical quantiles\",\n    ylab = \"Sample quantiles\", col = \"blue\"\n)\nqqline(redbull$heart_rate[redbull$group == \"control\"], col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nggplot(redbull, aes(sample = heart_rate)) +\n    stat_qq() +\n    stat_qq_line() +\n    facet_wrap(~group) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggpubr)\nggqqplot(redbull$heart_rate) +\n    facet_wrap(~ redbull$group) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The data appear to be normally distributed."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#normality-formal-test",
    "href": "lectures/L06/lecture-06.html#normality-formal-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Normality: formal test",
    "text": "Normality: formal test\n\nUse the Shapiro-Wilk test which tests the null hypothesis that the data are normally distributed.\nThis test is sensitive to deviations from normality in the tails of the distribution, and is suitable for small sample sizes (about 5 to 50 observations).\n\n\n\nCode\nshapiro.test(redbull$heart_rate[redbull$group == \"redbull\"])\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  redbull$heart_rate[redbull$group == \"redbull\"]\nW = 0.97459, p-value = 0.9524\n\n\nCode\nshapiro.test(redbull$heart_rate[redbull$group == \"control\"])\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  redbull$heart_rate[redbull$group == \"control\"]\nW = 0.93733, p-value = 0.4643\n\n\n\nConclusion: p-values are greater than 0.05, so we do not reject the null hypothesis of normality. The data are normally distributed."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#what-if-the-normality-assumption-is-violated",
    "href": "lectures/L06/lecture-06.html#what-if-the-normality-assumption-is-violated",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "What if the normality assumption is violated?",
    "text": "What if the normality assumption is violated?\n\nThe t-test is robust to deviations from normality, especially for large sample sizes due to the Central Limit Theorem.\nIf the sample size is small, consider using a non-parametric test (e.g. the Wilcoxon rank-sum test): next week\nAlternatively, transform the data: later"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#equal-variances-boxplot",
    "href": "lectures/L06/lecture-06.html#equal-variances-boxplot",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Equal variances: boxplot",
    "text": "Equal variances: boxplot\n\nWe visually inspect the spread of the data using boxplots, generally for each group.\nLook out for: differences in spread, outliers, and symmetry.\n\n\nR base graphicsggplot2ggpubr\n\n\n\n\nCode\npar(mfrow = c(1, 2))\nboxplot(heart_rate ~ group,\n    data = redbull,\n    main = \"Heart rate\", xlab = \"Group\", ylab = \"Heart rate (bpm)\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nggplot(redbull, aes(x = group, y = heart_rate, fill = group)) +\n    geom_boxplot(alpha = .2) +\n    labs(x = \"Group\", y = \"Heart rate (bpm)\") +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggpubr)\nggboxplot(redbull,\n    x = \"group\", y = \"heart_rate\",\n    fill = \"group\", alpha = .2\n) +\n  labs(x = \"Group\", y = \"Heart rate (bpm)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The spread of the data appears to be similar between the two groups."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#equal-variances-formal-tests",
    "href": "lectures/L06/lecture-06.html#equal-variances-formal-tests",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Equal variances: formal tests",
    "text": "Equal variances: formal tests\n\nBartlett’s and Levene’s tests may be used to test the null hypothesis that the variances of the groups are equal.\nThese tests are sensitive to deviations from normality (Levene’s test is less so compared to Bartlett’s), and are suitable for small sample sizes.\n\n\nLevene’s testBartlett’s test\n\n\n\n\nCode\nlibrary(car)\nleveneTest(heart_rate ~ group, data = redbull)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   0.289 0.5962\n      22               \n\n\n\n\n\n\nCode\nbartlett.test(heart_rate ~ group, data = redbull)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  heart_rate by group\nBartlett's K-squared = 0.075121, df = 1, p-value = 0.784\n\n\n\n\n\n\nConclusion: p-values are greater than 0.05, so we do not reject the null hypothesis of equal variances. The variances of the two groups are equal."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#what-if-the-equal-variance-assumption-is-violated",
    "href": "lectures/L06/lecture-06.html#what-if-the-equal-variance-assumption-is-violated",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "What if the equal variance assumption is violated?",
    "text": "What if the equal variance assumption is violated?\nSome debate exists on what to do, but choices include:\n\nUse the Welch’s t-test, which is robust to unequal variances: coming up next\nTransform the data to stabilise the variances: later\nPerform a non-parametric test: next week"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#the-welchs-t-test",
    "href": "lectures/L06/lecture-06.html#the-welchs-t-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "The Welch’s t-test",
    "text": "The Welch’s t-test\n\nThe Welch’s t-test is a modification of the two-sample t-test that does not assume equal variances.\nAlso applicable when the sample sizes are unequal.\n\nWhy not use the Welch’s t-test all the time?\n\nOngoing debate on whether to use the Welch’s t-test or the Student’s t-test when the variances are equal.\nThe Welch’s t-test is generally considered more robust, and is the default in R’s t.test() function.\nYou can still use the Student’s t-test by setting var.equal = TRUE in the t.test() function."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#are-the-assumptions-of-normality-and-homogeneity-of-variance-met",
    "href": "lectures/L06/lecture-06.html#are-the-assumptions-of-normality-and-homogeneity-of-variance-met",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Are the assumptions of normality and homogeneity of variance met?",
    "text": "Are the assumptions of normality and homogeneity of variance met?\nWhen reporting in journals, it is common to simply state that the assumptions were met and what tests were used to confirm them, without showing the exact results of the tests!\n\nExample 1\n\nThe assumptions of normality and homogeneity of variance were met for the data (Shapiro-Wilk test, \\(p &gt; 0.05\\); Levene’s test, \\(p &gt; 0.05\\)). Thus, we performed a two-sample t-test…\n\n\n\nExample 2\n\nVisual inspection of the histograms, QQ-plots, and boxplots showed that the data met the assumptions of both normality and homogeneity of variance. Thus, we performed a two-sample t-test..\n\n\n\nFor your lab reports, you should show the results of the tests (because we want to check your work!)"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#performing-the-t-test",
    "href": "lectures/L06/lecture-06.html#performing-the-t-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Performing the t-test",
    "text": "Performing the t-test\n\nt.test(heart_rate ~ group, data = redbull, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  heart_rate by group\nt = -1.1365, df = 22, p-value = 0.268\nalternative hypothesis: true difference in means between group control and group redbull is not equal to 0\n95 percent confidence interval:\n -12.947117   3.780451\nsample estimates:\nmean in group control mean in group redbull \n             75.00000              79.58333 \n\n\nResults indicate that the means of the two groups are not significantly different (p = 0.27).\n\nCompare with the Welch’s t-test\n\nt.test(heart_rate ~ group, data = redbull)\n\n\n    Welch Two Sample t-test\n\ndata:  heart_rate by group\nt = -1.1365, df = 21.845, p-value = 0.2681\nalternative hypothesis: true difference in means between group control and group redbull is not equal to 0\n95 percent confidence interval:\n -12.950568   3.783902\nsample estimates:\nmean in group control mean in group redbull \n             75.00000              79.58333"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#conclusion",
    "href": "lectures/L06/lecture-06.html#conclusion",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Conclusion",
    "text": "Conclusion\nDifferences in heart rate we not statistically significant between the Red Bull and control groups (t22 = -1.1, p = 0.27) indicating that Red Bull did not significantly increase the heart rate of students sampled from ENVX1002."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#are-the-two-sample-independent",
    "href": "lectures/L06/lecture-06.html#are-the-two-sample-independent",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Are the two sample independent?",
    "text": "Are the two sample independent?\nWhen testing if two samples are different from each other, we need to consider two possible scenarios:\n\n\nIndependent samples: The samples are drawn from two different populations, or the samples are not related to each other – independent groups.\nRelated samples: The samples are drawn from the same population, and/or the samples are related to each other – repeated measures or matched pairs.\n\n\n\nIf the samples are related, a paired t-test is more appropriate than a two-sample t-test as it accounts for the relationship between the samples that could confound the results."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#paired-t-test",
    "href": "lectures/L06/lecture-06.html#paired-t-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Paired t-test",
    "text": "Paired t-test\nExperimental design (what if?)\nBefore\n\nTwo groups of students selected at random, without replacement, from the ENVX1002 cohort.\n\n\nPaired design\nThe same student was used in a before/after experiment, where the heart rate was measured before and after consuming 250ml of Red Bull. Twelve (12) students were selected at random from the ENVX1002 cohort.\n\nData is no longer independent, as the same student is measured twice.\nThe student now confounds the results, as the heart rate of the same student is likely to be correlated even without consuming Red Bull.\nTotal number of students is now 12, not 24.\nLet’s assume the data collected are exactly the same."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#hypothesis-1",
    "href": "lectures/L06/lecture-06.html#hypothesis-1",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor a paired t-test, the null hypothesis is that the mean difference between the two groups is zero, and the alternative hypothesis is that the mean difference is different from zero.\n\\[H_0: \\mu_{\\text{diff}} = 0\\] \\[H_1: \\mu_{\\text{diff}} \\neq 0\\]\n\nCompare this to the two-sample t-test, where the null hypothesis is that the means of the two groups are equal: \\[H_0: \\mu_{\\text{redbull}} = \\mu_{\\text{control}}\\] \\[H_1: \\mu_{\\text{redbull}} \\neq \\mu_{\\text{control}}\\]"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#assumptions-of-the-paired-t-test",
    "href": "lectures/L06/lecture-06.html#assumptions-of-the-paired-t-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Assumptions of the paired t-test",
    "text": "Assumptions of the paired t-test\n\nThe assumption of the paired t-test is that the differences between the two groups are normally distributed.\nThere is no assumption of equal variances, as the paired t-test is a one-sample t-test on the differences.\n\nAnother way to think about it is that since the data are paired, the variance of the differences is the same for both groups."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#performing-the-paired-t-test",
    "href": "lectures/L06/lecture-06.html#performing-the-paired-t-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Performing the paired t-test",
    "text": "Performing the paired t-test\nThere are two ways.\n\n\n\nMethod 1: Calculate the differences, then perform a one-sample t-test using t.test()\n\ndiff &lt;- redbull$heart_rate[redbull$group == \"redbull\"] - \n  redbull$heart_rate[redbull$group == \"control\"]\nt.test(diff)\n\n\n    One Sample t-test\n\ndata:  diff\nt = 1.6578, df = 11, p-value = 0.1256\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.501628 10.668294\nsample estimates:\nmean of x \n 4.583333 \n\n\n\n\n\nMethod 2: Use the t.test() function with the paired = TRUE argument\n\n# t.test(heart_rate ~ group, data = redbull, \n#   paired = TRUE)\n\n\n\n\nThe results for both methods are identical; the mean difference is not significantly different from zero (p = 0.13)."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#recap-assumptions-of-the-two-sample-t-test",
    "href": "lectures/L06/lecture-06.html#recap-assumptions-of-the-two-sample-t-test",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Recap: Assumptions of the two-sample t-test",
    "text": "Recap: Assumptions of the two-sample t-test\nWith independent samples:\n\nNormality: the data are normally distributed\nHomogeneity of variance (equal variances): the variances of the two groups are equal\n\nWith paired samples:\n\nNormality: the differences between the paired samples are normally distributed\nEqual variances is implied"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#if-we-analyse-the-data-anyway",
    "href": "lectures/L06/lecture-06.html#if-we-analyse-the-data-anyway",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "If we analyse the data anyway…",
    "text": "If we analyse the data anyway…\nThe t-test:\n\nmay provide incorrect results as mean and variance calculations depend on normally distributed data.\nmay be less powerful (i.e., less likely to detect a true difference).\nmay be biased (i.e., systematically over- or under-estimating the true difference)."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#what-can-we-do",
    "href": "lectures/L06/lecture-06.html#what-can-we-do",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "What can we do?",
    "text": "What can we do?\nThe t-test is quite robust to violations of normality, especially when the sample size is large. However, the assumption of equal variances is more critical – we cannot simply depend on large sample sizes to “fix” the problem.\nOptions include:\n\nTransform the data to normalise the data and/or scale the variance\nUse a Welch’s t-test or a Welch’s ANOVA (limited cases)\nUse a non-parametric test, such as the Mann-Whitney U test or Wilcoxon signed-rank test (paired samples) – however, these tests have less power than the t-test i.e. less likely to detect a true difference."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#is-the-food-collected-by-ants-different-between-two-sites",
    "href": "lectures/L06/lecture-06.html#is-the-food-collected-by-ants-different-between-two-sites",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Is the food collected by ants different between two sites?",
    "text": "Is the food collected by ants different between two sites?\nData structure\n\n\nCode\nlibrary(tidyverse)\nants &lt;- read.csv(\"data/ants.csv\") %&gt;%\n  mutate(Tree = factor(Tree))\n\nglimpse(ants)\n\n\nRows: 54\nColumns: 2\n$ Food &lt;dbl&gt; 11.9, 33.3, 4.6, 5.5, 6.2, 11.0, 24.3, 20.7, 5.7, 12.6, 10.2, 4.7…\n$ Tree &lt;fct&gt; Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Ro…\n\n\nWe want to compare the mean biomass of food, collected by ants between the two sites in dry weight (mg) of prey, divided by the total number of ants leaving the tree in 30 minutes."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#visualising-the-data",
    "href": "lectures/L06/lecture-06.html#visualising-the-data",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Visualising the data",
    "text": "Visualising the data\n\n\nCode\nlibrary(ggplot2)\np_ants &lt;-\n  ggplot(ants, aes(x = Tree, y = Food)) +\n  geom_boxplot() +\n  ylab(\"Biomass of food (mg per ant)\") +\n  theme_minimal()\n\np_ants\n\n\n\nDoes this data meet the assumptions of the two-sample t-test?"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#checking-assumptions",
    "href": "lectures/L06/lecture-06.html#checking-assumptions",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nWe have some idea that the data may not be normally distributed, but are not quite sure. So let’s check using the Q-Q plot.\n\n\nCode\nggplot(ants, aes(sample = Food)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Tree) +\n  theme_minimal()\n\n\n\n\nCurvature of the data points away from the line indicates non-normality.\nBoxplots (previous slide) suggest equal variances.\nLet’s transform the data."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#picking-a-transformation",
    "href": "lectures/L06/lecture-06.html#picking-a-transformation",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Picking a transformation",
    "text": "Picking a transformation\nWe need to consider the type of data and the shape of its distribution when choosing a transformation. These can be assessed using:\n\nHistograms and Q-Q plots to assess normality - DONE\nBox plots to assess homogeneity of variance - DONE\nSkewness and kurtosis to assess the shape of the distribution - NEXT"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#skewness",
    "href": "lectures/L06/lecture-06.html#skewness",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Skewness",
    "text": "Skewness\nThe degree of asymmetry in the data distribution when compared to a normal distribution.\n\n\nRepresented by the skewness coefficient (\\(\\gamma_1\\)) and can be positive, negative, or zero.\nSkewness values between -0.5 and 0.5 are considered acceptable (fairly symmetrical).\nNegative skewness indicates a left-skewed distribution, while positive skewness indicates a right-skewed distribution.\nAbove 1 or below -1, the distribution is considered highly skewed."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#example-skewness",
    "href": "lectures/L06/lecture-06.html#example-skewness",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Example: skewness",
    "text": "Example: skewness\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\n\nx &lt;- seq(0, 1, length.out = 100)\n\n# Calculate the density of the Beta distribution at these points\ndata1 &lt;- data.frame(x = x, y = dbeta(x, 5, 2), dist = \"Negative (left) skewed\")\ndata2 &lt;- data.frame(x = x, y = dbeta(x, 5, 5), dist = \"Symmetrical\")\ndata3 &lt;- data.frame(x = x, y = dbeta(x, 2, 5), dist = \"Positive (right) skewed\")\ndata &lt;- rbind(data1, data2, data3) %&gt;%\n  mutate(dist = factor(dist, levels = c(\"Negative (left) skewed\", \"Symmetrical\", \"Positive (right) skewed\")))\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_line(color = \"skyblue\") +\n  geom_area(fill = \"skyblue\", alpha = 0.4) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  facet_wrap(~dist) +\n  ylab(\"density\") +\n  xlab(\"\")"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#kurtosis",
    "href": "lectures/L06/lecture-06.html#kurtosis",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Kurtosis",
    "text": "Kurtosis\nUsed to describe the extreme values (outliers) in the distribution versus the tails.\n\n\nHigh kurtosis (&gt;3) indicates a distribution with heavy tails and a peaked centre. When this happens, we should investigate the data for outliers.\nLow kurtosis (&lt;3) indicates a distribution with light tails and a flat centre. There are fewer to no outliers in the data."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#example-kurtosis",
    "href": "lectures/L06/lecture-06.html#example-kurtosis",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Example: kurtosis",
    "text": "Example: kurtosis\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(moments)\n\n# Generate data\nset.seed(123)\nx1 &lt;- seq(-10, 10, length.out = 1000)\nx2 &lt;- seq(-5, 5, length.out = 1000)\ndata1 &lt;- data.frame(x = x1, y = dt(x1, df = 1), dist = \"High Kurtosis\")\ndata2 &lt;- data.frame(x = x2, y = dt(x2, df = 10), dist = \"Low Kurtosis\")\ndata &lt;- rbind(data1, data2) %&gt;%\n  mutate(dist = factor(dist, levels = c(\"High Kurtosis\", \"Low Kurtosis\")))\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_line(color = \"skyblue\") +\n  geom_area(fill = \"skyblue\", alpha = 0.4) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank()) +\n  facet_wrap(~dist, scales = \"free_x\") +\n  ylab(\"density\") +\n  xlab(\"\")"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#skewness-and-kurtosis-in-the-ants-data",
    "href": "lectures/L06/lecture-06.html#skewness-and-kurtosis-in-the-ants-data",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Skewness and kurtosis in the ants data",
    "text": "Skewness and kurtosis in the ants data\nWith experience we can “eyeball” the data, but we can also calculate the skewness and kurtosis.\n\n\nCode\nants %&gt;%\n  group_by(Tree) %&gt;%\n  summarise(skewness = skewness(Food), kurtosis = kurtosis(Food))\n\n\n# A tibble: 2 × 3\n  Tree     skewness kurtosis\n  &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 Rowan       1.04      3.15\n2 Sycamore    0.807     2.63\n\n\n\n\nFrom the results we can see that both sites have a positive skewness. Site Rowan has high kurtosis."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#workflow",
    "href": "lectures/L06/lecture-06.html#workflow",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Workflow",
    "text": "Workflow\n\nCheck the data for normality and homogeneity of variance (i.e. test assumptions).\nIf the assumptions are violated, consider transforming the data.\nRepeat checks on assumptions. If assumptions are met, proceed with the t-test on the transformed scale. Otherwise, use a different transformation or consider using a non-parametric test.\nInterpret the statistical results and back-transform the results to the original scale (optional but recommended) to aid interpretation."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#picking-a-transformation-1",
    "href": "lectures/L06/lecture-06.html#picking-a-transformation-1",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Picking a transformation",
    "text": "Picking a transformation\n\nFor positive skewness\n\nSquare root transformation: \\(\\sqrt{x}\\) for skewness between 0.5 and 1 and kurtosis &lt; 3.\nLogarithmic transformation: \\(\\log(x)\\) for skewness &gt; 1 and kurtosis &lt; 3.\nReciprocal transformation: \\(\\frac{1}{x}\\) for skewness &gt; 1 and kurtosis &gt; 3 (quite extreme).\n\n\n\nFor negative skewness\n\nThis is rare as most biological data are positively skewed. However, you can try the square \\(x^2\\) or cube \\(x^3\\) transformation.\nIf negatively skewed data contains zeros, consider using the log transform and adding a constant to the data before transformation e.g. \\(\\log(x + 1)\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\nThere is also the Box-Cox transformation which informs us of the best transformation to apply to the data without the need to check skewness and kurtosis. This method is not covered in this unit, but you can read more about it here (the simple R version) or here (more detailed mathematical explanation)."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#how-do-we-check-if-the-transformation-worked",
    "href": "lectures/L06/lecture-06.html#how-do-we-check-if-the-transformation-worked",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "How do we check if the transformation worked?",
    "text": "How do we check if the transformation worked?\nWe need to apply the transformation to the entire dataset and check the Q-Q plot again.\n\n\nCode\nants$Food_log &lt;- log(ants$Food)\n\n\n\n\nCode\n# compare the Q-Q plot before and after transformation\npfood &lt;- ggplot(ants, aes(sample = Food)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Tree) +\n  ggtitle(\"Before transformation\") +\n  theme_classic()\n\npfoodlog &lt;- ggplot(ants, aes(sample = Food_log)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Tree) +\n  ggtitle(\"After transformation\") +\n  theme_classic()\n\npfood / pfoodlog"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#checking-skewness-and-kurtosis-after-transformation",
    "href": "lectures/L06/lecture-06.html#checking-skewness-and-kurtosis-after-transformation",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Checking skewness and kurtosis after transformation",
    "text": "Checking skewness and kurtosis after transformation\n\n\nCode\nants %&gt;%\n  group_by(Tree) %&gt;%\n  summarise(skewness = skewness(Food_log), kurtosis = kurtosis(Food_log))\n\n\n# A tibble: 2 × 3\n  Tree      skewness kurtosis\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Rowan    0.271         1.77\n2 Sycamore 0.0000457     1.86"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#performing-the-t-test-1",
    "href": "lectures/L06/lecture-06.html#performing-the-t-test-1",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Performing the t-test",
    "text": "Performing the t-test\n\n\nCode\nfit &lt;- t.test(Food_log ~ Tree, data = ants, var.equal = TRUE)\nfit\n\n\n\n    Two Sample t-test\n\ndata:  Food_log by Tree\nt = -2.0521, df = 52, p-value = 0.04521\nalternative hypothesis: true difference in means between group Rowan and group Sycamore is not equal to 0\n95 percent confidence interval:\n -0.732858080 -0.008203447\nsample estimates:\n   mean in group Rowan mean in group Sycamore \n              2.287756               2.658287 \n\n\n\nHow do we interpret the results?\nEvidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045)."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#back-transforming-the-results",
    "href": "lectures/L06/lecture-06.html#back-transforming-the-results",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Back-transforming the results",
    "text": "Back-transforming the results\n\nFor power transformations, we can back-transform the results to the original scale using the inverse function.\nLog transformations are a bit tricky as the inverse function is the exponential function.\n\nFor the natural log transformation which is log() in R, the inverse function is the exponential function: \\(e^x\\).\nFor the base 10 log transformation which is log10() in R, the inverse function is \\(10^x\\)."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#interpretation",
    "href": "lectures/L06/lecture-06.html#interpretation",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Interpretation",
    "text": "Interpretation\nBack-transforming mean values\n\n\nCode\nbrowan &lt;- exp(fit$estimate[[1]]) # mean biomass from the Rowan site\nbsycamore &lt;- exp(fit$estimate[[2]]) # mean biomass from the Sycamore site\n\n# check the ratio\nbsycamore / browan\n\n\n[1] 1.448503\n\n\n\nEvidence suggests that the log-transformed mean biomass of food collected by ants from the Rowan site is significantly different from the log-transformed mean biomass of food collected by ants from the Sycamore site (t = -2.05, df = 52, p = 0.045).\n\nThe mean biomass of food collected by ants from the Sycamore site (14.3 mg) is 1.4 times greater than the mean biomass of food collected by ants from the Rowan site (9.9 mg).\nBack-transforming confidence intervals\n\n\nCode\nant_ci &lt;- exp(fit$conf.int)\nant_ci\n\n\n[1] 0.4805336 0.9918301\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "lectures/L06/lecture-06.html#comparing-to-a-test-without-transformation",
    "href": "lectures/L06/lecture-06.html#comparing-to-a-test-without-transformation",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "Comparing to a test without transformation",
    "text": "Comparing to a test without transformation\n\n\nCode\nfit2 &lt;- t.test(Food ~ Tree, data = ants, var.equal = TRUE)\nfit2\n\n\n\n    Two Sample t-test\n\ndata:  Food by Tree\nt = -1.9217, df = 52, p-value = 0.06013\nalternative hypothesis: true difference in means between group Rowan and group Sycamore is not equal to 0\n95 percent confidence interval:\n -10.4916030   0.2267678\nsample estimates:\n   mean in group Rowan mean in group Sycamore \n              12.27143               17.40385 \n\n\n\n\n\nOriginal mean values:\n\nRowan = 12.3 mg\nSycamore = 17.4 mg\n\nLog-transformed mean values:\n\nRowan = 2.3 lg(mg)\nSycamore = 2.7 lg(mg)\n\nBack-transformed mean values:\n\nRowan = 9.9 mg\nSycamore = 14.3 mg\n\n\nThe original mean values are based on the arithmetic mean, while the log-transformed mean values are based on the geometric mean. The geometric mean is more appropriate for skewed data.\n\n\nOriginal 95% confidence interval:\n\n-10.5 to 0.2 mg\n\nLog-transformed 95% confidence interval:\n\n0.5 to 1 lg(mg)\n\nBack-transformed 95% confidence interval:\n\n1.6 to 2.7 mg\n\n\nThe influence of kurtosis on the 95% confidence interval is evident when comparing the original and back-transformed confidence intervals, as the log transform reduces the effect of outliers on the data."
  },
  {
    "objectID": "lectures/L06/lecture-06.html#references",
    "href": "lectures/L06/lecture-06.html#references",
    "title": "Topic 6 – Two-sample t-tests",
    "section": "References",
    "text": "References\n\nQuinn G. P. & Keough M. J. (2002) Experimental design and data analysis for biologists. Cambridge University Press, Cambridge, UK.\nLogan, M. (2010). Biostatistical design and analysis using R a practical guide. Hoboken, N.J., Wiley-Blackwell."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#at-the-end-of-this-week-you-will-be-able-to",
    "href": "lectures/L07/lecture-07.html#at-the-end-of-this-week-you-will-be-able-to",
    "title": "Topic 7 – Non-parametric tests",
    "section": "At the end of this week, you will be able to:",
    "text": "At the end of this week, you will be able to:\n\nExplain the rationale for using non-parametric tests, including when to choose them over parametric tests due to violations of assumptions (e.g., non-normality, ordinal data).\nDifferentiate between:\n\nThe Wilcoxon signed-rank tests (for one-sample or paired data),\nThe Mann-Whitney U test (AKA Wilcoxon rank-sum test) for two independent samples, and\nThe Chi-squared test, including tests for proportions (goodness-of-fit) and association (independence).\n\nApply and interpret Wilcoxon and Mann-Whitney tests in R, for one-sample paired samples or two independent samples\nConduct and interpret Chi-squared tests in R, including:\n\nChi-squared test of independence to examine association between two categorical variables\nChi-squared goodness-of-fit test to compare observed proportions with expected proportions\nCheck assumptions for the Chi-squared test (e.g., expected cell counts) and understand potential limitations and alternatives (e.g., Fisher’s Exact Test).\n\nPresent and visualise results from all non-parametric tests using RStudio output, with appropriate interpretation and reporting."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#overview",
    "href": "lectures/L07/lecture-07.html#overview",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Overview",
    "text": "Overview\nParametric methods\nDepends on the assumption that the data is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) ,e.g. \\(t\\)-test, ANOVA, linear regression.\n\nNon-parametric methods\nDo not make any assumptions about the distribution of the data.\nUses other properties e.g. ranking of the data, e.g. Wilcoxon signed-rank test, Mann-Whitney U test, Kruskal-Wallis test."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#rank-based-tests",
    "href": "lectures/L07/lecture-07.html#rank-based-tests",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Rank-based tests",
    "text": "Rank-based tests\nGeneral idea\n\nRank the data e.g., from smallest to largest.\nReplace the data with their ranks.\nPerform the test on the ranks."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#its-kind-of-like-a-transformation",
    "href": "lectures/L07/lecture-07.html#its-kind-of-like-a-transformation",
    "title": "Topic 7 – Non-parametric tests",
    "section": "It’s kind of like a transformation…",
    "text": "It’s kind of like a transformation…\n\nFor the Wilcoxon signed-rank test suppose we have the following data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample:\n12\n10\n8\n6\n4\n10\n8\n6\n10\n\n\n\n\n\n\n\nWe arrange the data in ascending order (similar values are given the same colour for illustration):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nordered:\n4\n6\n6\n8\n8\n10\n10\n10\n12\n\n\n\n\n\n\n\nThen, we rank the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nordered ranks:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\n\n\nFinally, ranks that are tied are given the average rank:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfinal rank:\n1\n2.5\n2.5\n4.5\n4.5\n7\n7\n7\n9\n\n\n\n\n\nThese ranks are then used to perform the test, instead of the original data."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#two-sample-t-test",
    "href": "lectures/L07/lecture-07.html#two-sample-t-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Two-sample \\(t\\)-test",
    "text": "Two-sample \\(t\\)-test\nConsider two sets of identical data that compares between a group A and B, where one contains an outlier.\n\n\nData:\n\n\nCode\nlibrary(tidyverse)\n\n# for display only\ndata.frame(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n           B = c(7, 8, 9, 10, 11, 12, 13, 14, 15, 16)) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\n7\n\n\n2\n8\n\n\n3\n9\n\n\n4\n10\n\n\n5\n11\n\n\n6\n12\n\n\n7\n13\n\n\n8\n14\n\n\n9\n15\n\n\n10\n16\n\n\n\n\n\n\n\n\nData with outlier:\n\n\nCode\ndata.frame(A = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n           B = c(7, 8, 9, 10, 11, 12, 13, 14, 15, 200)) %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\n7\n\n\n2\n8\n\n\n3\n9\n\n\n4\n10\n\n\n5\n11\n\n\n6\n12\n\n\n7\n13\n\n\n8\n14\n\n\n9\n15\n\n\n10\n200"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#should-there-be-a-difference",
    "href": "lectures/L07/lecture-07.html#should-there-be-a-difference",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Should there be a difference?",
    "text": "Should there be a difference?\nWithout the outlier, the data would have been normally distributed.\n\n\nCode\ndf &lt;- data.frame(\n  group = rep(c(\"A\", \"B\"), each = 10),\n  response = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n            7, 8, 9, 10, 11, 12, 13, 14, 15, 16))\n\ndf_with_outlier &lt;- data.frame(\n  group = rep(c(\"A\", \"B\"), each = 10),\n  response = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n            7, 8, 9, 10, 11, 12, 13, 14, 15, 200))\n\nggplot(df, aes(x = response, y = group)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#outlier",
    "href": "lectures/L07/lecture-07.html#outlier",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Outlier",
    "text": "Outlier\nThe same data, but with a single outlier in group B:\n\n\nCode\nggplot(df_with_outlier, aes(x = response, y = group)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#analysis",
    "href": "lectures/L07/lecture-07.html#analysis",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Analysis",
    "text": "Analysis\nIf we perform \\(t\\)-tests on both data sets, we get the following results:\n\n\n\n\nCode\nfit &lt;- t.test(response ~ group, data = df)\nfit\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by group\nt = -4.4313, df = 18, p-value = 0.0003224\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -8.844662 -3.155338\nsample estimates:\nmean in group A mean in group B \n            5.5            11.5 \n\n\nResults indicate that there is a statistically significant difference between the two groups (t18 = -4.4, p &lt; 0.05).\n\n\n\nCode\nfit2 &lt;- t.test(response ~ group, data = df_with_outlier)\nfit2\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by group\nt = -1.2882, df = 9.0461, p-value = 0.2297\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -67.21615  18.41615\nsample estimates:\nmean in group A mean in group B \n            5.5            29.9 \n\n\nResults indicate that the two groups are not significantly different (t18 = -2.1, p = 0.23).\n\n\nThe real difference between the two groups is obscured by the outlier. Type II error (false negative)?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#when-to-use",
    "href": "lectures/L07/lecture-07.html#when-to-use",
    "title": "Topic 7 – Non-parametric tests",
    "section": "When to use",
    "text": "When to use\n\nIf assumptions are met (normality, homogeneity of variance), use parametric tests as they are more powerful and efficient than non-parametric tests.\nIf the normality assumption is violated, transform the data and check for normality again (optional).\nNon-parametric tests are a good way to deal with circumstances in which parametric tests perform “poorly”."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#what-to-use",
    "href": "lectures/L07/lecture-07.html#what-to-use",
    "title": "Topic 7 – Non-parametric tests",
    "section": "What to use",
    "text": "What to use\n\n\nCode\nlibrary(tidyverse)\nlibrary(gt)\n\ndf &lt;- tibble(\n  parametric = c(\"One-sample t-test\", \"Two-sample t-test\", \"ANOVA\", \"Pearson's correlation\"),\n  non_parametric = c(\"Wilcoxon signed-rank test\", \"Mann-Whitney U test\", \"Kruskal-Wallis test\", \"Spearman's rank correlation\"))\n\ngt(df) %&gt;%\n  cols_label(parametric = \"Parametric tests\", non_parametric = \"Non-parametric counterpart\") %&gt;%\n  tab_options(\n    table.font.size = px(24),\n    column_labels.font.weight = \"bold\")\n\n\n\n\n\n\n\n\nParametric tests\nNon-parametric counterpart\n\n\n\n\nOne-sample t-test\nWilcoxon signed-rank test\n\n\nTwo-sample t-test\nMann-Whitney U test\n\n\nANOVA\nKruskal-Wallis test\n\n\nPearson's correlation\nSpearman's rank correlation\n\n\n\n\n\n\n\nAll of the non-parametric techniques above convert the data into ranks before performing the test.\n\n\n\n\n\n\n\nNote\n\n\nWe will focus on the Wilcoxon signed-rank test and the Mann-Whitney U test."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#overview-1",
    "href": "lectures/L07/lecture-07.html#overview-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Overview",
    "text": "Overview\nThe Wilcoxon signed-rank test is a non-parametric test used to compare two related samples, matched pairs, or repeated measures on a single sample.\n\nIs an alternative to:\n\nOne-sample \\(t\\)-test\nPaired \\(t\\)-test"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#assumptions",
    "href": "lectures/L07/lecture-07.html#assumptions",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nData comes from the same population\nData are randomly and independently sampled\n\n\n\nBasically, used in same situations as the one-sample or paired \\(t\\)-test, but when the data is not normally distributed but still symmetric."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#calculating-ranks",
    "href": "lectures/L07/lecture-07.html#calculating-ranks",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Calculating ranks",
    "text": "Calculating ranks\nIf comparing two groups, the ranks are calculated as follows:\n\nCalculate the difference \\(D\\) between the two groups.\nRank the absolute values of the differences in ascending order.\nAssign the sign of the difference to the rank.\nSum the ranks for each group – zero differences are ignored.\n\n\n\n\n\n\n\n\nNote\n\n\nSee Slide 5 to recall how ranks are calculated, but we will show another example in the next slide."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#weight-gain",
    "href": "lectures/L07/lecture-07.html#weight-gain",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Weight gain",
    "text": "Weight gain\nWe measured weight gain in chickens before and after a diet.\n\n\nCode\nweight &lt;- c(2.5, 3.5, 3.5, 3.4)\nweight_after &lt;- c(4, 5, 5, 4.6)\n\ndf &lt;- tibble(\n  chicken = 1:4,\n  weight = weight,\n  weight_after = weight_after)\n\ngt(df) %&gt;%\n  tab_options(\n    table.font.size = px(24),\n    column_labels.font.weight = \"bold\")\n\n\n\n\n\n\n\n\nchicken\nweight\nweight_after\n\n\n\n\n1\n2.5\n4.0\n\n\n2\n3.5\n5.0\n\n\n3\n3.5\n5.0\n\n\n4\n3.4\n4.6\n\n\n\n\n\n\n\nIs there a significant increase in weight gain after the diet?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#rank-values",
    "href": "lectures/L07/lecture-07.html#rank-values",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Rank values",
    "text": "Rank values\n\n\nCode\ndf %&gt;%\n  mutate(D = weight_after - weight) %&gt;%\n  mutate(Sign = ifelse(D &gt; 0, \"+\", \"-\")) %&gt;%\n  mutate(rank = rank(abs(D))) %&gt;%\n  mutate(\"Signed rank\" = ifelse(D &gt; 0, rank, -rank)) %&gt;%\n  gt() %&gt;%\n  tab_options(\n    table.font.size = px(24),\n    column_labels.font.weight = \"bold\")\n\n\n\n\n\n\n\n\nchicken\nweight\nweight_after\nD\nSign\nrank\nSigned rank\n\n\n\n\n1\n2.5\n4.0\n1.5\n+\n3\n3\n\n\n2\n3.5\n5.0\n1.5\n+\n3\n3\n\n\n3\n3.5\n5.0\n1.5\n+\n3\n3\n\n\n4\n3.4\n4.6\n1.2\n+\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe order of the ranks is based on the absolute values of the differences; the signs are assigned afterward."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#hypothesis",
    "href": "lectures/L07/lecture-07.html#hypothesis",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nIs there a significant increase in weight gain after the diet?\n\n\\[H_0: \\mu_{\\text{before}} = \\mu_{\\text{after}}\\] \\[H_1: \\mu_{\\text{before}} &lt; \\mu_{\\text{after}}\\]\n\nIn words:\n\n\\(H_0\\): There is no difference in weight gain before and after the diet.\n\\(H_1\\): There is an increase in weight gain after the diet."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#assumptions-1",
    "href": "lectures/L07/lecture-07.html#assumptions-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nCode\np1 &lt;- ggplot(df, aes(x = weight_after - weight)) +\n  geom_histogram(bins = 10)\np2 &lt;- ggplot(df, aes(sample = weight_after - weight)) +\n  stat_qq() +\n  stat_qq_line()\np3 &lt;- ggplot(df, aes(x = \"\", y = weight_after - weight)) +\n  geom_boxplot()\n\nlibrary(patchwork)\np1 + p2 + p3\n\n\n\nWith so few data points, we may want to use a formal test to check for normality."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#assumptions-2",
    "href": "lectures/L07/lecture-07.html#assumptions-2",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\nshapiro.test(df$weight_after - df$weight)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$weight_after - df$weight\nW = 0.62978, p-value = 0.001241\n\n\nResults indicate that the data significantly deviates from normality (W = 0.63, p &lt; 0.05). We will use the Wilcoxon signed-rank test."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#performing-the-test",
    "href": "lectures/L07/lecture-07.html#performing-the-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Performing the test",
    "text": "Performing the test\nIn R\n\nwilcox.test(x = df$weight_after, y = df$weight,  # data must be x - y\n  alternative = \"greater\",  # because we are testing for an increase\n  paired = TRUE)            # because the data is paired\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  df$weight_after and df$weight\nV = 10, p-value = 0.04449\nalternative hypothesis: true location shift is greater than 0\n\n\nwhere V is the sum of the signed ranks.\n\nThe results indicate that there is a significant increase in weight gain after the diet (V = 10, p &lt; 0.05)."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#beetle-consumption-in-lizards",
    "href": "lectures/L07/lecture-07.html#beetle-consumption-in-lizards",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Beetle consumption in lizards",
    "text": "Beetle consumption in lizards\nResearchers investigated differences in beetle consumption between two size classes of eastern horned lizard (Phrynosoma douglassi brevirostre)\n\nLarger class: adult females.\nSmaller class: adult males, yearling females.\n\n\nFocusing on just the smaller size class (for now) – it was hypothesised that this size class would eat a minimum of 100 beetles per day."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#hypothesis-1",
    "href": "lectures/L07/lecture-07.html#hypothesis-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypothesis",
    "text": "Hypothesis\nDoes the average smaller size class lizard eat about 100 beetles per day?\n\\[H_0: \\mu = 100\\] \\[H_1: \\mu \\neq 100\\]\nDataset Download\n\n\nCode\nbeetle &lt;- read_csv(\"data/beetle.csv\")\nglimpse(beetle)\n\n\nRows: 45\nColumns: 2\n$ SIZE    &lt;chr&gt; \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\",…\n$ BEETLES &lt;dbl&gt; 256, 209, 0, 0, 0, 44, 49, 117, 6, 0, 0, 75, 34, 13, 0, 90, 0,…"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#first-check-assumptions",
    "href": "lectures/L07/lecture-07.html#first-check-assumptions",
    "title": "Topic 7 – Non-parametric tests",
    "section": "First, check assumptions",
    "text": "First, check assumptions\n\n\nCode\np1 &lt;- ggplot(beetle, aes(x = BEETLES)) +\n  geom_histogram(bins = 10)\np2 &lt;- ggplot(beetle, aes(sample = BEETLES)) +\n  stat_qq() +\n  stat_qq_line()\np3 &lt;- ggplot(beetle, aes(x = \"\", y = BEETLES)) +\n  geom_boxplot()\n\nlibrary(patchwork)\np1 + p2 + p3\n\n\n\nIs it normally distributed?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#run-the-test",
    "href": "lectures/L07/lecture-07.html#run-the-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Run the test",
    "text": "Run the test\nThe Wilcoxon signed-rank test for one sample can be performed as follows:\n\nbeetle %&gt;%\n  filter(SIZE == \"small\") %&gt;%  # filter only the smaller size class\n  pull(BEETLES) %&gt;%            # convert to a vector using pull()\n  wilcox.test(mu = 100)        # wilcox.text(x, mu)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  .\nV = 92, p-value = 0.09755\nalternative hypothesis: true location is not equal to 100\n\n\n\nResults indicate that the average number of beetles consumed by the smaller size class lizard is not significantly different from 100 (V = 92, p = 0.1).\n\n\n\n\n\n\n\n\nImportant\n\n\nWe are unable to make a conclusion about effect size from non-parametric tests as the information is lost when the data is transformed into ranks."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#about",
    "href": "lectures/L07/lecture-07.html#about",
    "title": "Topic 7 – Non-parametric tests",
    "section": "About",
    "text": "About\n\nA non-parametric test used to compare two independent samples similar to the two-sample \\(t\\)-test.\nLike the Wilcoxon signed-rank test, it uses ranks to perform the test and does not assume normality.\nIt is also more relaxed in that it does not assume symmetry in the distribution of the data – instead, it assumes that the two groups have the same shape/distribution."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#beetle-consumption-in-lizards-1",
    "href": "lectures/L07/lecture-07.html#beetle-consumption-in-lizards-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Beetle consumption in lizards",
    "text": "Beetle consumption in lizards\nResearchers investigated differences in beetle consumption between two size classes of eastern horned lizard (Phrynosoma douglassi brevirostre)\n\nLarger class: adult females.\nSmaller class: adult males, yearling females.\n\nWe will now compare the number of beetles consumed by the larger and smaller size classes of lizards."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#hypotheses",
    "href": "lectures/L07/lecture-07.html#hypotheses",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nAre the number of beetles consumed by the larger and smaller size classes of lizards different?\n\nLoosely speaking, because we are not assuming symmetry, the most appropriate summary statistic to use when comparing the two groups is the median.\n\\[H_0: median_{\\text{larger}} = median_{\\text{smaller}}\\] \\[H_1: median_{\\text{larger}} \\neq median_{\\text{smaller}}\\]\nMore accurately, we are testing for a difference in the distribution of the two groups."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#assumptions-3",
    "href": "lectures/L07/lecture-07.html#assumptions-3",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nCode\np1 &lt;- ggplot(beetle, aes(x = BEETLES)) +\n  geom_histogram(bins = 14, position = \"dodge\") +\n  facet_wrap(~SIZE, ncol = 1)\np2 &lt;- ggplot(beetle, aes(x = SIZE, y = BEETLES)) +\n  geom_boxplot()\np3 &lt;- ggplot(beetle, aes(sample = BEETLES)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~SIZE, ncol = 1)\n\nlibrary(patchwork)\np1 + p2 + p3\n\n\n\nData does not meet the normality assumption."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#test-statistic",
    "href": "lectures/L07/lecture-07.html#test-statistic",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Test statistic",
    "text": "Test statistic\nThe same function wilcox.test() can be used to perform the Mann-Whitney U test.\n\nwilcox.test(BEETLES ~ SIZE, data = beetle)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  BEETLES by SIZE\nW = 329, p-value = 0.07494\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nW is the sum of the ranks of the smaller group.\nThe “true location shift” is the median of the larger group minus the median of the smaller group.\n\n\nThe results indicate that the number of beetles consumed by the larger and smaller size classes of lizards is not significantly different (W = 329, p = 0.07)."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#transform-or-non-parametric",
    "href": "lectures/L07/lecture-07.html#transform-or-non-parametric",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Transform, or non-parametric?",
    "text": "Transform, or non-parametric?\n\nAs usual, there is ongoing debate on whether to transform the data or use non-parametric tests, but the general consensus is to always prefer parametric tests and transformations when assumptions are met using those techniques.\n\ne.g. Parametric analysis of transformed data is more powerful than non-parametric analysis\n\nSome argue that non-parametric tests must be decided during experimental design and not when the data fails to meet the normality assumption: as the decision to rank data has implications on the interpretation of the results.\n\ne.g. Graphpad Advice: Don’t automate the decision to use a nonparametric test\n\nThe conventional wisdom is to transform the data and check for normality if the assumption is not met. If the data is still not normal, then use non-parametric tests (after considering the implications on interpretation).\n\nOr, use bootstrapping (next week!)."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#summary",
    "href": "lectures/L07/lecture-07.html#summary",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Summary",
    "text": "Summary\n\nWilcoxon signed-rank test: alternative to the one-sample \\(t\\)-test and paired \\(t\\)-test.\nMann-Whitney U test: alternative to the two-sample \\(t\\)-test.\nAdvantages: Robust to outliers, skewness, and non-normality.\nDrawbacks: Less powerful than parametric tests when assumptions are met, provide no insight into the size of the effect."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#parametric-and-non-parametric-alternatives",
    "href": "lectures/L07/lecture-07.html#parametric-and-non-parametric-alternatives",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Parametric and non-parametric alternatives",
    "text": "Parametric and non-parametric alternatives\n\nSo far, all of our techniques have been aimed at comparing means/medians of continuous variables.\nThe assumption of normality underpins these techniques – if the data is not normally distributed, we have alternatives like transforming the data or using non-parametric tests.\nDoes this apply to all data?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#a-rational-assumption",
    "href": "lectures/L07/lecture-07.html#a-rational-assumption",
    "title": "Topic 7 – Non-parametric tests",
    "section": "A rational assumption?",
    "text": "A rational assumption?\n\nAre all randomly sampled data normally distributed?\nRecall probability distributions (Week 3) – normal distribution is just one of several possible distributions of data.\nIt turns out that there are non-parametric techniques that are not just alternatives of parametric tests, but better suited for certain types of data."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#what-are-categorical-variables",
    "href": "lectures/L07/lecture-07.html#what-are-categorical-variables",
    "title": "Topic 7 – Non-parametric tests",
    "section": "What are categorical variables?",
    "text": "What are categorical variables?\nConsider the following questions:\n\n\nA biologist claims that when sampling the Australian Botanical Gardens for butterflies, the ratio of the most dominant colours (red, blue, green, and yellow) is equal. How would you determine if the biologist’s claim is true?\n\n\nA study was conducted on a population of deer to see if there is a relationship between their age group (young, adult, old) and their preferred type of vegetation (grass, leaves, bark). Is age group of the deer independent of their vegetation preference?\n\n\n\nHow would you measure these variables, and what sort of summary statistics can you use?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#visualising-categorical-variables",
    "href": "lectures/L07/lecture-07.html#visualising-categorical-variables",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Visualising categorical variables",
    "text": "Visualising categorical variables\n\n\nCode\nlibrary(ggplot2)\n# generate some butterfly data\nbutterfly_data &lt;- data.frame(\n  color = c(\"red\", \"blue\", \"green\", \"yellow\"),\n  count = c(48, 62, 56, 34)\n)\n\n# plot the data\nggplot(butterfly_data, aes(x = color, y = count)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Butterfly colour distribution\")\n\n\n\nWe can only count the number of times a particular category occurs, or the proportion of the total that each category represents."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#types-of-categorical-data",
    "href": "lectures/L07/lecture-07.html#types-of-categorical-data",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Types of categorical data",
    "text": "Types of categorical data\n\nRather than measuring a continuous variable, we are interested in counting the number of times a particular category occurs, or the proportion of the total that each category represents.\nThese are known as categorical variables.\nGenerally 3 types of categorical data:\n\nNominal: Categories have no inherent order (e.g. colours, breeds of dogs).\nOrdinal: Categories have an inherent order (e.g. Likert scales, grades).\nBinary: Only two mutually exclusive categories (e.g rain or no rain)."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#the-chi-squared-test",
    "href": "lectures/L07/lecture-07.html#the-chi-squared-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "The chi-squared test",
    "text": "The chi-squared test\n\nThe chi-squared test is perhaps one of the most prominent examples of non-parametric tests.\nDeveloped by Karl Pearson in 1900, pronounced “ki” as in “kite”, uses the Greek letter \\(\\chi\\).\nActually derived from the normal distribution: a chi-squared distribution is the sum of squared standard normal deviates – essentially a folded-over and stretched out normal."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#chi-squared-distribution-vs-normal-distribution",
    "href": "lectures/L07/lecture-07.html#chi-squared-distribution-vs-normal-distribution",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Chi-squared distribution vs normal distribution",
    "text": "Chi-squared distribution vs normal distribution\n\n\nCode\ncurve(dchisq(x, df = 1), from = 0, to = 9, xlim = c(-10, 10), col = \"blue\", lwd = 2, ylab = \"Density\", xlab = \"Value\", main = \"Chi-squared vs. normal\")\ncurve(dnorm(x, mean = 0, sd = 1), from = -9, to = 9, col = \"red\", lwd = 2, add = TRUE)\nabline(v = 0, col = \"black\", lty = 2)\nlegend(\"topright\", legend = c(\"Chi-squared with 1 df\", \"Normal\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\nHow is the chi-squared distribution used in hypothesis testing?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#butterflies-data",
    "href": "lectures/L07/lecture-07.html#butterflies-data",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Butterflies data",
    "text": "Butterflies data\n\nA biologist claims that when sampling the Australian Botanical Gardens for butterflies, the ratio of the most dominant colours (red, blue, green, and yellow) is equal. How would you determine if the biologist’s claim is true?\n\nSuppose we have the following data on the colours of butterflies after randomly sampling 200 of them:\n\n\nCode\nbutterfly_data %&gt;%\n  knitr::kable()\n\n\n\n\n\ncolor\ncount\n\n\n\n\nred\n48\n\n\nblue\n62\n\n\ngreen\n56\n\n\nyellow\n34"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#testing-the-claim",
    "href": "lectures/L07/lecture-07.html#testing-the-claim",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Testing the claim",
    "text": "Testing the claim\n\nIf the biologist’s claim is true, we would expect the number of butterflies of each colour to be equal.\nIf 200 butterflies were sampled, we would expect 50 of each colour, as the expected frequency of each colour is 200 \\(\\times\\) 0.25 = 50.\n\nTherefore:\n\n\nCode\ndf &lt;- butterfly_data %&gt;%\n  mutate(expected = 200 * 0.25)\nknitr::kable(df)\n\n\n\n\n\ncolor\ncount\nexpected\n\n\n\n\nred\n48\n50\n\n\nblue\n62\n50\n\n\ngreen\n56\n50\n\n\nyellow\n34\n50"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#test-statistic-1",
    "href": "lectures/L07/lecture-07.html#test-statistic-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Test statistic",
    "text": "Test statistic\nThe test statistic for the chi-squared test is calculated as:\n\\[ \\chi^2 = \\sum \\frac{(O - E)^2}{E} \\]\nwhere \\(O\\) is the observed frequency and \\(E\\) is the expected frequency.\n\nSo for the butterfly data:\n\nchi_squared &lt;- sum((df$count - df$expected)^2 / df$expected)\nchi_squared\n\n[1] 8.8\n\n\nThis is the test statistic for one sample. How do we interpret this value?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#simulate-the-null-distribution",
    "href": "lectures/L07/lecture-07.html#simulate-the-null-distribution",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Simulate the null distribution",
    "text": "Simulate the null distribution\nUnder the null hypothesis, the observed frequencies are equal to the expected frequencies i.e. the biologist’s claim is true.\nSuppose we repeat the sampling process many times, assuming the null hypothesis is true, each time calculating the test statistic. What would the distribution of test statistics look like?\n\n\nCode\n# simulate sampling of 200 butterflies 1000 times under the null hypothesis and plot the distribution of chi-squared values\nset.seed(123)\ncols &lt;- c(\"red\", \"blue\", \"green\", \"yellow\")\nB &lt;- 3000\ntest_statistic &lt;- vector(mode = \"numeric\", length = B)\nfor (i in 1:B) {\n  sim &lt;- sample(\n    x = cols, size = 200, replace = TRUE,\n    prob = c(0.25, 0.25, 0.25, 0.25)\n  )\n  sim_y &lt;- table(sim)\n  test_statistic[i] &lt;- sum((sim_y - 50)^2 / 50)\n}\n\np1 &lt;-\n  ggplot(data.frame(test_statistic), aes(x = test_statistic)) +\n  geom_histogram(aes(y = ..density..), bins = 20, fill = \"blue\", colour = \"black\", alpha = 0.3) +\n  ylim(0, 0.3) +\n  labs(title = \"Simulated distribution of chi-squared values under the null hypothesis\") +\n  xlab(\"Chi-squared value\") +\n  ylab(\"Frequency\")\np1"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#what-does-our-test-statistic-tell-us",
    "href": "lectures/L07/lecture-07.html#what-does-our-test-statistic-tell-us",
    "title": "Topic 7 – Non-parametric tests",
    "section": "What does our test statistic tell us?",
    "text": "What does our test statistic tell us?\n\n\n\nCode\np1 +\n  geom_vline(xintercept = chi_squared, color = \"red\", size = 1) +\n  # label the line\n  annotate(\"text\", x = chi_squared + 1.5, y = 0.25, label = \"Observed value\", color = \"red\", size = 5)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean(test_statistic &gt;= chi_squared)\n\n\n[1] 0.034\n\n\nComparing our test statistic to the simulated distribution, we can see that the 0.03% of the simulated values are greater than our test statistic. What does this tell us?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#a-chi2-test",
    "href": "lectures/L07/lecture-07.html#a-chi2-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "A \\(\\chi^2\\) test",
    "text": "A \\(\\chi^2\\) test\nA chi-squared distribution allows us to perform the same hypothesis test without the need for simulation.\n\n\n\nCode\nggplot(data.frame(test_statistic), aes(x = test_statistic)) +\n  geom_histogram(aes(y = ..density..), bins = 20, fill = \"blue\", colour = \"black\", alpha = 0.3) +\n  ylim(0, 0.3) +\n  stat_function(fun = dchisq, args = list(df = 3), color = \"red\", size = 1) +\n  labs(title = \"Simulated distribution of chi-squared values under the null hypothesis\") +\n  xlab(\"Chi-squared value\") +\n  ylab(\"Frequency\")"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#conclusion",
    "href": "lectures/L07/lecture-07.html#conclusion",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Conclusion?",
    "text": "Conclusion?\nThe results of the simulation suggest that the observed frequencies of butterfly colours are significantly different from the expected frequencies, and we can reject the biologist’s claim."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#more-on-the-chi-squared-distribution",
    "href": "lectures/L07/lecture-07.html#more-on-the-chi-squared-distribution",
    "title": "Topic 7 – Non-parametric tests",
    "section": "More on the chi-squared distribution",
    "text": "More on the chi-squared distribution\n\nThe chi-squared distribution is non-symmetric and right-skewed.\nThe shape of the distribution is determined by the degrees of freedom, calculated as the number of categories minus 1.\nAs the degrees of freedom increase, the chi-squared distribution approaches a normal distribution due to the central limit theorem."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#definitions",
    "href": "lectures/L07/lecture-07.html#definitions",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Definitions",
    "text": "Definitions\n\nChi-squared distribution: a distribution derived from the normal distribution that allows us to determine whether the observed frequencies of a categorical variable differ from the expected frequencies.\nContingency table: a table that displays the frequency of observations for two or more categorical variables.\nExpected frequency: the frequency that we would expect to observe if the null hypothesis is true.\nObserved frequency: the frequency that we actually observe.\nTest statistic: a measure of how much the observed frequencies differ from the expected frequencies, standardised by the expected frequencies."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#types-of-chi-squared-tests",
    "href": "lectures/L07/lecture-07.html#types-of-chi-squared-tests",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Types of chi-squared tests",
    "text": "Types of chi-squared tests\n\nGoodness-of-fit test: used to determine whether the observed frequencies of a categorical variable differ from the expected frequencies.\nTest of independence: used to determine whether there is a relationship between two or more categorical variables.\nTest of homogeneity: used to determine whether the distribution of a categorical variable is the same across different groups."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#assumptions-4",
    "href": "lectures/L07/lecture-07.html#assumptions-4",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe chi-squared test is a non-parametric test, so it does not rely on the assumption of normality. However, it does have some assumptions:\n\nIndependence: the observations are independent.\nSample size: the expected frequency of each category is at least 1, and no more than 20% of the expected frequencies are less than 5.\n\n\nThe sample size assumption ensures that the chi-squared distribution is a good approximation of the normal distribution."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#hypothesis-2",
    "href": "lectures/L07/lecture-07.html#hypothesis-2",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nNull hypothesis: the observed proportion of butterfly colours are equal to the expected proportions of 0.25 each.\nAlternative hypothesis: the observed proportions are not equal.\n\n\\[ H_0: p_1 = p_2 = p_3 = p_4 = 0.25 \\] \\[ H_1: \\text{at least one } p_i \\neq 0.25 \\]"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#test-statistic-and-check-assumptions-in-r",
    "href": "lectures/L07/lecture-07.html#test-statistic-and-check-assumptions-in-r",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Test statistic and check assumptions (in R)",
    "text": "Test statistic and check assumptions (in R)\n\n# chi-squared test for goodness of fit\nfit &lt;- chisq.test(df$count, p = rep(0.25, 4))\n\nAssumptions\nBy performing the chi-squared test, we can check the assumptions of the test by looking at the calculated frequences in the output:\n\n\nCode\nfit$observed\n\n\n[1] 48 62 56 34\n\n\nTest statistic\n\n\nCode\nfit\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  df$count\nX-squared = 8.8, df = 3, p-value = 0.03207"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#conclusion-1",
    "href": "lectures/L07/lecture-07.html#conclusion-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Conclusion",
    "text": "Conclusion\nThe results of the chi-squared test suggest that the observed frequencies of butterfly colours are significantly different from the expected frequencies (\\(\\chi^2 = 8.8\\), \\(df = 3\\), \\(p &lt; 0.001\\)). We can reject the null hypothesis and conclude that the biologist’s claim is not true.\n\n\n\n\n\n\nNote\n\n\nIf you’re interested, compare this result to the simulation we performed earlier."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#hypothesis-3",
    "href": "lectures/L07/lecture-07.html#hypothesis-3",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nNull hypothesis: the age group of the deer is independent of their vegetation preference.\nAlternative hypothesis: the age group of the deer is not independent of their vegetation preference.\n\n\\[ H_0: \\text{Age group is independent of vegetation preference} \\]\n\nNo relationship between the two variables\n\n\\[ H_1: \\text{Age group is not independent of vegetation preference} \\]\n\nThere is a relationship between the two variables"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#data",
    "href": "lectures/L07/lecture-07.html#data",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Data",
    "text": "Data\nSuppose we have the following data on the age group and vegetation preference of 100 deer:\n\n\nCode\n# create contingency table as matrix with labels\ndeer_data &lt;- matrix(c(20, 30, 10, 10, 10, 20, 10, 10, 10), nrow = 3, byrow = TRUE)\nrownames(deer_data) &lt;- c(\"young\", \"adult\", \"old\")\ncolnames(deer_data) &lt;- c(\"grass\", \"leaves\", \"bark\")\ndeer_data\n\n\n      grass leaves bark\nyoung    20     30   10\nadult    10     10   20\nold      10     10   10"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#test-statistic-and-check-assumptions-in-r-1",
    "href": "lectures/L07/lecture-07.html#test-statistic-and-check-assumptions-in-r-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Test statistic and check assumptions (in R)",
    "text": "Test statistic and check assumptions (in R)\nAssumptions are met as we can see the contingency table in the previous slide.\nTest statistic\n\n\n# chi-squared test for independence\nfit &lt;- chisq.test(deer_data) # exclude the age group column\nfit\n\n\n    Pearson's Chi-squared test\n\ndata:  deer_data\nX-squared = 13.542, df = 4, p-value = 0.008911\n\n\nWe reject the null hypothesis since the p-value is less than 0.05."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#conclusion-2",
    "href": "lectures/L07/lecture-07.html#conclusion-2",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Conclusion",
    "text": "Conclusion\nThe results of the chi-squared test suggest that the age group of the deer is not independent of their vegetation preference (\\(\\chi^2 = 12.4\\), \\(df = 4\\), \\(p &lt; 0.001\\)). We can reject the null hypothesis and conclude that there is a relationship between the age group of the deer and their vegetation preference."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#mosaic-plots",
    "href": "lectures/L07/lecture-07.html#mosaic-plots",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Mosaic plots",
    "text": "Mosaic plots\n\n\nCode\nmosaicplot(deer_data,\n  shade = TRUE,\n  main = \"Deer age group and vegetation preference\"\n)"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#interpretation",
    "href": "lectures/L07/lecture-07.html#interpretation",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe area of each rectangle is proportional to the number of observations in that category.\nThe shading of each rectangle indicates the expected frequency of observations in that category.\nThe darker the shading, the greater the difference between the observed and expected frequencies.\nDotted lines indicate independence between the two variables.\nSolid lines indicate dependence between the two variables."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#association-plots",
    "href": "lectures/L07/lecture-07.html#association-plots",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Association plots",
    "text": "Association plots\n\n\nCode\nlibrary(vcd)\nassoc(deer_data, shade = TRUE, main = \"Deer age group and vegetation preference\")"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#interpretation-1",
    "href": "lectures/L07/lecture-07.html#interpretation-1",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Interpretation",
    "text": "Interpretation\n\nSize of cells indicate the number of observations in that category.\nThe shadings are made based on the residuals of the chi-squared test (see legend), highlighting which cells contribute most to the chi-squared statistic.\nColour of the shadings indicate whether they are more or less frequent than expected (again, see legend)."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#test-of-homogeneity-vs.-test-of-independence",
    "href": "lectures/L07/lecture-07.html#test-of-homogeneity-vs.-test-of-independence",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Test of homogeneity vs. test of independence",
    "text": "Test of homogeneity vs. test of independence\n\nThe test of homogeneity is similar to the test of independence, but is used when we have two or more groups and we want to determine whether the distribution of a categorical variable is the same across different groups.\nIn general, this means that the null hypothesis is stated differently, and the test statistic is calculated in a slightly different way with different degrees of freedom.\nHomogeneity\n\n\\(H_0\\): The distribution of the categorical variable is the same across different groups.\n\\(H_1\\): The distribution of the categorical variable is not the same across different groups.\n\nIndependence\n\n\\(H_0\\): The variables of interest are independent.\n\\(H_1\\): The variables of interest are not independent."
  },
  {
    "objectID": "lectures/L07/lecture-07.html#differences-are-subtle",
    "href": "lectures/L07/lecture-07.html#differences-are-subtle",
    "title": "Topic 7 – Non-parametric tests",
    "section": "Differences are subtle",
    "text": "Differences are subtle\n\nIn the test of independence, observational units are collected at random from a single population and two (or more) categorical variables are observed for each unit.\nFor the deer example, the experimental design would involve randomly sampling deer and recording their age group and vegetation preference.\n\n\nIs age group independent of vegetation preference?\n\n\nIn the test of homogeneity, the data are collected by randomly sampling from two or more subgroups, and the same categorical variable is observed for each unit.\nFor the deer example, the experimental design would have to be modified to sample the vegetation preference of deer from young, adult, and old populations.\n\n\nIs the distribution of vegetation preference the same if we compare young, adult, and old deer?"
  },
  {
    "objectID": "lectures/L07/lecture-07.html#when-to-use-a-chi-squared-test",
    "href": "lectures/L07/lecture-07.html#when-to-use-a-chi-squared-test",
    "title": "Topic 7 – Non-parametric tests",
    "section": "When to use a chi-squared test?",
    "text": "When to use a chi-squared test?\n\nThe chi-squared test is not an “alternative” to a parametric test, but is better suited for certain types of data and requires delibarate experimental design that collects data in a certain way.\nIf we have categorical data and we want to determine whether the observed frequencies differ from the expected frequencies, then we can use a chi-squared test."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#announcement",
    "href": "lectures/L08/lecture-08.html#announcement",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Announcement",
    "text": "Announcement\n\nPractice Skills assessment this week, run in your respective labs. You must attend your lab to complete the assessment.\nA reminder that you are allowed to use cheat sheets for the assessment, but you must write or print them yourself\n\nmaximum 4 sides of A4 paper\nno electronic devices allowed\n\nPrinting services are available on campus. See here."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#learning-outcomes",
    "href": "lectures/L08/lecture-08.html#learning-outcomes",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAt the end of this week, you will be able to:\n\nUnderstand and explain resampling techniques as alternatives to parametric and non-parametric tests\nApply permutation tests to analyse data when parametric assumptions are not met\nImplement bootstrap methods to estimate confidence intervals and statistical parameters\nCompare and contrast different statistical approaches (parametric, non-parametric, and resampling)"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#non-normal-data",
    "href": "lectures/L08/lecture-08.html#non-normal-data",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Non-normal data",
    "text": "Non-normal data\nWhere data does not meet the assumptions of parametric tests, we have two options:\n\nTransform the data, and continue with parametric tests; or\nUse non-parametric “equivalents” of parametric tests at the cost of power and loss of information.\n\n\nA third option exists.\n\n\nUse computer intensive, randomisation-based methods to test hypotheses, called resampling techniques.\nThese methods include randomisation (or permutation) tests and bootstrap.\nRetains estimates of effect size and confidence intervals."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#model-based-inferential-techniques",
    "href": "lectures/L08/lecture-08.html#model-based-inferential-techniques",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Model-based inferential techniques",
    "text": "Model-based inferential techniques\n\nTraditionally, inferential statistics is based on mathematical approximations and assumptions about how data is obtained.\nBased on knowledge that “randomness” somehow obeys certain patterns in nature which can be reliably described by probability distributions.\nUses probability theory to draw approximate conclusions about these patterns when we observe data."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#resampling-techniques-1",
    "href": "lectures/L08/lecture-08.html#resampling-techniques-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Resampling techniques",
    "text": "Resampling techniques\n\nBased on the idea that we can use the data itself to estimate the distribution of the test statistic or parameter of interest.\nThese methods are model-free and distribution-free.\nRequires comparatively higher computational power, but nowadays it is not a problem – any modern personal computer can handle it."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#randomisation-or-bootstrap",
    "href": "lectures/L08/lecture-08.html#randomisation-or-bootstrap",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Randomisation or Bootstrap?",
    "text": "Randomisation or Bootstrap?\n\nThey are not the same:\n\n\n\nRandomisation\nGenerate a distribution of the test statistic under the null hypothesis by randomly sub-sampling the data, without replacement. Can be used to estimate a p-value.\n\n\n\nBootstrap\nGenerate a distribution of the parameter of interest (e.g. mean) by resampling the data with replacement1. Can be used to estimate confidence intervals.\n\n\n\n\n\n\nBasically, shuffle the data and see what happens.\n\n\n\n\n\nBasically, create alternative versions of the data and see what happens.\n\n\n\n\nAlso known as hallucination as it creates alternative versions of the data."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#why-would-these-techniques-work",
    "href": "lectures/L08/lecture-08.html#why-would-these-techniques-work",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Why would these techniques work?",
    "text": "Why would these techniques work?\nAt the core of the resampling approach is the idea that the observed data is a random sample from a larger population.\n\nIf the sampled data is truly representative of the population…\nThen, if we infinitely resample from the sample itself, we should be able to somewhat approximate the distribution of the test statistic under the null hypothesis, or parameter of interest (will show example later)."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#example-comparing-two-groups",
    "href": "lectures/L08/lecture-08.html#example-comparing-two-groups",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Example: comparing two groups",
    "text": "Example: comparing two groups\n\nSuppose we have two samples (groups) and we want to test if the mean scores1 are different.\nUnder the null hypothesis that there is no difference between the groups, the two sets of scores will have the same distribution.\nThus, we can pool the scores and reassign them to the two groups, since any score is equally likely to belong in either group, i.e. the scores are exchangeable.\n\nBasically any measure of interest."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#steps",
    "href": "lectures/L08/lecture-08.html#steps",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Steps",
    "text": "Steps\n\nPool the scores from both groups into a single dataset.\nRandomly reassign the scores to two groups.\nCalculate the test statistic of interest, in this case the t-test statistic.\nRepeat steps 2 and 3 many times to generate a distribution of the test statistic under the null hypothesis.\nCompare the observed test statistic to the randomised distribution to calculate a p-value."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#data",
    "href": "lectures/L08/lecture-08.html#data",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Data",
    "text": "Data\nThe sleep dataset in R contains the average extra hours of sleep, compared to control, for 10 patients who were given two different drugs.\n\n\nCode\nlibrary(tidyverse)\nglimpse(sleep)\n\n\nRows: 20\nColumns: 3\n$ extra &lt;dbl&gt; 0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0.0, 2.0, 1.9, 0.8, …\n$ group &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ ID    &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#are-assumptions-of-normality-met",
    "href": "lectures/L08/lecture-08.html#are-assumptions-of-normality-met",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Are assumptions of normality met?",
    "text": "Are assumptions of normality met?\nWe picked a dataset where the assumptions are met, so that we can compare the results with the parametric test.\n\n\nCode\nggplot(sleep, aes(x = group, y = extra)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#calculating-the-t-test-statistic",
    "href": "lectures/L08/lecture-08.html#calculating-the-t-test-statistic",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Calculating the t-test statistic",
    "text": "Calculating the t-test statistic\nRecall that the test statistic for the two-sample t-test is:\n\\[ t = \\frac{Difference\\ in\\ the\\ means}{Standard\\ error\\ of\\ the\\ difference} \\]\nWe could calculate it manually, but let’s just use the t.test() function in R since the function calculates the test statistic for us. For example, the observed test statistic for the sleep data is:\n\nt.test(extra ~ group, data = sleep)$statistic\n\n        t \n-1.860813"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-1-pool-the-scores",
    "href": "lectures/L08/lecture-08.html#step-1-pool-the-scores",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 1: Pool the scores",
    "text": "Step 1: Pool the scores\nThe first step is to pool the data. The pooled data is:\n\npooled_data &lt;- sleep$extra\npooled_data\n\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0  1.9  0.8  1.1  0.1 -0.1\n[16]  4.4  5.5  1.6  4.6  3.4\n\n\nWhere the first 10 scores are from the first group, and the next 10 scores are from the second group."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-2-randomly-reassign-the-scores",
    "href": "lectures/L08/lecture-08.html#step-2-randomly-reassign-the-scores",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 2: Randomly reassign the scores",
    "text": "Step 2: Randomly reassign the scores\n\n\nCode\npooled_data\n\n\n [1]  0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0  1.9  0.8  1.1  0.1 -0.1\n[16]  4.4  5.5  1.6  4.6  3.4\n\n\nNext, we randomly shuffle the pooled data and re-assign the first 10 scores to group 1, and the next 10 scores to group 2.\n\nset.seed(1022)\nshuffled_data &lt;- sample(pooled_data)\nshuffled_data\n\n [1] -1.2  2.0  3.7  0.1  0.8  1.6  0.8  1.9  0.0  3.4 -1.6 -0.2  1.1 -0.1  0.7\n[16]  4.4  5.5  3.4 -0.1  4.6\n\ngroup1 &lt;- shuffled_data[1:10]\ngroup2 &lt;- shuffled_data[11:20]"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-3-calculate-the-test-statistic",
    "href": "lectures/L08/lecture-08.html#step-3-calculate-the-test-statistic",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 3: Calculate the test statistic",
    "text": "Step 3: Calculate the test statistic\nWe’re not using the results from the t.test() function, but just extracting the test statistic.\n\nt.test(group1, group2)$statistic\n\n         t \n-0.4995608"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-4-repeat-many-times",
    "href": "lectures/L08/lecture-08.html#step-4-repeat-many-times",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 4: Repeat many times",
    "text": "Step 4: Repeat many times\nPutting it all together, we can write a function to obtain the test statistic:\n\nrandom_t &lt;- function(data) {\n  shuffled_data &lt;- sample(data)\n  group1 &lt;- shuffled_data[1:10]\n  group2 &lt;- shuffled_data[11:20]\n\n  return(t.test(group1, group2)$statistic)\n}\n\nRepeat the function 10,000 times:\n\nset.seed(1034)\nrandom_t_values &lt;- replicate(10000, random_t(pooled_data))"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-5-compare-the-observed-test-statistic",
    "href": "lectures/L08/lecture-08.html#step-5-compare-the-observed-test-statistic",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 5: Compare the observed test statistic",
    "text": "Step 5: Compare the observed test statistic\nFinally, we can compare the observed test statistic to the randomised distribution. This can be done by calculating the proportion of randomised test statistics that are more extreme than the observed test statistic.\n\nfit &lt;- t.test(extra ~ group, data = sleep) # test on observed data\np_value &lt;- mean(abs(random_t_values) &gt;= abs(fit$statistic))\nround(p_value, 2)\n\n[1] 0.08\n\n\nHow does this compare to the parametric t-test?\nIf we round the p-values of both tests to two decimal places, we get:\n\nround(fit$p.value, 2)\n\n[1] 0.08\n\n\nAs we can see, the p-values are very similar. This is because the assumptions of the parametric test are met, so the results will be close even though the methods are different!"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#whats-the-difference-between-the-two-techniques",
    "href": "lectures/L08/lecture-08.html#whats-the-difference-between-the-two-techniques",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "What’s the difference between the two techniques?",
    "text": "What’s the difference between the two techniques?\n\n\n\nt-test\n\nThe parametric test compares the observed test statistic to a theoretical distribution that has fixed parameters.\n\n\nRandomisation test\n\nThe randomisation test compares the observed test statistic to a randomised distribution that is generated from the data itself.\n\n\n\n\n\n\n\nAssumes that the data is normally distributed.\n\n\n\nNo assumptions about the data distribution, but if the assumption were met, the results would be similar.\n\n\n\n\n\n\n\nP-value is calculated from the theoretical distribution.\n\n\n\nP-value is calculated from the simulated distribution."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#visualising-the-randomised-distribution",
    "href": "lectures/L08/lecture-08.html#visualising-the-randomised-distribution",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Visualising the randomised distribution",
    "text": "Visualising the randomised distribution\n\n\nCode\nggplot(data.frame(t = random_t_values), aes(x = t)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = fit$statistic, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Randomised distribution of t-test statistic\",\n       x = \"t-test statistic\",\n       y = \"Frequency\")\n\n\n\nWe can see that the observed test statistic is well within the distribution of the randomised test statistics (which is normally distributed)."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#randomisation-tests-using-coin",
    "href": "lectures/L08/lecture-08.html#randomisation-tests-using-coin",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Randomisation tests using coin",
    "text": "Randomisation tests using coin\nThe coin package in R provides a simple interface to perform randomisation tests.\n\n\nCode\nlibrary(coin)\n\n\nLet’s use the sleep dataset to demonstrate how to use the coin package to perform a randomisation test.\n\n\nCode\nperm_test &lt;- independence_test(extra ~ group, data = sleep, distribution = approximate(B = 9999))\n\n# Print results\nprint(perm_test)\n\n\n\n    Approximative General Independence Test\n\ndata:  extra by group (1, 2)\nZ = -1.7508, p-value = 0.08321\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#example-estimating-the-mean",
    "href": "lectures/L08/lecture-08.html#example-estimating-the-mean",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Example: estimating the mean",
    "text": "Example: estimating the mean\n\nSuppose we have two samples (groups) and we want to estimate the difference in means, and the 95% confidence interval of the difference.\nWe can use the usual mathematical equation to calculate 95% CI, but if the data does not meet the assumption of normality, then the CI will be a bad estimate.\nInstead, we can use the bootstrap to estimate the 95% CI, which is based on the simulated distribution of the mean difference."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#steps-1",
    "href": "lectures/L08/lecture-08.html#steps-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Steps",
    "text": "Steps\n\nResample the data with replacement.\nCalculate the parameter of interest (e.g. mean) for each resample.\nRepeat steps 1 and 2 many times (\\(N\\)) to generate a distribution of the parameter of interest.\nCalculate the 95% confidence interval from the simulated distribution:\n\nThe mean of the distribution is the point estimate.\nThe \\(0.025 \\times N\\)th smallest mean is the lower bound of the 95% CI.\nThe \\(0.975\\times N\\)th smallest mean is the upper bound of the 95% CI."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#data-1",
    "href": "lectures/L08/lecture-08.html#data-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Data",
    "text": "Data\nThe BOD dataset in R contains the biochemical oxygen demand (mg/L) measurements of 6 samples over time.\n\nglimpse(BOD)\n\nRows: 6\nColumns: 2\n$ Time   &lt;dbl&gt; 1, 2, 3, 4, 5, 7\n$ demand &lt;dbl&gt; 8.3, 10.3, 19.0, 16.0, 15.6, 19.8"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-1-resample-the-data",
    "href": "lectures/L08/lecture-08.html#step-1-resample-the-data",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 1: Resample the data",
    "text": "Step 1: Resample the data\nFrom the pooled original data:\n\nBOD$demand\n\n[1]  8.3 10.3 19.0 16.0 15.6 19.8\n\n\nWe sample() with replacement:\n\nset.seed(1113)\nresampled_data &lt;- sample(BOD$demand, replace = TRUE)\nresampled_data\n\n[1] 15.6  8.3  8.3  8.3 16.0  8.3\n\n\nNoting that some scores will be repeated, and some will be missing."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-2-calculate-the-parameter-of-interest",
    "href": "lectures/L08/lecture-08.html#step-2-calculate-the-parameter-of-interest",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 2: Calculate the parameter of interest",
    "text": "Step 2: Calculate the parameter of interest\nThe parameter of interest is the mean value.\n\nmean(resampled_data)\n\n[1] 10.8"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-3-repeat-many-times",
    "href": "lectures/L08/lecture-08.html#step-3-repeat-many-times",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 3: Repeat many times",
    "text": "Step 3: Repeat many times\nSince it’s a simple process, we can write a function to calculate the mean:\n\n\nCode\nbootstrap_mean &lt;- function(data) {\n  resampled_data &lt;- sample(data, replace = TRUE)\n  return(mean(resampled_data))\n}\n\n\nThen repeat the function 10,000 times:\n\nset.seed(1116)\nbootstrap_means &lt;- replicate(10000, bootstrap_mean(BOD$demand))"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#step-4-calculate-the-95-ci",
    "href": "lectures/L08/lecture-08.html#step-4-calculate-the-95-ci",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Step 4: Calculate the 95% CI",
    "text": "Step 4: Calculate the 95% CI\nThe 95% CI is calculated from the simulated distribution:\n\nmeanval &lt;- mean(bootstrap_means)\nCI &lt;- quantile(bootstrap_means, c(0.025, 0.975))\n\nPutting it together, the mean is 14.85131 with a 95% CI of [11.25, 18.13].\nHow does this compare to the parametric test?\nIf we use the t.test() function to calculate the 95% CI:\n\nt.test(BOD$demand)\n\n\n    One Sample t-test\n\ndata:  BOD$demand\nt = 7.8465, df = 5, p-value = 0.0005397\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  9.973793 19.692874\nsample estimates:\nmean of x \n 14.83333"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#how-different-are-the-results",
    "href": "lectures/L08/lecture-08.html#how-different-are-the-results",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "How different are the results?",
    "text": "How different are the results?\n\n\n\nMethod\nMean\n95% CI\nCI size\n\n\n\n\nBootstrap\n14.85\n[11.25, 18.13]\n6.88\n\n\nParametric\n14.83\n[9.97, 19.69]\n9.72\n\n\n\n\nThe point estimates of the mean are almost identical.\nThe 95% CI of the mean are similar, but the bootstrap CI is non-symmetric - it represents the true distribution of the mean.\nThe size of the CI is smaller for the bootstrap method, indicating that the parametric method is overestimating the precision of the estimate."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#again-we-can-use-a-package-to-do-this",
    "href": "lectures/L08/lecture-08.html#again-we-can-use-a-package-to-do-this",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Again we can use a package to do this",
    "text": "Again we can use a package to do this\nThe boot package in R provides a simple interface to perform bootstrap tests.\n\n\nCode\nlibrary(boot)\n\n\nLet’s use the BOD dataset to demonstrate how to use the boot package to perform a bootstrap test.\n\n\nCode\n# Define function to compute mean\nboot_mean &lt;- function(data, indices) {\n  return(mean(data[indices]))\n}\n\nboot_test &lt;- boot(BOD$demand, boot_mean, R = 10000)\n\n# Print results \nprint(boot_test)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = BOD$demand, statistic = boot_mean, R = 10000)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1* 14.83333 -0.010455     1.72223\n\n\nCode\n# Compute bootstrap confidence interval\nboot.ci(boot_test, type = c(\"perc\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_test, type = c(\"perc\"))\n\nIntervals : \nLevel     Percentile     \n95%   (11.40, 18.13 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#using-infer",
    "href": "lectures/L08/lecture-08.html#using-infer",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Using infer",
    "text": "Using infer\n\n\nCode\nlibrary(tidymodels)\n\n\nLet’s use the sleep dataset to demonstrate how to use the infer package to perform a randomisation test (also makes it easier to compare against manual method).\nThe infer package requires the user to use an expressive grammar to specify the analysis.\nSteps\n\nspecify() the response variable of interest, then\nhypothesise() the null hypothesis, then\ngenerate() the null distribution, and finally\ncalculate() the p-value."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#two-sample-t-test-using-infer",
    "href": "lectures/L08/lecture-08.html#two-sample-t-test-using-infer",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Two-sample t-test using infer",
    "text": "Two-sample t-test using infer\nFirst we need to calculate the observed test statistic so that we can compare it to the simulated distribution.\n\nobserved &lt;- sleep %&gt;%\n  specify(extra ~ group) %&gt;%\n  calculate(stat = \"diff in means\", order = c(1, 2))\nobserved\n\nResponse: extra (numeric)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -1.58\n\n\nThen we generate the null distribution and calculate the p-value:\n\nset.seed(1034)\npval_infer &lt;- \n  sleep %&gt;%\n  specify(extra ~ group) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 10000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(1, 2)) %&gt;%\n   get_p_value(obs_stat = observed,\n              direction = \"two-sided\")"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#what-are-the-differences",
    "href": "lectures/L08/lecture-08.html#what-are-the-differences",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "What are the differences?",
    "text": "What are the differences?\n\n\n\nMethod\nP-value\n\n\n\n\nManual\n0.082\n\n\ninfer\n0.084\n\n\nt.test()\n0.079\n\n\n\nAs we can see, the results are very similar because the assumptions of the parametric test were already met.\n\n\n\n\n\n\nNote\n\n\nTo calculate confidence intervals, use the get_ci() function as documented here."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#example-beetles",
    "href": "lectures/L08/lecture-08.html#example-beetles",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Example: beetles",
    "text": "Example: beetles\nThe beetle dataset was used in last week’s lecture to demonstrate the non-parametric Wilcoxon rank-sum test.\n\nbeetle &lt;- readr::read_csv(\"data/beetle.csv\")\nglimpse(beetle)\n\nRows: 45\nColumns: 2\n$ SIZE    &lt;chr&gt; \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\",…\n$ BEETLES &lt;dbl&gt; 256, 209, 0, 0, 0, 44, 49, 117, 6, 0, 0, 75, 34, 13, 0, 90, 0,…"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#assumption",
    "href": "lectures/L08/lecture-08.html#assumption",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Assumption",
    "text": "Assumption\nRecall that the data does not meet the assumptions of normality:\n\n\nCode\np1 &lt;- ggplot(beetle, aes(x = BEETLES)) +\n  geom_histogram(bins = 14, position = \"dodge\") +\n  facet_wrap(~SIZE, ncol = 1)\np2 &lt;- ggplot(beetle, aes(x = SIZE, y = BEETLES)) +\n  geom_boxplot()\np3 &lt;- ggplot(beetle, aes(sample = BEETLES)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~SIZE, ncol = 1)\n\nlibrary(patchwork)\np1 + p2 + p3"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#t-test-via-resampling-using-infer",
    "href": "lectures/L08/lecture-08.html#t-test-via-resampling-using-infer",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "T-test via resampling using infer",
    "text": "T-test via resampling using infer\nFirst, calculate the test statistic:\n\nobserved &lt;- beetle %&gt;%\n  specify(BEETLES ~ SIZE) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"small\", \"large\"))\n\nThen generate the null distribution and calculate the p-value:\n\nset.seed(1034)\npval_infer &lt;- \n  beetle %&gt;%\n  specify(BEETLES ~ SIZE) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 10000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"small\", \"large\")) %&gt;%\n  get_p_value(obs_stat = observed,\n              direction = \"two-sided\")\n\npval_infer\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.025"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#comparisons",
    "href": "lectures/L08/lecture-08.html#comparisons",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Comparisons",
    "text": "Comparisons\nLet’s compare the p-values from\n\na t-test (if we ignore violations of assumptions),\nthe wilcoxon rank-sum test, and\nthe infer randomisation test."
  },
  {
    "objectID": "lectures/L08/lecture-08.html#comparisons-1",
    "href": "lectures/L08/lecture-08.html#comparisons-1",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Comparisons",
    "text": "Comparisons\nT-test\n\n\nCode\np_ttest &lt;- t.test(BEETLES ~ SIZE, data = beetle)$p.value %&gt;%\n  round(3)\n\n\nWilcoxon rank-sum test\n\n\nCode\np_wilcox &lt;- wilcox.test(BEETLES ~ SIZE, data = beetle)$p.value %&gt;%\n  round(3)\n\n\nRandomisation test\n\n\nCode\np_infer &lt;- pull(pval_infer, p_value) %&gt;%\n  round(3)"
  },
  {
    "objectID": "lectures/L08/lecture-08.html#results",
    "href": "lectures/L08/lecture-08.html#results",
    "title": "Topic 8 – Permutation tests and bootstrap",
    "section": "Results",
    "text": "Results\n\n\n\nMethod\nP-value\n\n\n\n\nT-test\n0.037\n\n\nWilcoxon\n0.075\n\n\ninfer\n0.025\n\n\n\n\nAs we can see, the p-values are quite different because the assumptions of the parametric test were violated.\nThe randomisation test is more robust and provides a more accurate estimate of the p-value than the Wilcoxon rank-sum test.\n\nHow to report results of randomisation test\nThe results of the randomisation test can be reported as follows:\n\nBeetle consumption was significantly different between small and large beetles (t = 2.19, R = 10000, p = 0.025)."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#about-me",
    "href": "lectures/L09/lecture-09.html#about-me",
    "title": "Topic 9 – Describing relationships",
    "section": "About me",
    "text": "About me\n\n\n\n\nResearch topics: spatial modelling and mapping, precision agriculture, winter grains\nTimeline at USYD\n\nBSc (Hons) in Agricultural Science\nPhD in Digital Agriculture\nPostdoc in Spatial Modelling\nAssociate Lecturer in Agricultural Data Science\n\n\n\n\n\n\n\nFaba beans at Trangie"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#learning-outcomes",
    "href": "lectures/L09/lecture-09.html#learning-outcomes",
    "title": "Topic 9 – Describing relationships",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLO1. Demonstrate proficiency in utilizing R and Excel to effectively explore and describe data sets in the life sciences.\nLO2. Evaluate and interpret different types of data in the natural sciences by visualising probability distributions and calculating probabilities using RStudio and Excel.\nLO3. Apply parametric and non-parametric statistical inference methods to experimental data using RStudio and effectively interpret and communicate the results in the context of the data.\nLO4. Apply both linear and non-linear models to describe relationships between variables using RStudio and Excel, demonstrating creativity in developing models that effectively represent complex data patterns.\nLO5. Articulate statistical and modelling results clearly and convincingly in both written reports and oral presentations, working effectively as an individual and collaboratively in a team, showcasing the ability to convey complex information to varied audiences."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#module-overview",
    "href": "lectures/L09/lecture-09.html#module-overview",
    "title": "Topic 9 – Describing relationships",
    "section": "Module overview",
    "text": "Module overview\n\nWeek 9. Describing Relationships\n\nCorrelation (calculation, interpretation)\nRegression (model structure, model fitting\nWhat/when/why/how\n\nWeek 10. Simple Linear Regression\n\nCan we use the model?(assumptions, hypothesis testing)\nHow good is the model?(interpretation, model fit)\n\nWeek 11. Multiple Linear Regression\n\nMultiple Linear Regression (MLR) modelling\nAssumptions, interpretation and the principle of parsimony\n\nWeek 12. Nonlinear Regression\n\nCommon nonlinear functions\nTransformations"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#module-overview-1",
    "href": "lectures/L09/lecture-09.html#module-overview-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Module overview",
    "text": "Module overview\n\nWeek 9. Describing Relationships\n\nCorrelation (calculation, interpretation)\nRegression (model structure, model fitting\nWhat/when/why/how"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example---galtons-data",
    "href": "lectures/L09/lecture-09.html#example---galtons-data",
    "title": "Topic 9 – Describing relationships",
    "section": "Example - Galton’s Data",
    "text": "Example - Galton’s Data\n\n\nCode\nlibrary(HistData)\nhead(Galton)\n\n\n  parent child\n1   70.5  61.7\n2   68.5  61.7\n3   65.5  61.7\n4   64.5  61.7\n5   64.0  61.7\n6   67.5  62.2\n\n\n\n\n928 children of 205 pairs of parents\nAverage height of both parents and child measured in inches\nSize classes were binned (hence data looks discrete)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example---galtons-data-1",
    "href": "lectures/L09/lecture-09.html#example---galtons-data-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Example - Galton’s Data",
    "text": "Example - Galton’s Data\nWe can visually inspect the relationship between the two variables using a scatterplot:\n\n\nCode\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n    geom_point(alpha = .2, size = 3)\n\n\n\n… but this is not a very good way to assess the strength of the relationship between the two variables.\nIs the relationship:\n\nLinear?\nPositive or negative?\nWeak, moderate or strong?"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#correlation-1",
    "href": "lectures/L09/lecture-09.html#correlation-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Correlation",
    "text": "Correlation\nThe correlation coefficient is a number between -1 and 1 that describes the relationship between two continuous variables.\n\nDirection:\n\nPositive – both variables increase together\nNegative – one variable increases as the other decreases\n\nStrength:\n\n-1 → perfect negative relationship\n0 → no relationship\n1 → perfect positive relationship\nCommon terms: weak (~0.1–0.3), moderate (~0.4–0.6) or strong (~0.7–1.0) – useful but subjective!"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#correlation-2",
    "href": "lectures/L09/lecture-09.html#correlation-2",
    "title": "Topic 9 – Describing relationships",
    "section": "Correlation",
    "text": "Correlation\n\n\nPearson’s Correlation (r) Formula:\n\\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \\]\nCovariance divided by the product of the standard deviations.\n\nKarl Pearson developed the correlation coefficient in 1800s (based on the work by Francis Galton)\n\n\n\n\nPearson’s r is:\n\nThe most commonly used correlation coefficient\nFor normally distributed data only (parametric)\nFor linear relationships only (i.e. a straight line)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#luckily-we-have-excel-and-r",
    "href": "lectures/L09/lecture-09.html#luckily-we-have-excel-and-r",
    "title": "Topic 9 – Describing relationships",
    "section": "Luckily we have Excel and R",
    "text": "Luckily we have Excel and R\n\nExcel: =CORREL() formula, or use the Analysis Toolpak\nR: cor() function\n\nBy default this calculates Pearson’s correlation coefficient\nCan also calculate other types of correlation (cor(x, y, method = \"spearman\"))"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example---galtons-data-2",
    "href": "lectures/L09/lecture-09.html#example---galtons-data-2",
    "title": "Topic 9 – Describing relationships",
    "section": "Example - Galton’s Data",
    "text": "Example - Galton’s Data\n\n\n\n\n\nGalton’s Results\n\n\n\n\n\nCode\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n    geom_point(alpha = .2, size = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncor(Galton$parent, Galton$child)\n\n\n[1] 0.4587624\n\n\nIs the relationship:\n\nLinear? Seems to be\nPositive or negative? Positive\nWeak, moderate or strong? Moderate\n\nIn words: “There is a moderate positive linear relationship between the height of parents and the height of their children.”"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#anscombes-quartet",
    "href": "lectures/L09/lecture-09.html#anscombes-quartet",
    "title": "Topic 9 – Describing relationships",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\nA set of four datasets that are nearly identical in simple descriptive statistics but have very different distributions.\n\n\nCode\nlibrary(tidyverse)\n\n# Compute Pearson, Spearman, and Kendall correlations for each set\ncor_values &lt;- anscombe %&gt;%\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\") %&gt;%\n  group_by(set) %&gt;%\n  summarise(\n    pearson = cor(x, y, method = \"pearson\"))\n\n# Create the plot with the correlation values added as text annotations\nanscombe %&gt;%\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\") %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  facet_wrap(~set, ncol = 4) +\n  # Add text annotations for Pearson, Spearman, and Kendall correlations\n  geom_text(data = cor_values, aes(x = 8, y = 4, label = \n    paste(\"Pearson: \", round(pearson, 2))),\n    inherit.aes = FALSE, size = 3.5, color = \"black\", hjust = 0)\n\n\n\nCorrelation coefficients are not reliable in inferring the ‘type’ of relationship between variables – we must visualise."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#datasaurus-dozen",
    "href": "lectures/L09/lecture-09.html#datasaurus-dozen",
    "title": "Topic 9 – Describing relationships",
    "section": "Datasaurus Dozen",
    "text": "Datasaurus Dozen\n\n\nCode\nlibrary(datasauRus)\nggplot(datasaurus_dozen, aes(x = x, y = y)) +\n    geom_point(size = .5, alpha = .3) +\n    geom_smooth(method = \"lm\", se = TRUE) +\n    facet_wrap(~dataset, ncol = 6)\n\n\n\nAll of these data have a correlation coefficient close to zero!"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#monotonic-vs-linear-relationships",
    "href": "lectures/L09/lecture-09.html#monotonic-vs-linear-relationships",
    "title": "Topic 9 – Describing relationships",
    "section": "Monotonic vs linear relationships",
    "text": "Monotonic vs linear relationships\n\nMonotonic: a relationship that is consistently increasing or decreasing\nLinear: a relationship that is increasing or decreasing at a constant rate i.e. a straight line\n\nPearson’s correlation is only for linear relationships.\nSpearman’s rank and Kendall’s tau are correlation coefficients for all monotonic relationships.\n\nWorks with non-parametric data\nMore ‘conservative’ i.e. values can be smaller in magnitude, but more robust against outliers\n\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(123)\nx &lt;- seq(0, 10, length.out = 100)\nx_length &lt;- length(x)\n\n# Create the data for each relationship\ny_straight &lt;- 2 * x + 5\ny_exponential &lt;- exp(x)\ny_log &lt;- logb(x+1, base = 2)\ny_sigmoid &lt;- 1 / (1 + exp(-x))\ny_quadratic &lt;- -1 * (x - 5)^2 + 100\ny_polynomial &lt;- x^3 - 4 * x^2 + 3 * x + 2\n\n# Function to scale y-values to a specific range [0, 100]\nscale_to_range &lt;- function(y_values, new_min = 0, new_max = 100) {\n  old_min &lt;- min(y_values)\n  old_max &lt;- max(y_values)\n  scaled_values &lt;- (y_values - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n  return(scaled_values)\n}\n\n# Apply scaling to each relationship\ny_straight_scaled &lt;- scale_to_range(y_straight)\ny_exponential_scaled &lt;- scale_to_range(y_exponential)\ny_log_scaled &lt;- scale_to_range(y_log)\ny_sigmoid_scaled &lt;- scale_to_range(y_sigmoid)\ny_quadratic_scaled &lt;- scale_to_range(y_quadratic)\ny_polynomial_scaled &lt;- scale_to_range(y_polynomial)\n\n# Combine the data into a tibble\ndata &lt;- tibble(\n  x = rep(x, 6),\n  y = c(y_straight_scaled, y_exponential_scaled, y_log_scaled, y_sigmoid_scaled, y_quadratic_scaled, y_polynomial_scaled),\n  type = rep(c(\"Straight Line\", \"Exponential\", \"Log Curve\", \"Sigmoid Curve\", \"Quadratic Line\", \"Polynomial\"), each = x_length)\n)\n\n# Calculate the correlation coefficients for each relationship type\ncor_values &lt;- data %&gt;%\n  group_by(type) %&gt;%\n  summarise(\n    pearson = cor(x, y, method = \"pearson\"),\n    spearman = cor(x, y, method = \"spearman\"),\n    kendall = cor(x, y, method = \"kendall\")\n  )\n\n# Plot the data and add correlation coefficients\nggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  facet_wrap(~type, scales = 'free') +\n  geom_text(data = cor_values, aes(x = 3, y =25, label = \n                                    paste(\"Pearson: \", round(pearson, 2), \n                                          \"\\nSpearman: \", round(spearman, 2),\n                                          \"\\nKendall: \", round(kendall, 2))),\n            inherit.aes = FALSE, size = 4, color = \"black\", hjust = 0) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example---iris-dataset",
    "href": "lectures/L09/lecture-09.html#example---iris-dataset",
    "title": "Topic 9 – Describing relationships",
    "section": "Example - iris Dataset",
    "text": "Example - iris Dataset\n\n\n\nCode\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example---iris-dataset-1",
    "href": "lectures/L09/lecture-09.html#example---iris-dataset-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Example - iris Dataset",
    "text": "Example - iris Dataset\nWhich correlation coefficient do we use?\n\n\nCode\nplot(iris[,-5])"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example---iris-dataset-2",
    "href": "lectures/L09/lecture-09.html#example---iris-dataset-2",
    "title": "Topic 9 – Describing relationships",
    "section": "Example - iris Dataset",
    "text": "Example - iris Dataset\n\n\nCode\ncor(iris[,-5]) |&gt; round(2)\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n\nCorrelation analysis identifies variables that have a strong linear relationship quickly and easily.\n\nWe want to predict the Petal.Width of an iris flower\nWhich variables would be good predictors?\n\nEither Petal.Length and Sepal Length would be good predictors\n\nWhich variables are likely to present issues for model fitting?\n\ne.g. Petal.Length and Sepal Length are strongly correlated - including both would cause multicollinearity – (L11 MLR)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#correlation-neq-causation",
    "href": "lectures/L09/lecture-09.html#correlation-neq-causation",
    "title": "Topic 9 – Describing relationships",
    "section": "Correlation \\(\\neq\\) causation",
    "text": "Correlation \\(\\neq\\) causation\nSpurious correlations: a relationship between two variables does not imply that one causes the other."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#what-comes-after-correlation",
    "href": "lectures/L09/lecture-09.html#what-comes-after-correlation",
    "title": "Topic 9 – Describing relationships",
    "section": "What comes after correlation?",
    "text": "What comes after correlation?\n\nWe have data with two or more numerical variables\nWe conduct correlation analysis to describe possible linear relationships\n\nFast, easy, interpretable, and widely used\n\n\nBut, we can’t infer causation: - Is there reason() to expect a relationship between the two variables? - Do you have a hypothesis* about the relationship between the two variables?\nIf we have a hypothesis about the relationship between two variables, we can use regression analysis to test it."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#why-regression",
    "href": "lectures/L09/lecture-09.html#why-regression",
    "title": "Topic 9 – Describing relationships",
    "section": "Why regression?",
    "text": "Why regression?\nDescribe the relationship between two variables\nWhat is the relationship between a response variable \\(Y\\) and a predictor variable \\(x\\)?\nCommon Terms: \\(Y\\) = response, independent variable, target, outcome etc. \\(x\\) = predictor, dependent variable, feature, input etc.\nExplain the relationship between two variables\nHow much variation in \\(Y\\) can be explained by a relationship with \\(x\\)?\nPredict the value of a response variable\nWhat is the value of \\(Y\\) for a given value of \\(x\\)?\nOften we can easily measure/obtain \\(x\\) but not \\(Y\\), so we need to predict \\(Y\\) from \\(x\\)."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#a-gateway-to-the-world-of-modelling",
    "href": "lectures/L09/lecture-09.html#a-gateway-to-the-world-of-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "A gateway to the world of modelling",
    "text": "A gateway to the world of modelling\nMany types of regression models exist:\n\nSimple linear regression (one predictor i.e. \\(x\\))\nMultiple linear regression (more than one predictor)\nNon-linear regression, using functions such as polynomials, exponentials, logarithms, etc.\n\n\nAsking ChatGPT for help with the next slide:\n\nUsing R code, can you generate some data that is useful to demonstrate simple linear regression, multiple linear regression, polynomial, exponential and logarithmic regressions in ggplot2?\n\n\nSure! Here’s an example code that generates a sample dataset and visualizes it using ggplot2 library in R."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#visualising-regression-models",
    "href": "lectures/L09/lecture-09.html#visualising-regression-models",
    "title": "Topic 9 – Describing relationships",
    "section": "Visualising regression models",
    "text": "Visualising regression models\n\n\nCode\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(136)\nx &lt;- 1:100\ny &lt;- x^2 + rnorm(100, sd = 100)\n\n# Plot the data and regression lines\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n    ggtitle(\"Regression Models\") +\n    geom_point(alpha = .2) +\n    xlab(\"X\") +\n    ylab(\"Y\") +\n    ylim(-6000, 15000)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#visualising-regression-models-1",
    "href": "lectures/L09/lecture-09.html#visualising-regression-models-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Visualising regression models",
    "text": "Visualising regression models\n\n\nCode\nlibrary(ggplot2)\n\n# Generate sample data\nset.seed(136)\nx &lt;- 1:100\ny &lt;- x^2 + rnorm(100, sd = 100)\n\n# Define regression functions\nslr &lt;- function(x, y) {\n    mod &lt;- lm(y ~ x)\n    return(list(\n        data.frame(x = x, y = predict(mod), model_type = \"Simple Linear Regression\"),\n        paste(\"y =\", round(coefficients(mod)[[2]], 2), \"x +\", round(coefficients(mod)[[1]], 2))\n    ))\n}\n\nmlr &lt;- function(x, y, z) {\n    mod &lt;- lm(y ~ x + z)\n    return(list(\n        data.frame(x = x, z = z, y = predict(mod), model_type = \"Multiple Linear Regression\"),\n        paste(\"y =\", round(coefficients(mod)[[3]], 2), \"x +\", round(coefficients(mod)[[2]], 2), \"z +\", round(coefficients(mod)[[1]], 2))\n    ))\n}\n\npoly_reg &lt;- function(x, y, degree) {\n    mod &lt;- lm(y ~ poly(x, degree, raw = TRUE))\n    x_new &lt;- seq(min(x), max(x), length.out = 100)\n    y_new &lt;- predict(mod, newdata = data.frame(x = x_new))\n    return(list(\n        data.frame(x = x_new, y = y_new, model_type = paste(\"Polynomial Regression (\", degree, \")\", sep = \"\")),\n        paste(paste(\"x^\", degree, sep = \"\"), \":\", paste(round(coefficients(mod), 2), collapse = \" + \"))\n    ))\n}\n\nexp_reg &lt;- function(x, y) {\n    mod &lt;- lm(log(y) ~ x)\n    x_new &lt;- seq(min(x), max(x), length.out = 100)\n    y_new &lt;- exp(predict(mod, newdata = data.frame(x = x_new)))\n    return(list(\n        data.frame(x = x_new, y = y_new, model_type = \"Exponential Regression\"),\n        paste(\"y =\", round(exp(coefficients(mod)[[2]]), 2), \"* e^(\", round(coefficients(mod)[[1]], 2), \"x\", \")\")\n    ))\n}\n\nlog_reg &lt;- function(x, y) {\n    mod &lt;- lm(y ~ log(x))\n    x_new &lt;- seq(min(x), max(x), length.out = 100)\n    y_new &lt;- predict(mod, newdata = data.frame(x = x_new))\n    return(list(\n        data.frame(x = x_new, y = y_new, model_type = \"Logarithmic Regression\"),\n        paste(\"y =\", round(coefficients(mod)[[2]], 2), \"* log(x) +\", round(coefficients(mod)[[1]], 2))\n    ))\n}\n\n# Create regression line dataframes and equations\nreg_data &lt;- list(slr(x, y), mlr(x, y, rnorm(100, sd = 10)), poly_reg(x, y, 3), exp_reg(x, y), log_reg(x, y))\nreg_eqs &lt;- sapply(reg_data, function(x) x[[2]])\n\n# Plot the data and regression lines\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n    lapply(seq_along(reg_data), function(i) geom_line(data = reg_data[[i]][[1]], aes(x, y, color = reg_data[[i]][[1]]$model_type), linewidth = 1.4)) +\n    ggtitle(\"Regression Models\") +\n    geom_point(alpha = .2) +\n    xlab(\"X\") +\n    ylab(\"Y\") +\n    scale_color_discrete(name = \"Model Type\") +   # This line adds the title to the legend\n    ylim(-6000, 15000) +\n    theme(legend.position = \"right\")  # Modify this to display the legend (default position is 'right')"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example-climate-change-modelling",
    "href": "lectures/L09/lecture-09.html#example-climate-change-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "Example: climate change modelling",
    "text": "Example: climate change modelling\n\nSource: https://science2017.globalchange.gov/chapter/4/"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#example-covid-19-transmission-modelling",
    "href": "lectures/L09/lecture-09.html#example-covid-19-transmission-modelling",
    "title": "Topic 9 – Describing relationships",
    "section": "Example: COVID-19 transmission modelling",
    "text": "Example: COVID-19 transmission modelling\n\nSource: https://www.nature.com/articles/s41598-021-84893-4/figures/1"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#how-does-regression-work",
    "href": "lectures/L09/lecture-09.html#how-does-regression-work",
    "title": "Topic 9 – Describing relationships",
    "section": "How does regression work?",
    "text": "How does regression work?\n  \nAdrien-Marie Legendre, Carl Friedrich Gauss, Francis Galton\n\n\n\n\n\n\nNote\n\n\nMany other people contributed to the development of regression analysis, but these three are the most well-known."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#how-does-regression-work-1",
    "href": "lectures/L09/lecture-09.html#how-does-regression-work-1",
    "title": "Topic 9 – Describing relationships",
    "section": "How does regression work?",
    "text": "How does regression work?\n\nMethod of least squares first theorised by Adrien-Marie Legendre in 1805\nTechnique of least squares first used by Carl Friedrich Gauss in 1809 (to fit a parabola to the orbit of the asteroid Ceres)\nModel fitting first published by Francis Galton in 1886 (predicting the height of a child from the height of the parents)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#least-squares",
    "href": "lectures/L09/lecture-09.html#least-squares",
    "title": "Topic 9 – Describing relationships",
    "section": "Least squares",
    "text": "Least squares\n\n\nWhen we fit a line, there is error between the observed and predicted values.\n\\[ \\color{firebrick}{\\hat{\\epsilon_i}} = \\color{royalblue}{y_i} - \\color{forestgreen}{\\hat{y_i}} \\] The method of least squares fits a line of best fit by minimising the sum of the squared errors.\n\\[\\color{firebrick} \\sum_{i=1}^n ({\\hat{\\epsilon_i}})^2\\]\n\n\n\nCode\n# simulate example data\n\nset.seed(340)\nx &lt;- runif(8, 0, 30)\ny &lt;- 5 * x + rnorm(8, 0, 40)\ndf &lt;- data.frame(x, y)\n\n# fit linear model, add residual vertical lines as arrows\nmod &lt;- lm(y ~ x, data = df)\np1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_segment(aes(xend = x, yend = fitted(mod)),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    color = \"royalblue\"\n  ) +\n  labs(x = \"x\", y = \"y\")\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  annotate(\"text\",\n    x = 6.3, y = -6, size = 7,\n    label = expression(hat(epsilon[i])), colour = \"royalblue\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = 25, size = 7,\n    label = expression(hat(y[i])), colour = \"forestgreen\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = -36, size = 7,\n    label = expression(y[i]), colour = \"firebrick\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe error is squared so positive and negative errors do not cancel each other out."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#least-squares-1",
    "href": "lectures/L09/lecture-09.html#least-squares-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Least squares",
    "text": "Least squares\n Which line fits the data better?"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#how-does-a-computer-fit-a-line-of-best-fit",
    "href": "lectures/L09/lecture-09.html#how-does-a-computer-fit-a-line-of-best-fit",
    "title": "Topic 9 – Describing relationships",
    "section": "How does a computer fit a line of best fit?",
    "text": "How does a computer fit a line of best fit?\n\nThe line is fitted again and again until the squared error stabilises. A computer can do this very quickly!"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#simple-linear-regression-model",
    "href": "lectures/L09/lecture-09.html#simple-linear-regression-model",
    "title": "Topic 9 – Describing relationships",
    "section": "Simple linear regression model",
    "text": "Simple linear regression model\nWe want to relate the response \\(Y\\) to a predictor \\(x\\) for \\(i\\) number of observations:\n\\[Y_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}\\]\nwhere\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\n\n\\(Y_i\\), the response, is an observed value of the dependent variable.\n\\(\\beta_0\\), the constant, is the y-intercept and is fixed.\n\\(\\beta_1\\) is the population slope parameter, and like \\(\\beta_0\\), is also fixed.\n\\(\\epsilon_i\\) is the error associated with predictions of \\(y_i\\), and unlike \\(\\beta_0\\) or \\(\\beta_1\\), it is not fixed.\n\n\n\n\n\n\n\nNote\n\n\n\\(\\epsilon_i\\) is generally associated with the residual (\\(observed - predicted\\)). True error occurs during data collection (e.g. faulty instruments, selection bias, etc.) and often immeasurable."
  },
  {
    "objectID": "lectures/L09/lecture-09.html#different-wordings",
    "href": "lectures/L09/lecture-09.html#different-wordings",
    "title": "Topic 9 – Describing relationships",
    "section": "Different wordings",
    "text": "Different wordings\nDifferent ways to think about the response:\n\nResponse = Prediction + Error\nResponse = Signal + Noise\nResponse = Model + Unexplained\nResponse = Deterministic + Random\nResponse = Explainable + Everything else\nY = f(x)\nDependent variable = f(Independent variable)"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#model-fitting",
    "href": "lectures/L09/lecture-09.html#model-fitting",
    "title": "Topic 9 – Describing relationships",
    "section": "Model fitting",
    "text": "Model fitting\nTwo approaches; analytical and numerical:\n\nAnalytical: equation(s) used directly to find solution\nNumerical: computer uses “random guesses” to find set of parameters to that minimises objective function, in this case residual sum of squares"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#analytical-slope-beta_1",
    "href": "lectures/L09/lecture-09.html#analytical-slope-beta_1",
    "title": "Topic 9 – Describing relationships",
    "section": "Analytical: Slope, \\(\\beta_1\\)",
    "text": "Analytical: Slope, \\(\\beta_1\\)\n\n\\[ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} \\]\n\n\nCode\n# Calculate slope from df\nbeta1 &lt;- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) /\n  sum((df$x - mean(df$x))^2)\n# beta0 &lt;- mean(df$y) - beta1 * mean(df$x)\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linetype = 2) +\n  # label the line\n  annotate(\"text\",\n    x = 15, y = 65, size = 7,\n    label = expression(beta[1]), colour = \"firebrick\"\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#analytical-intercept",
    "href": "lectures/L09/lecture-09.html#analytical-intercept",
    "title": "Topic 9 – Describing relationships",
    "section": "Analytical: Intercept",
    "text": "Analytical: Intercept\n\\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\]\n\n\nCode\n# calculate mean y from df\nybar &lt;- mean(df$y)\nxbar &lt;- mean(df$x)\nbeta0 &lt;- ybar - beta1 * xbar\n\np1 + geom_vline(xintercept = xbar, linetype = \"dashed\", color = \"slateblue\") +\n  geom_hline(yintercept = ybar, linetype = \"dashed\", color = \"slateblue\") +\n  # label the lines\n  annotate(\"text\",\n    x = 25, y = ybar * 0.8, size = 7,\n    label = expression(bar(y)), colour = \"slateblue\"\n  ) +\n  annotate(\"text\",\n    x = xbar * 1.05, y = 150, size = 7,\n    label = expression(bar(x)), colour = \"slateblue\"\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linetype = 2) +\n  # extend the geom_smooth line to intercept x=0\n  geom_segment(aes(x = xbar, y = ybar, xend = 0, yend = beta0),\n    color = \"firebrick\", linetype = 2\n  ) +\n  # label the slope line\n  annotate(\"text\",\n    x = 15, y = 65, size = 7,\n    label = expression(beta[1]), colour = \"firebrick\"\n  ) +\n  # add a dot at the intercept\n  geom_point(aes(x = 0, y = beta0), color = \"seagreen\", size = 3) +\n  # label the intercept\n  annotate(\"text\",\n    x = 0, y = beta0 * 1.4, size = 7,\n    label = expression(beta[0]), colour = \"seagreen\"\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#linearity-check",
    "href": "lectures/L09/lecture-09.html#linearity-check",
    "title": "Topic 9 – Describing relationships",
    "section": "Linearity check",
    "text": "Linearity check\nRecap - there appears to be a linear relationship between child height and parent height (plot). This is a moderately positive relationship (correlation).\n\n\nCode\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n    geom_point(alpha = .2, size = 3)\n\n\n\nCode\ncor(Galton$parent, Galton$child)\n\n\n[1] 0.4587624"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#hypothesis-testing",
    "href": "lectures/L09/lecture-09.html#hypothesis-testing",
    "title": "Topic 9 – Describing relationships",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nThe null hypothesis for a linear model: \\(H_0: \\beta_1=0\\)\n\nIf there is no slope (\\(\\beta_1=0\\)) then \\(y = \\beta_0\\) (the mean)\ni.e. is the mean (\\(\\bar{y}\\)) a better fit than a linear model?\n\nThe alternative hypothesis for a linear model: \\(H_0: \\beta_1 \\neq 0\\)\n\ni.e. the estimate from the linear model (\\(\\hat{y}\\)) fits the data better than the mean (\\(\\bar{y}\\))\n\n\n\n\nCode\nnull_model &lt;- Galton %&gt;%\n  lm(child ~ 1, data = .) %&gt;%\n  broom::augment(Galton)\nlin_model &lt;- Galton %&gt;%\n  lm(child ~ parent, data = .) %&gt;%\n  broom::augment(Galton)\nmodels &lt;- bind_rows(null_model, lin_model) %&gt;%\n  mutate(model = rep(c(\"Null Model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null Model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR Model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#simple-linear-regression-1",
    "href": "lectures/L09/lecture-09.html#simple-linear-regression-1",
    "title": "Topic 9 – Describing relationships",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nWe fit the model in one simple line of code: fit &lt;- lm(child ~ parent, data = Galton)\n\n\nCode\nfit &lt;- lm(child ~ parent, data = Galton)\n\n\n\nAnd then we can use summary() to get a summary of the model:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/L09/lecture-09.html#summary",
    "href": "lectures/L09/lecture-09.html#summary",
    "title": "Topic 9 – Describing relationships",
    "section": "Summary",
    "text": "Summary\nCorrelation is a measure of the relationship between two numerical variables between -1 and 1.\n\nA correlation coefficient measures the strength and direction of the relationship.\nPearson’s correlation is for linear relationships, Spearman’s/Kendall’s correlation is for monotonic relationships.\nCorrelation \\(\\neq\\) causation.\n\nRegression is an analysis that models the relationship between a dependent variable and independent variable(s).\n\nThe most common method for regression is least squares, which minimises the sum of the squared residuals.\nSimple linear regression fits a straight line between two variables.\n\n\nNext week: interpreting results and assumptions for simple linear regression."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#module-overview",
    "href": "lectures/L10/lecture-10.html#module-overview",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Module overview",
    "text": "Module overview\n\n\n\n\nWeek 9. Describing Relationships\n\nCorrelation (calculation, interpretation)\nRegression (model structure, model fitting\nWhat/when/why/how\n\nWeek 10. Simple Linear Regression\n\nCan we use the model?(assumptions, hypothesis testing)\nHow good is the model?(interpretation, model fit)\n\nWeek 11. Multiple Linear Regression\n\nMultiple Linear Regression (MLR) modelling\nAssumptions, interpretation and the principle of parsimony\n\nWeek 12. Nonlinear Regression\n\nCommon nonlinear functions\nTransformations"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#last-week",
    "href": "lectures/L10/lecture-10.html#last-week",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Last week…",
    "text": "Last week…\n\nCorrelation \\(r\\): a measure of the strength and direction of the linear relationship between two variables\nIs there a moderate to strong causal relationship?\n\n\nSimple linear regression modelling\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\n\nBasically, a deterministic straight line equation \\(y=c+mx\\), with added random variation that is normally distributed\n\n\\[ Y = c + mx + \\epsilon \\]"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#fitting-the-line",
    "href": "lectures/L10/lecture-10.html#fitting-the-line",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Fitting the line",
    "text": "Fitting the line\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\]\n\\[ Y = c + mx + \\epsilon \\]\nHow do we fit a line to data if data are “noisy”?\n\n\nCode\nx &lt;- 1:10\ny &lt;- 2 * x + rnorm(10, 0, 2)\n# generate y with predicted values\ny_pred &lt;- 2 * x\ndf &lt;- data.frame(x, y)\n\np1 &lt;- ggplot(df, aes(x, y_pred)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  labs(x = \"x\", y = \"y\", title = \"A\")\n\np2 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"royalblue\") +\n  labs(x = \"x\", y = \"y\", title = \"B (How do we fit this?)\")\n\nlibrary(patchwork)\np1 + p2 + plot_layout(ncol = 2)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#usage-of-least-squares",
    "href": "lectures/L10/lecture-10.html#usage-of-least-squares",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Usage of least squares",
    "text": "Usage of least squares\n\nStudent’s t-test (indirectly)\nlinear regression\n\n\n\nnonlinear regression (logistic, polynomial, exponential, etc.)\nanalysis of variance (ANOVA)\ngeneralised linear model\nprinciple component analysis\nmachine learning models\netc…"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#galtons-data-revisited",
    "href": "lectures/L10/lecture-10.html#galtons-data-revisited",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Galton’s data revisited",
    "text": "Galton’s data revisited\n\nGalton’s data on the heights of parents and their children.\nIs there a relationship between the heights of parents and their children?\n\n\n\nCode\nlibrary(HistData)\ndata(Galton)\nfit &lt;- lm(child ~ parent, data = Galton)\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  labs(x = \"Parent height (inches)\", y = \"Child height (inches)\")\n\n\n\nHow did we end up with the line in the plot above?"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#how-do-we-analytically-fit-a-line",
    "href": "lectures/L10/lecture-10.html#how-do-we-analytically-fit-a-line",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "How do we analytically fit a line?",
    "text": "How do we analytically fit a line?\nWe calculate slope (\\(\\beta_1\\)):\n\\[ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} \\] Then substitute below to get the intercept (\\(\\beta_0\\)):\n\\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\] Imagine a dataset with a million points - this would be computationally taxing."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#how-do-we-numerically-fit-a-line",
    "href": "lectures/L10/lecture-10.html#how-do-we-numerically-fit-a-line",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "How do we numerically fit a line?",
    "text": "How do we numerically fit a line?\n\nMinimise the sum of the squared residuals via trial and error\nMost common method used via computers (gradient-descent)\nCan be done by hand (but not recommended)\n\n\\[\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)}\\color{black})^2\\]\n\nSource"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#fitting-a-linear-model-in-r",
    "href": "lectures/L10/lecture-10.html#fitting-a-linear-model-in-r",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Fitting a linear model in R",
    "text": "Fitting a linear model in R\nIs there a relationship between the heights of parents and their children?\n\n\nCode\nfit &lt;- lm(child ~ parent, data = Galton)\nfit\n\n\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n\n\n\\[ \\widehat{child} = 23.9 + 0.646 \\cdot parent\\]\n\nDo we trust our model? How good is the model? How can we interpret the results?"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#steps-for-regression",
    "href": "lectures/L10/lecture-10.html#steps-for-regression",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Steps for Regression",
    "text": "Steps for Regression\n\nUnderstand the variables\nExplore data\nFit model\nCheck assumptions\nAssess model fit\nInterpret output"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumptions---line",
    "href": "lectures/L10/lecture-10.html#assumptions---line",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Assumptions - LINE",
    "text": "Assumptions - LINE\n\nLinearity. The relationship between \\(y\\) and \\(x\\) is linear.\nIndependence. The errors \\(\\epsilon\\) are independent.\nNormal. The errors \\(\\epsilon\\) are normally distributed.\nEqual Variance of errors \\(\\epsilon\\). At each value of \\(x\\), the variance of \\(y\\) is the same i.e. homoskedasticity, or constant variance.\n\n\nWhy do we care?\n\nIf the assumptions are met, then we can be confident that the model is a good representation of the data.\nIf they are not met, the results are still presented, but our interpretation of the model is likely to be flawed.\n\nHypothesis test results are unreliable\nStandard error is unreliable\nPoor estimates of coefficients = poor predictions"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#how-do-we-check-the-assumptions",
    "href": "lectures/L10/lecture-10.html#how-do-we-check-the-assumptions",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "How do we check the assumptions?",
    "text": "How do we check the assumptions?\nRecall that the linear model is a deterministic straight line equation \\(y = c + mx\\) plus some random noise \\(\\epsilon\\):\n\\[ Y_i = \\beta_0 + \\beta_1 x + \\epsilon \\]\n\nIf the only source of variation in \\(y\\) is \\(\\epsilon\\), then we can check our assumptions by just looking at the residuals \\(\\hat{\\epsilon}\\).\n\n\n\n\n\n\n\n\nTip\n\n\nAll but the independence assumption can be assessed using diagnostic plots. R will not warn you if the assumptions are not met. It is up to you to check them!"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#how-do-we-get-the-residuals",
    "href": "lectures/L10/lecture-10.html#how-do-we-get-the-residuals",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "How do we get the residuals?",
    "text": "How do we get the residuals?\n\nFit the model!\nResiduals need to be calculated from the model, not from the raw data.\nIn R, these values are stored automatically.\n\n\\[ \\color{firebrick}{\\hat{\\epsilon_i}} = \\color{royalblue}{y_i} - \\color{forestgreen}{\\hat{y_i}} \\]\n\n\nCode\n# simulate example data\nset.seed(340)\nx &lt;- runif(8, 0, 30)\ny &lt;- 5 * x + rnorm(8, 0, 40)\ndf &lt;- data.frame(x, y)\n\n# fit linear model, add residual vertical lines as arrows\nmod &lt;- lm(y ~ x, data = df)\np1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_segment(aes(xend = x, yend = fitted(mod)),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    color = \"royalblue\"\n  ) +\n  labs(x = \"x\", y = \"y\")\n\np1 +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"firebrick\") +\n  annotate(\"text\",\n    x = 6.3, y = -6, size = 7,\n    label = expression(hat(epsilon[i])), colour = \"royalblue\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = 25, size = 7,\n    label = expression(hat(y[i])), colour = \"forestgreen\"\n  ) +\n  annotate(\"text\",\n    x = 5.6, y = -36, size = 7,\n    label = expression(y[i]), colour = \"firebrick\"\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#another-way-to-look-at-residuals",
    "href": "lectures/L10/lecture-10.html#another-way-to-look-at-residuals",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Another way to look at residuals",
    "text": "Another way to look at residuals\n For a given ‘x’, there is a range of ‘y’ values that are possible.\n\ne.g. For a given parent height, there is a range of child heights that are possible."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#step",
    "href": "lectures/L10/lecture-10.html#step",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "1-step",
    "text": "1-step\n\nResiduals vs. Fitted: check for linearity, equal variance.\nQ-Q Residuals: check for normality.\nScale-Location: check for equal variance (standardised).\nResiduals vs. Leverage: check for outliers (influential points).\n\n\n\nCode\npar(mfrow = c(2, 2)) # need to do this to get 4 plots on one page\nplot(fit)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumption-linearity",
    "href": "lectures/L10/lecture-10.html#assumption-linearity",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Assumption: Linearity",
    "text": "Assumption: Linearity\n\nResiduals vs. fitted plot looks at the relationship between the residuals and the fitted values.\nIf the relationship is linear:\n\nResiduals should be randomly scattered around the horizontal axis.\nThe red line should be reasonably straight.\n\nCould also look at a scatterplot of x and y!\n\n\n\nCode\nplot(fit, which = 1)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#examples",
    "href": "lectures/L10/lecture-10.html#examples",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Examples",
    "text": "Examples\n \nLinear Models with R (Faraway 2005, p59)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumption-normality",
    "href": "lectures/L10/lecture-10.html#assumption-normality",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\nQ-Q plot looks at the distribution of the residuals against a normal distribution function (the dotted line).\nSometimes, a histogram is still useful to see the shape of the distribution.\n\n\n\nCode\npar(mfrow = c(1, 2))\nplot(fit, which = 2)\nhist(rstandard(fit))"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#assumption-normality-1",
    "href": "lectures/L10/lecture-10.html#assumption-normality-1",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Assumption: Normality",
    "text": "Assumption: Normality\n\nIf normally distributed, the points should follow the red line.\nDeviation from the red line is common in the tails (i.e. the ends), but not in the middle.\n\n\nTips\n\nLight-tailed: small variance in residuals, resulting in a narrow distribution.\nHeavy-tailed: many extreme positive and negative residuals, resulting in a wide distribution.\nLeft-skewed (n shape): more data falls to the left of the mean.\nRight-skewed (u shape): more data falls to the right of the mean.\n\n\n\n\n\n\n\nTip\n\n\nLeft or right-skewed? Look at where the tail points."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#examples-1",
    "href": "lectures/L10/lecture-10.html#examples-1",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Examples",
    "text": "Examples\n\n\nCode\nset.seed(915)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x + rchisq(100, df = 2)\ndf &lt;- data.frame(x, y)\nfit_eg &lt;- lm(y ~ x, data = df)\npar(mfrow = c(1, 2))\nplot(fit_eg, which = 2)\nhist(rstandard(fit_eg))"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#equal-variances",
    "href": "lectures/L10/lecture-10.html#equal-variances",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Equal variances",
    "text": "Equal variances\n\nLook at the scale-location plot.\nIf variances are equal, the points should be randomly scattered around the horizontal axis.\nThe red line should be more or less horizontal.\n\n\n\nCode\nplot(fit, which = 3)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#equal-variances-1",
    "href": "lectures/L10/lecture-10.html#equal-variances-1",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Equal variances",
    "text": "Equal variances\n\nIf variances are not equal we may see:\n\nA funnel shape, where the points are more spread out at the ends than in the middle. Sometimes also called “fanning”.\nPatterns in the scale-location plot, such as a curve or a wave, indicating that the variance is changing.\n\nLook at the red line for a general trend, but don’t depend on it too much.\n\n\n\nCode\nset.seed(915)\nx &lt;- rnorm(100)\ny &lt;- 2 + 5 * x^2 + rchisq(100, df = 2)\ndf &lt;- data.frame(x, y)\nfit_eg &lt;- lm(y ~ x, data = df)\nplot(fit_eg, which = 3)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#outliers",
    "href": "lectures/L10/lecture-10.html#outliers",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Outliers",
    "text": "Outliers\n\nLeverage is a measure of how far away the predictor variable is from the mean of the predictor variable.\nThe Residuals vs Leverage plot shows the relationship between the residuals and the leverage of each point.\nCook’s distance is a measure of how much the model would change if a point was removed."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#what-can-we-do-if-the-assumptions-arent-met",
    "href": "lectures/L10/lecture-10.html#what-can-we-do-if-the-assumptions-arent-met",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "What can we do if the assumptions aren’t met?",
    "text": "What can we do if the assumptions aren’t met?\nIt depends…\n…which assumption is not met and the type of data i.e. circumstances.\n\nIf data is non-linear, try a transformation of the response variable \\(y\\), from light to extreme:\n\nroot: \\(\\sqrt{y}\\) or \\(\\sqrt{y+1}\\) if \\(y\\) contains zeros\nlog: \\(\\log(y)\\) or \\(\\log(y+1)\\) if \\(y\\) contains zeros\ninverse: \\(\\frac{1}{y}\\) or \\(\\frac{1}{y+1}\\) if \\(y\\) contains zeros\n\nIf residuals are not normally distributed, try a transformation of the response variable \\(y\\) first, otherwise transform the predictor variable \\(x\\). Both can be done at the same time.\nIf equal variances assumption is not met, same as above.\nIf outliers are present, try removing them, or transforming the response variable \\(y\\)."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#what-if-transformation-doesnt-work",
    "href": "lectures/L10/lecture-10.html#what-if-transformation-doesnt-work",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "What if transformation doesn’t work?",
    "text": "What if transformation doesn’t work?\nIf the assumptions are still not met after trying the above, you can try:\n\nUsing a different type of regression e.g. logistic regression, non-linear regression\nUsing a different model e.g. machine learning.\nUsing a non-parametric test."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#back-to-galton---model-assumptions-are-met",
    "href": "lectures/L10/lecture-10.html#back-to-galton---model-assumptions-are-met",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Back to Galton - model assumptions are met",
    "text": "Back to Galton - model assumptions are met\n\nNow what?"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nCall: the model formula\nResiduals: distribution of the residuals"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-1",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-1",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\n\nCoefficients: a summary table of the coefficients, their standard errors, t-values, and p-values.\n(Intercept)/Estimate: the y-intercept, or the mean response when all predictors are 0.\nparent/Estimate: the slope coefficient - i.e. the change in the mean of the response for a one-unit increase in the predictor."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-2",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-2",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nPr: the p-value\n\n(Intercept)/Pr: the p-value of the y-intercept is not meaningful.\nparent/Pr: is parent a significant predictor to the model?"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-3",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-3",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also use the Estimate values to write the equation of the regression line: \\[ \\widehat{child} = 23.94153 + 0.64629 \\cdot parent\\]\nFor every one-inch increase in the parent height, the child height is predicted to increase by 0.64629 inches.\ne.g. if a parent is 70 inches, how tall will the child be? \\(23.94153 + 0.64629 \\cdot 70 = 68.5\\) inches (174 cm)."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-4",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-4",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nResidual standard error: the standard deviation of the residuals.\n\nInterpretation: the average amount that the response will deviate from the true regression line.\n\ndegrees of freedom: the number of observations minus the number of parameters being estimated. Used in hypothesis testing and calculating the standard error of the regression coefficients."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-5",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-5",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nMultiple R-squared: the proportion of variance (0-1) explained by the model (for simple linear regression).\nAdjusted R-squared: the proportion of variance (0-1) explained by the model, adjusted for the number of predictors (for multiple linear regression).\nRanges from 0 to 1; R2 = 1 is a perfect fit.\n“The proportion of variance in the response that is explained by parent: 21.05%.”"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpreting-the-output-6",
    "href": "lectures/L10/lecture-10.html#interpreting-the-output-6",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nF-statistic: the ratio of the variance explained by predictors, and the residual variance (variance not explained by predictors).\n\nAlso known as the partial F-test between the full model and the intercept-only (null) model.\n\np-value: the probability that the F-statistic is greater than the observed value under the null hypothesis.\n\nIn a simple linear regression, the p-value for the slope coefficient is the same as the p-value for the F-statistic."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#hypothesis-testing",
    "href": "lectures/L10/lecture-10.html#hypothesis-testing",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nHow does our null (\\(H_0: \\beta_1=0\\)) model compare to the linear (\\(H_0: \\beta_1 \\neq 0\\)) model?\n\nIn simple linear regression, the p-value for the slope coefficient is the same as the p-value for the F-statistic.\n\n\n\nCode\nnull_model &lt;- Galton %&gt;%\n  lm(child ~ 1, data = .) %&gt;%\n  augment(Galton)\nlin_model &lt;- Galton %&gt;%\n  lm(child ~ parent, data = .) %&gt;%\n  augment(Galton)\nmodels &lt;- bind_rows(null_model, lin_model) %&gt;%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#what-are-we-testing",
    "href": "lectures/L10/lecture-10.html#what-are-we-testing",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "What are we testing?",
    "text": "What are we testing?\n\nThe null model is a model with no predictors, i.e. \\(y = \\beta_0 + \\epsilon\\)\nThe alternative model is a linear model with one predictor, i.e. \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\nWe use the t-test to compare the two models:\n\n\\[ t = \\frac{estimate - 0}{Standard\\ error} = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} \\] where \\(SE(\\hat{\\beta}_1)\\) is the standard error of the slope estimate:\n\\[ SE(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} \\]\n\n\n\n\n\n\nNote\n\n\nIf the model is not significant, then the null model (i.e. mean) is better. If the model is significant, then the linear model is better."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#reporting",
    "href": "lectures/L10/lecture-10.html#reporting",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Reporting",
    "text": "Reporting\n\nRefresher on earlier steps\n\nGalton collected data on the average height of both parents and their children. There wasa moderate, positive, linear relationship (\\(r\\) = 0.46) between parent and child height.\n\nCan we rely on our model results? Assumptions and hypothesis?\n\nWe fitted a linear model to predict child height from parent height, and model assumptions were met. The model was statistically significant (R2 = 0.21, F(1, 926) = 246.84, p &lt; .001), hence the null hypothesis was rejected in favour of the linear model. The effect of parent height is statistically significant and positive (\\(\\beta\\) = 0.65, t(926) = 15.71, p &lt; .001).\n\nEquation? Inference?\n\nFor every one-inch increase in parent height, child height is predicted to increase by 0.65 inches ($ = 23.94 + 0.65 parent$). The average height of both parents explains 21.05% of the variance in child height – there is an effect but there are clearly more factors at play."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#fun-fact",
    "href": "lectures/L10/lecture-10.html#fun-fact",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Fun fact",
    "text": "Fun fact\n\nGalton’s key finding was that children of tall parents are not as tall, children of short parents are not as short.\n\n\\[ \\widehat{child} = 23.94153 + 0.64629 \\cdot parent\\]\n\ne.g. if average parent height is 60 inches (152 cm), how tall will the child be? \\(23.94153 + 0.64629 \\cdot 60 = 62.7\\) inches (159 cm).\ne.g. if average parent height is 75 inches (191 cm), how tall will the child be? \\(23.94153 + 0.64629 \\cdot 75 = 72.4\\) inches (184 cm).\nThe height of children appeared to regress towards the population mean, i.e. the concept of regression to the mean\nHence the Galton is credited with coining the term regression (and also correlation, percentile, median, etc.)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#explore",
    "href": "lectures/L10/lecture-10.html#explore",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Explore",
    "text": "Explore\nRead the data:\n\n\nCode\nlibrary(readxl) # load the readxl package\n\nalligator &lt;- read_excel(path = \"data/ENVX1002_Lecture_wk10_data.xlsx\", \n  sheet = \"Alligator\") # read in the data\n\n\nWhat does the data look like?\n\n\nCode\nstr(alligator)\n\n\ntibble [25 × 2] (S3: tbl_df/tbl/data.frame)\n $ Length: num [1:25] 58 61 63 68 69 72 72 74 74 76 ...\n $ Weight: num [1:25] 28 44 33 39 36 38 61 54 51 42 ..."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#plot",
    "href": "lectures/L10/lecture-10.html#plot",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Plot",
    "text": "Plot\n\nUsing base RUsing ggplot2\n\n\n\n\nCode\nplot(x = alligator$Length, y = alligator$Weight, \n  xlab = \"Length (cm)\", ylab = \"Weight (kg)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2) # load the ggplot2 package\nggplot(data = alligator, aes(x = Length, y = Weight)) +\n  geom_point() +\n  labs(x = \"Length (cm)\", y = \"Weight (kg)\")"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#plot-residual-diagnostics",
    "href": "lectures/L10/lecture-10.html#plot-residual-diagnostics",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Plot residual diagnostics",
    "text": "Plot residual diagnostics\nTo check assumptions, we need to fit the model first, then plot the model.\n\n\nCode\nfit &lt;- lm(formula = Weight ~ Length, data = alligator)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#check-assumptions",
    "href": "lectures/L10/lecture-10.html#check-assumptions",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Check assumptions",
    "text": "Check assumptions\nIs the relationship linear?\n\n\nCode\nplot(fit, which = 1)\n\n\n\nIf the linearity assumption is not met, there is no reason to validate the model since it is no longer suitable for the data."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#dealing-with-non-linearity-transform-the-data",
    "href": "lectures/L10/lecture-10.html#dealing-with-non-linearity-transform-the-data",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Dealing with non-linearity: transform the data",
    "text": "Dealing with non-linearity: transform the data\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- ggplot(data = alligator, aes(x = Length, y = Weight)) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"Weight (kg)\", title = \"Original\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np2 &lt;- ggplot(data = alligator, aes(x = Length, y = sqrt(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"sqrt[Weight (kg)]\", title = \"Square root\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np3 &lt;- ggplot(data = alligator, aes(x = Length, y = log(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"log[Weight (kg)]\", title = \"Natural log\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np4 &lt;- ggplot(data = alligator, aes(x = Length, y = log10(Weight))) +\n  geom_point(size = 3) +\n  labs(x = \"Length (cm)\", y = \"log10[Weight (kg)]\", title = \"Log base 10\") +\n  geom_smooth(se = FALSE, linetype = 2)\n\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#natural-log-transformation-check-assumptions-again",
    "href": "lectures/L10/lecture-10.html#natural-log-transformation-check-assumptions-again",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Natural log transformation – Check assumptions again",
    "text": "Natural log transformation – Check assumptions again\n\n\nCode\nfit &lt;- lm(formula = log(Weight) ~ Length, data = alligator)\npar(mfrow = c(2, 2)) # set up a 2 x 2 grid for plots\nplot(fit)"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#interpretation",
    "href": "lectures/L10/lecture-10.html#interpretation",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = log(Weight) ~ Length, data = alligator)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.289266 -0.079989  0.000933  0.102216  0.288491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.335335   0.131394   10.16 5.63e-10 ***\nLength      0.035416   0.001506   23.52  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1493 on 23 degrees of freedom\nMultiple R-squared:  0.9601,    Adjusted R-squared:  0.9583 \nF-statistic:   553 on 1 and 23 DF,  p-value: &lt; 2.2e-16\n\n\n\nLength is a statistically significant predictor of log(Weight) (p &lt; .001)\nThe model explains a statistically significant and large proportion (96%) of variance (R2 = 0.96, F(1, 23) = 553, p &lt; .001)\nFor every 1 cm increase in Length, log(Weight) increases by a value of 0.0354, Weight increases by \\(e^{0.0354}\\) times and Weight increases by approximately 3.54%"
  },
  {
    "objectID": "lectures/L10/lecture-10.html#percent-change-with-ln-transformation",
    "href": "lectures/L10/lecture-10.html#percent-change-with-ln-transformation",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Percent change with \\(ln\\) transformation",
    "text": "Percent change with \\(ln\\) transformation\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute \\(e\\) below for 10 or any other base), but the quick approximation only works with natural log transformations.\nIf \\(y\\) has been transformed with a natural log (log(y)), for a one-unit increase in \\(x\\) the percent change in \\(y\\) (not log(y)) is calculated with:\n\\[\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)\\]\nIf \\(\\beta_1\\) is small (i.e. \\(-0.25 &lt; \\beta_1 &lt; 0.25\\)), then: \\(e^{\\beta_1} \\approx 1 + \\beta_1\\). So \\(\\Delta y \\% \\approx 100 \\cdot \\beta_1\\).\n\n\n\n\n\n\n\n\n\n\nβ\nExact \\((e^{\\beta} - 1)\\)%\nApproximate \\(100 \\cdot \\beta\\)\n\n\n\n\n-0.25\n-22.13\n-25\n\n\n-0.1\n-9.52\n-10\n\n\n0.01\n1.01\n1\n\n\n0.1\n10.52\n10\n\n\n0.25\n28.41\n25\n\n\n0.5\n64.87\n50\n\n\n2\n638.91\n200\n\n\n\n\n\n\\(y\\) transformed: a one-unit increase in \\(x\\) is approximately a \\(\\beta_1\\)% change in \\(y\\).\n\\(x\\) transformed: a 1% increase in \\(x\\) is approximately a \\(0.01 \\cdot \\beta_1\\) change in \\(y\\).\nBoth \\(x\\) and \\(y\\) transformed: a 1% increase in x is approximately a \\(\\beta_1\\)% change in y."
  },
  {
    "objectID": "lectures/L10/lecture-10.html#summary",
    "href": "lectures/L10/lecture-10.html#summary",
    "title": "Topic 10 – Simple Linear Regression",
    "section": "Summary",
    "text": "Summary\nWorkflow\n\nUnderstand the variables: Which is the response variable? Is there a reason to believe a causal relationship?\nExplore data: How many observations? Summary statistics? Scatterplot and correlation?\nFit model\nCheck assumptions: Remember - it’s about the residuals! If assumptions fail, try transforming and return to Step 3. If assumptions still fail, consider another model and return to Step 3.\nAssess model fit: Hypothesis test, significance, F-statistic, p-value. R2, how much model variation was explained by the model.\nInterpret output: ‘For every one-unit increase in x, y increases by \\(\\beta_1\\) units…’ and any additional research/insight."
  },
  {
    "objectID": "lectures/L11/index.html",
    "href": "lectures/L11/index.html",
    "title": "Lecture 11",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L11 -- Multiple predictors"
    ]
  },
  {
    "objectID": "lectures/L12/index.html",
    "href": "lectures/L12/index.html",
    "title": "Lecture 12",
    "section": "",
    "text": "Full Screen | PDF",
    "crumbs": [
      "{{< fa house-chimney >}}",
      "**Module 3 modelling**",
      "L12 -- Non-linear functions"
    ]
  }
]